{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"UMAP","text":"<p>This is a fork of the original UMAP repository (lmcinnes/umap). For the official UMAP project, please visit the original repository.</p> <p>Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualisation similarly to t-SNE, but also for general non-linear dimension reduction. The algorithm is founded on three assumptions about the data:</p> <ol> <li>The data is uniformly distributed on a Riemannian manifold;</li> <li>The Riemannian metric is locally constant (or can be approximated as such);</li> <li>The manifold is locally connected.</li> </ol> <p>From these assumptions it is possible to model the manifold with a fuzzy topological structure. The embedding is found by searching for a low dimensional projection of the data that has the closest possible equivalent fuzzy topological structure.</p> <p>The details for the underlying mathematics can be found in our paper on ArXiv:</p> <p>McInnes, L, Healy, J, UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction, ArXiv e-prints 1802.03426, 2018</p> <p>A broader introduction to UMAP targetted the scientific community can be found in our paper published in Nature Review Methods Primers:</p> <p>Healy, J., McInnes, L. Uniform manifold approximation and projection. Nat Rev Methods Primers 4, 82 (2024).</p> <p>A read only version of this paper can accessed via link</p> <p>The important thing is that you don't need to worry about that\u2014you can use UMAP right now for dimension reduction and visualisation as easily as a drop in replacement for scikit-learn's t-SNE.</p> <p>Documentation is available via Read the Docs.</p> <p>New: this package now also provides support for densMAP. The densMAP algorithm augments UMAP to preserve local density information in addition to the topological structure of the data. Details of this method are described in the following paper:</p> <p>Narayan, A, Berger, B, Cho, H, Assessing Single-Cell Transcriptomic Variability through Density-Preserving Data Visualization, Nature Biotechnology, 2021</p>"},{"location":"#installing","title":"Installing","text":"<p>UMAP depends upon <code>scikit-learn</code>, and thus <code>scikit-learn</code>'s dependencies such as <code>numpy</code> and <code>scipy</code>. UMAP adds a requirement for <code>numba</code> for performance reasons. The original version used Cython, but the improved code clarity, simplicity and performance of Numba made the transition necessary.</p> <p>New in this version: This fork includes an optimized Rust-based HNSW nearest neighbor backend (<code>hnsw-rs</code>) that provides significant performance improvements over the default implementations, especially for large datasets.</p> <p>Requirements:</p> <ul> <li>Python 3.6 or greater</li> <li>numpy</li> <li>scipy</li> <li>scikit-learn</li> <li>numba</li> <li>tqdm</li> <li>pynndescent</li> </ul> <p>Recommended packages:</p> <ul> <li>For plotting</li> <li>matplotlib</li> <li>datashader</li> <li>holoviews</li> <li>For Parametric UMAP (PyTorch-based)</li> <li>torch &gt;= 1.9.0</li> <li>torchvision (for image utilities)</li> <li>For optimized nearest neighbor search</li> <li>hnsw-rs (Rust-based HNSW backend)</li> </ul>"},{"location":"#install-options","title":"Install Options","text":"<p>The recommended way to install UMAP is via PyPI using uv, which provides faster and more reliable dependency resolution:</p> Bash<pre><code>uv pip install umap\n</code></pre> <p>If you don't have uv installed, you can install it from https://docs.astral.sh/uv/getting-started/installation/.</p> <p>Alternatively, you can install UMAP using pip:</p> Bash<pre><code>pip install umap\n</code></pre> <p>This will install UMAP and all required dependencies.</p> <p>If you wish to use the plotting functionality you can use</p> Bash<pre><code>uv pip install umap[plot]\n</code></pre> <p>or with pip:</p> Bash<pre><code>pip install umap[plot]\n</code></pre> <p>to install all the plotting dependencies.</p> <p>If you wish to use Parametric UMAP, you need to install PyTorch, which can be installed using the instructions at https://pytorch.org/get-started/locally (choose your platform and preferences), or use:</p> Bash<pre><code>uv pip install umap[parametric_umap]\n</code></pre> <p>or with pip:</p> Bash<pre><code>pip install umap[parametric_umap]\n</code></pre> <p>This will install PyTorch with CPU support. For GPU support, follow the official PyTorch installation guide to install the appropriate CUDA version.</p> <p>If you're on an x86 processor, you can also optionally install <code>tbb</code>, which will provide additional CPU optimizations:</p> Bash<pre><code>uv pip install umap[tbb]\n</code></pre> <p>or with pip:</p> Bash<pre><code>pip install umap[tbb]\n</code></pre>"},{"location":"#manual-development-install","title":"Manual Development Install","text":"<p>For a manual development install, clone the repository and install in editable mode:</p> Bash<pre><code>git clone https://github.com/lmcinnes/umap.git\ncd umap\n</code></pre> <p>Then create a virtual environment (recommended) and install the package using uv:</p> Bash<pre><code>uv venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\nuv pip install -e .\n</code></pre> <p>Or if you prefer to use pip:</p> Bash<pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\npip install -e .\n</code></pre>"},{"location":"#development-testing-with-testmon","title":"Development Testing with testmon","text":"<p>This project uses <code>pytest-testmon</code> to optimize test execution by only running tests affected by code changes. For best results (especially in CI/CD environments), ensure testmon uses a file-based database:</p> Bash<pre><code># Run tests with file-based testmon cache\npytest --testmon-db=.testmon.db umap/tests/\n</code></pre> <p>Configure in <code>pyproject.toml</code> or pytest config:</p> INI<pre><code>[tool:pytest]\ntestmon_db = .testmon.db\n</code></pre> <p>The file-based database (<code>.testmon.db</code>) should be persistent across test runs to provide reliable optimization benefits.</p>"},{"location":"#how-to-use-umap","title":"How to use UMAP","text":"<p>The umap package inherits from sklearn classes, and thus drops in neatly next to other sklearn transformers with an identical calling API.</p> Python<pre><code>import umap\nfrom sklearn.datasets import load_digits\n\ndigits = load_digits()\n\nembedding = umap.UMAP().fit_transform(digits.data)\n</code></pre> <p>There are a number of parameters that can be set for the UMAP class; the major ones are as follows:</p> <ul> <li> <p><code>n_neighbors</code>: This determines the number of neighboring points used in local approximations of manifold structure. Larger values will result in more global structure being preserved at the loss of detailed local structure. In general this parameter should often be in the range 5 to 50, with a choice of 10 to 15 being a sensible default.</p> </li> <li> <p><code>min_dist</code>: This controls how tightly the embedding is allowed compress points together. Larger values ensure embedded points are more evenly distributed, while smaller values allow the algorithm to optimise more accurately with regard to local structure. Sensible values are in the range 0.001 to 0.5, with 0.1 being a reasonable default.</p> </li> <li> <p><code>metric</code>: This determines the choice of metric used to measure distance in the input space. A wide variety of metrics are already coded, and a user defined function can be passed as long as it has been JITd by numba.</p> </li> </ul> <p>An example of making use of these options:</p> Python<pre><code>import umap\nfrom sklearn.datasets import load_digits\n\ndigits = load_digits()\n\nembedding = umap.UMAP(n_neighbors=5,\n                      min_dist=0.3,\n                      metric='correlation').fit_transform(digits.data)\n</code></pre> <p>UMAP also supports fitting to sparse matrix data. For more details please see the UMAP documentation</p>"},{"location":"#benefits-of-umap","title":"Benefits of UMAP","text":"<p>UMAP has a few signficant wins in its current incarnation.</p> <p>First of all UMAP is fast. It can handle large datasets and high dimensional data without too much difficulty, scaling beyond what most t-SNE packages can manage. This includes very high dimensional sparse datasets. UMAP has successfully been used directly on data with over a million dimensions.</p> <p>Second, UMAP scales well in embedding dimension\u2014it isn't just for visualisation! You can use UMAP as a general purpose dimension reduction technique as a preliminary step to other machine learning tasks. With a little care it partners well with the hdbscan clustering library (for more details please see Using UMAP for Clustering).</p> <p>Third, UMAP often performs better at preserving some aspects of global structure of the data than most implementations of t-SNE. This means that it can often provide a better \"big picture\" view of your data as well as preserving local neighbor relations.</p> <p>Fourth, UMAP supports a wide variety of distance functions, including non-metric distance functions such as cosine distance and correlation distance. You can finally embed word vectors properly using cosine distance!</p> <p>Fifth, UMAP supports adding new points to an existing embedding via the standard sklearn <code>transform</code> method. This means that UMAP can be used as a preprocessing transformer in sklearn pipelines.</p> <p>Sixth, UMAP supports supervised and semi-supervised dimension reduction. This means that if you have label information that you wish to use as extra information for dimension reduction (even if it is just partial labelling) you can do that\u2014as simply as providing it as the <code>y</code> parameter in the fit method.</p> <p>Seventh, UMAP supports a variety of additional experimental features including: an \"inverse transform\" that can approximate a high dimensional sample that would map to a given position in the embedding space; the ability to embed into non-euclidean spaces including hyperbolic embeddings, and embeddings with uncertainty; very preliminary support for embedding dataframes also exists.</p> <p>Finally, UMAP has solid theoretical foundations in manifold learning (see our paper on ArXiv). This both justifies the approach and allows for further extensions that will soon be added to the library.</p>"},{"location":"#performance-and-examples","title":"Performance and Examples","text":"<p>UMAP is very efficient at embedding large high dimensional datasets. In particular it scales well with both input dimension and embedding dimension. For the best possible performance we recommend:</p> <ol> <li>Installing the Rust-based HNSW backend (hnsw-rs) for optimized nearest neighbor search</li> <li>Installing the nearest neighbor computation library pynndescent as a fallback</li> </ol> <p>UMAP will work without these packages, but with them installed it will run significantly faster, particularly on multicore machines and large datasets.</p> <p>For a problem such as the 784-dimensional MNIST digits dataset with 70000 data samples, UMAP can complete the embedding in under a minute (as compared with around 45 minutes for scikit-learn's t-SNE implementation). Despite this runtime efficiency, UMAP still produces high quality embeddings.</p> <p>The obligatory MNIST digits dataset, embedded in 42 seconds (with pynndescent installed and after numba jit warmup) using a 3.1 GHz Intel Core i7 processor (n_neighbors=10, min_dist=0.001):</p> <p></p> <p>The MNIST digits dataset is fairly straightforward, however. A better test is the more recent \"Fashion MNIST\" dataset of images of fashion items (again 70000 data sample in 784 dimensions). UMAP produced this embedding in 49 seconds (n_neighbors=5, min_dist=0.1):</p> <p></p> <p>The UCI shuttle dataset (43500 sample in 8 dimensions) embeds well under correlation distance in 44 seconds (note the longer time required for correlation distance computations):</p> <p></p> <p>The following is a densMAP visualization of the MNIST digits dataset with 784 features based on the same parameters as above (n_neighbors=10, min_dist=0.001). densMAP reveals that the cluster corresponding to digit 1 is noticeably denser, suggesting that there are fewer degrees of freedom in the images of 1 compared to other digits.</p> <p></p>"},{"location":"#plotting","title":"Plotting","text":"<p>UMAP includes a subpackage <code>umap.plot</code> for plotting the results of UMAP embeddings. This package needs to be imported separately since it has extra requirements (matplotlib, datashader and holoviews). It allows for fast and simple plotting and attempts to make sensible decisions to avoid overplotting and other pitfalls. An example of use:</p> Python<pre><code>import umap\nimport umap.plot\nfrom sklearn.datasets import load_digits\n\ndigits = load_digits()\n\nmapper = umap.UMAP().fit(digits.data)\numap.plot.points(mapper, labels=digits.target)\n</code></pre> <p>The plotting package offers basic plots, as well as interactive plots with hover tools and various diagnostic plotting options. See the documentation for more details.</p>"},{"location":"#parametric-umap","title":"Parametric UMAP","text":"<p>Parametric UMAP provides support for training a neural network to learn a UMAP based transformation of data using PyTorch. This can be used to support faster inference of new unseen data, more robust inverse transforms, autoencoder versions of UMAP and semi-supervised classification (particularly for data well separated by UMAP and very limited amounts of labelled data). The PyTorch-based implementation provides better performance and flexibility compared to TensorFlow. See the documentation of Parametric UMAP or the example notebooks for more.</p>"},{"location":"#densmap","title":"densMAP","text":"<p>The densMAP algorithm augments UMAP to additionally preserve local density information in addition to the topological structure captured by UMAP. One can easily run densMAP using the umap package by setting the <code>densmap</code> input flag:</p> Python<pre><code>embedding = umap.UMAP(densmap=True).fit_transform(data)\n</code></pre> <p>This functionality is built upon the densMAP implementation provided by the developers of densMAP, who also contributed to integrating densMAP into the umap package.</p> <p>densMAP inherits all of the parameters of UMAP. The following is a list of additional parameters that can be set for densMAP:</p> <ul> <li> <p><code>dens_frac</code>: This determines the fraction of epochs (a value between 0 and 1) that will include the density-preservation term in the optimization objective. This parameter is set to 0.3 by default. Note that densMAP switches density optimization on after an initial phase of optimizing the embedding using UMAP.</p> </li> <li> <p><code>dens_lambda</code>: This determines the weight of the density-preservation objective. Higher values prioritize density preservation, and lower values (closer to zero) prioritize the UMAP objective. Setting this parameter to zero reduces the algorithm to UMAP. Default value is 2.0.</p> </li> <li> <p><code>dens_var_shift</code>: Regularization term added to the variance of local densities in the embedding for numerical stability. We recommend setting this parameter to 0.1, which consistently works well in many settings.</p> </li> <li> <p><code>output_dens</code>: When this flag is True, the call to <code>fit_transform</code> returns, in addition to the embedding, the local radii (inverse measure of local density defined in the densMAP paper) for the original dataset and for the embedding. The output is a tuple <code>(embedding, radii_original, radii_embedding)</code>. Note that the radii are log-transformed. If False, only the embedding is returned. This flag can also be used with UMAP to explore the local densities of UMAP embeddings. By default this flag is False.</p> </li> </ul> <p>For densMAP we recommend larger values of <code>n_neighbors</code> (e.g. 30) for reliable estimation of local density.</p> <p>An example of making use of these options (based on a subsample of the mnist_784 dataset):</p> Python<pre><code>import umap\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.utils import resample\n\ndigits = fetch_openml(name='mnist_784')\nsubsample, subsample_labels = resample(digits.data, digits.target, n_samples=7000,\n                                       stratify=digits.target, random_state=1)\n\nembedding, r_orig, r_emb = umap.UMAP(densmap=True, dens_lambda=2.0, n_neighbors=30,\n                                     output_dens=True).fit_transform(subsample)\n</code></pre> <p>See the documentation for more details.</p>"},{"location":"#interactive-umap-with-nomic-atlas","title":"Interactive UMAP with Nomic Atlas","text":"<p>For interactive exploration of UMAP embeddings, especially for visualizing large datasets data over time/training epochs, you can use Nomic Atlas. Nomic Atlas is a platform for embedding generation, visualization, analysis, and retrieval that directly integrates UMAP as one of its projection models.</p> <p>Using Nomic Atlas with UMAP is straightforward:</p> Python<pre><code>from nomic import AtlasDataset\nfrom nomic.data_inference import ProjectionOptions\n\n# Create a dataset\ndataset = AtlasDataset(\"my-dataset\")\n\n# data is a DataFrame or a list of dicts\ndataset.add_data(data)\n\n# Create an interactive UMAP in Atlas\natlas_map = dataset.create_index(\n    indexed_field='text',\n    projection=ProjectionOptions(\n        model=\"umap\",\n        n_neighbors=15,\n        min_dist=0.1,\n        n_epochs=200\n    )\n)\n# you can access your UMAP coordinates later on with\n# atlas_map.maps[0].embeddings.projected\n</code></pre> <p>Nomic Atlas provides:</p> <ul> <li>In-browser analysis of your UMAP data with the Atlas Analyst</li> <li>Vector search over your UMAP data using the Nomic API</li> <li>Interactive features like zooming, recoloring, searching, and filtering in the Nomic Atlas data map</li> <li>Scalability for millions of data points</li> <li>Rich information display on hover</li> <li>Shareable UMAPs via URL links to your embeddings and data maps in Atlas</li> </ul>"},{"location":"#gpu-accelerated-umap-with-torchdr","title":"GPU-Accelerated UMAP with torchdr","text":"<p>For GPU-accelerated UMAP computations, torchdr provides a PyTorch-based implementation that significantly speed up the algorithm. torchdr accelerates every step of the dimensionality reduction pipeline on GPU: kNN computation, affinity construction and embedding optimization.</p> <p>Using torchdr with UMAP is straightforward:</p> Python<pre><code>from torchdr import UMAP as torchdrUMAP\n\numap_gpu = torchdrUMAP(\n    n_neighbors=15,\n    min_dist=0.1,\n    n_components=2,\n    device='cuda'\n)\nembedding = umap_gpu.fit_transform(data-maps)\n</code></pre> <p>For more information and advanced usage, see the torchdr documentation.</p>"},{"location":"#help-and-support","title":"Help and Support","text":"<p>Documentation is at Read the Docs. The documentation includes a FAQ that may answer your questions. If you still have questions then please open an issue and I will try to provide any help and guidance that I can.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you make use of this software for your work we would appreciate it if you would cite the paper from the Journal of Open Source Software:</p> BibTeX<pre><code>@article{mcinnes2018umap-software,\n  title={UMAP: Uniform Manifold Approximation and Projection},\n  author={McInnes, Leland and Healy, John and Saul, Nathaniel and Grossberger, Lukas},\n  journal={The Journal of Open Source Software},\n  volume={3},\n  number={29},\n  pages={861},\n  year={2018}\n}\n</code></pre> <p>If you would like to cite this algorithm in your work the ArXiv paper is the current reference:</p> BibTeX<pre><code>@article{2018arXivUMAP,\n  author = {{McInnes}, L. and {Healy}, J. and {Melville}, J.},\n  title = \"{UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction}\",\n  journal = {ArXiv e-prints},\n  archivePrefix = \"arXiv\",\n  eprint = {1802.03426},\n  primaryClass = \"stat.ML\",\n  keywords = {Statistics - Machine Learning, Computer Science - Computational Geometry, Computer Science - Learning},\n  year = 2018,\n  month = feb,\n}\n</code></pre> <p>If you found the Nature Primer introduction useful please cite the following reference:</p> BibTeX<pre><code>@article{Healy2024,\n  author={Healy, John and McInnes, Leland},\n  title={Uniform manifold approximation and projection},\n  journal={Nature Reviews Methods Primers},\n  year={2024},\n  month={Nov},\n  day={21},\n  volume={4},\n  number={1},\n  pages={82},\n  abstract={Uniform manifold approximation and projection is a nonlinear dimension reduction method often used for visualizing data and as pre-processing for further machine-learning tasks such as clustering. In this Primer, we provide an introduction to the uniform manifold approximation and projection algorithm, the intuitions behind how it works, how best to apply it on data and how to interpret and understand results.},\n  issn={2662-8449},\n  doi={10.1038/s43586-024-00363-x},\n  url={https://doi.org/10.1038/s43586-024-00363-x}\n}\n</code></pre> <p>Additionally, if you use the densMAP algorithm in your work please cite the following reference:</p> BibTeX<pre><code>@article {NBC2020,\n  author = {Narayan, Ashwin and Berger, Bonnie and Cho, Hyunghoon},\n  title = {Assessing Single-Cell Transcriptomic Variability through Density-Preserving Data Visualization},\n  journal = {Nature Biotechnology},\n  year = {2021},\n  doi = {10.1038/s41587-020-00801-7},\n  publisher = {Springer Nature},\n  URL = {https://doi.org/10.1038/s41587-020-00801-7},\n  eprint = {https://www.biorxiv.org/content/early/2020/05/14/2020.05.12.077776.full.pdf},\n}\n</code></pre> <p>If you use the Parametric UMAP algorithm in your work please cite the following reference:</p> BibTeX<pre><code>@article {SMG2020,\n  author = {Sainburg, Tim and McInnes, Leland and Gentner, Timothy Q.},\n  title = {Parametric UMAP: learning embeddings with deep neural networks for representation and semi-supervised learning},\n  journal = {ArXiv e-prints},\n  archivePrefix = \"arXiv\",\n  eprint = {2009.12981},\n  primaryClass = \"stat.ML\",\n  keywords = {Statistics - Machine Learning, Computer Science - Computational Geometry, Computer Science - Learning},\n  year = 2020,\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>The umap package is 3-clause BSD licensed.</p> <p>We would like to note that the umap package makes heavy use of NumFOCUS sponsored projects, and would not be possible without their support of those projects, so please consider contributing to NumFOCUS.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are more than welcome! There are lots of opportunities for potential projects, so please get in touch if you would like to help out. Everything from code to notebooks to examples and documentation are all equally valuable so please don't feel you can't contribute. To contribute please fork the project make your changes and submit a pull request. We will do our best to work through any issues with you and get your code merged into the main branch.</p>"},{"location":"aligned_umap_basic_usage/","title":"How to use AlignedUMAP","text":"<p>It may happen that it would be beneficial to have different UMAP embeddings aligned with each other. There are several ways to go about doing this. One simple approach is to simply embed each dataset with UMAP independently and then solve for a <code>Procrustes transformation &lt;https://en.wikipedia.org/wiki/Procrustes_transformation&gt;</code>__ on shared points. An alternative approach is to embed the first dataset and then construct an initial embedding for the second dataset based on locations of shared points in the first embedding and then go from there. A third approach, which will provide better alignments in general, is to optimize both embeddings at the same time with some form of constraint as to how far shared points can take different locations in different embeddings during the optimization. This last option is possible, but is not easily tractable to implement yourself (unlike the first two options). To remedy this issue it has been implemented as a separate model class in <code>umap</code> called <code>AlignedUMAP</code>. The resulting class is quite flexible, but here we will walk through simple usage on some basic (and somewhat contrived) data just to demonstrate how to get it running on data.</p> Python<pre><code>import numpy as np\nimport sklearn.datasets\nimport umap\nimport umap.plot\nimport umap.utils as utils\nimport umap.aligned_umap\nimport matplotlib.pyplot as plt\n</code></pre> <p>For our demonstration we\u2019ll just use the pendigits dataset from sklearn.</p> Python<pre><code>digits = sklearn.datasets.load_digits()\n</code></pre> <p>To make a sequence of datasets with some shared points between each different dataset we\u2019ll first sort the data so we have some vaguely sensible progression. In this case we\u2019ll sort by the total amount of \u201cink\u201d in the handwritten digit. This isn\u2019t meant to be meaningful, it is merely meant to provide something useful to slicing into overlapping chunks that we will want to embed separately and yet keep aligned.</p> Python<pre><code>ordered_digits = digits.data[np.argsort(digits.data.sum(axis=1))]\nordered_target = digits.target[np.argsort(digits.data.sum(axis=1))]\nplt.matshow(ordered_digits[-1].reshape((8,8)))\n</code></pre> <p></p> <p>We can then divide up the dataset into slices of 400 samples, moving along in chunks of 150 to ensure that there are overlaps between consecutive slices. This will give us a list of ten different datasets that we can embed, with the goal being to ensure that the positions of points in the embeddings are relatively consistent.</p> Python<pre><code>slices = [ordered_digits[150 * i:min(ordered_digits.shape[0], 150 * i + 400)] for i in range(10)]\n</code></pre> <p>To ensure that consistency <code>AlignedUMAP</code> will need more information than just the datasets \u2013 we also need some information about how the datasets relate to one another. These take the form of dictionaries that relate the indices of one dataset to the indices of another. Currently <code>AlignedUMAP</code> only supports sequences of datasets with relations between each consecutive pair in the sequence. To construct the relations for this dataset we note that the last 250 samples of one dataset are going to be the same samples as the first 250 samples of the next dataset \u2013 this makes it easy to construct the dictionary: it is mapping</p> <p>::</p> <p>150 \u2192 0    151 \u2192 1    ...    398 \u2192 248    399 \u2192 249</p> <p>which we can construct easily using a dictionary comprehension. We will have the same relation between each consecutive pair, so to make the list of relations between pairs we can just duplicate the constructed relation the requisite number of times.</p> Python<pre><code>relation_dict = {i+150:i for i in range(400-150)}\nrelation_dicts = [relation_dict.copy() for i in range(len(slices) - 1)]\n</code></pre> <p>Note that while in this case the relation defines a map between identical samples in different datasets it can be much more general \u2013 see the politics example later for a case where the relation is constructed from external information (representatives names and states).</p> <p>Now that we have both a list of data slices and a list of relations between the consecutive pairs we can use the <code>AlignedUMAP</code> class to generate a list of embeddings. The <code>AlignedUMAP</code> class takes most of the parameters that UMAP accepts. The major difference is that the fit method requires a list of datasets, and a keyword argument <code>relations</code> that specifies the relation dictionaries between consecutive pairs of datasets. Other than that things are essentially push-button.</p> Python<pre><code>%%time\naligned_mapper = umap.AlignedUMAP().fit(slices, relations=relation_dicts)\n</code></pre> Text Only<pre><code>CPU times: user 57.4 s, sys: 8.43 s, total: 1min 5s\nWall time: 57.4 s\n</code></pre> <p>You will note that this took a non-trivial amount of time to run, despite being on the relatively small pendigits dataset. This is because we are completing 10 different UMAP embeddings at once, so on average we are taking about five seconds per embedding, which is more reasonable \u2013 the alignment does have overhead cost however.</p> <p>The next step is to look at the results. To ensure that the plots we produce have a consistent x and y axis we\u2019ll use a small function to compute a set of axis bounds for plotting.</p> Python<pre><code>def axis_bounds(embedding):\n    left, right = embedding.T[0].min(), embedding.T[0].max()\n    bottom, top = embedding.T[1].min(), embedding.T[1].max()\n    adj_h, adj_v = (right - left) * 0.1, (top - bottom) * 0.1\n    return [left - adj_h, right + adj_h, bottom - adj_v, top + adj_v]\n</code></pre> <p>Now it is just a matter of plotting the results in ten different scatter plots. We can do this most easily with matplotlib directly, setting up a grid of plots. Note that the progression proceeds by row then column, so read the progression as if you were reading a page of text (across, then down).</p> Python<pre><code>fig, axs = plt.subplots(5,2, figsize=(10, 20))\nax_bound = axis_bounds(np.vstack(aligned_mapper.embeddings_))\nfor i, ax in enumerate(axs.flatten()):\n    current_target = ordered_target[150 * i:min(ordered_target.shape[0], 150 * i + 400)]\n    ax.scatter(*aligned_mapper.embeddings_[i].T, s=2, c=current_target, cmap=\"Spectral\")\n    ax.axis(ax_bound)\n    ax.set(xticks=[], yticks=[])\nplt.tight_layout()\n</code></pre> <p></p> <p>So despite being different embeddings on different datasets, the clusters keep their general alignment \u2013 the top left plot and bottom right plot have the same rough positions for specific digit clusters. We can also, to a degree, see how the structure changes over the course of the different slices. Thus we are keeping the various embeddings aligned, but allowing the changes dictated by the differing structures of each different slice of data.</p>"},{"location":"aligned_umap_basic_usage/#online-updating-of-aligned-embeddings","title":"Online updating of aligned embeddings","text":"<p>It may be the case that we have incoming temporal data and would like to have embeddings of time-windows that, ideally, align with the embeddings of prior time-windows. As long as we overlap the time-windows we use to allow for relations between time windows then this is possible \u2013 except that the previous code required all the time-windows to be input at once for fitting. We would instead like to train an initial model and then update it as we go. This is possible via the <code>update</code> method which we\u2019ll demonstrate below.</p> <p>First we need to fit a base <code>AlignedUMAP</code> model; we\u2019ll use the first two slices and the first relation dict to do so.</p> Python<pre><code>%%time\nupdating_mapper = umap.AlignedUMAP().fit(slices[:2], relations=relation_dicts[:1])\n</code></pre> Text Only<pre><code>CPU times: user 9.32 s, sys: 1.47 s, total: 10.8 s\nWall time: 9.17 s\n</code></pre> <p>Note that this is fairly quick, since we are only fitting two slices. Given the trained model the update method requires a new slice of data to add, along with a relation dictionary (passed in with the <code>relations</code> keyword argument as with <code>fit</code>). This will append a new embedding to the <code>embeddings_</code> attribute of the model for the new data, aligned with what has been seen so far.</p> Python<pre><code>for i in range(2,len(slices)):\n    %time updating_mapper.update(slices[i], relations={v:k for k,v in relation_dicts[i-1].items()})\n</code></pre> Text Only<pre><code>CPU times: user 7.78 s, sys: 1.15 s, total: 8.93 s\nWall time: 7.92 s\nCPU times: user 6.64 s, sys: 1.17 s, total: 7.81 s\nWall time: 6.6 s\nCPU times: user 6.94 s, sys: 1.17 s, total: 8.11 s\nWall time: 6.81 s\nCPU times: user 6.45 s, sys: 1.51 s, total: 7.96 s\nWall time: 6.45 s\nCPU times: user 7.44 s, sys: 1.32 s, total: 8.76 s\nWall time: 7.16 s\nCPU times: user 7.68 s, sys: 1.73 s, total: 9.41 s\nWall time: 7.59 s\nCPU times: user 7.88 s, sys: 1.65 s, total: 9.54 s\nWall time: 7.39 s\nCPU times: user 7.82 s, sys: 1.98 s, total: 9.8 s\nWall time: 7.7 s\n</code></pre> <p>Note that each new slice takes a relatively short period of time, as we might hope. The downside of this, as you can imagine, is that we have no \u201cforward\u201d relations \u2013 the windows over slices only look backward. This means the results are less good, but we are trading that for the ability to quickly and easily update as we go.</p> <p>We can look at how we did using essentially the same code as before.</p> Python<pre><code>fig, axs = plt.subplots(5,2, figsize=(10, 20))\nax_bound = axis_bounds(np.vstack(updating_mapper.embeddings_))\nfor i, ax in enumerate(axs.flatten()):\n    current_target = ordered_target[150 * i:min(ordered_target.shape[0], 150 * i + 400)]\n    ax.scatter(*updating_mapper.embeddings_[i].T, s=2, c=current_target, cmap=\"Spectral\")\n    ax.axis(ax_bound)\n    ax.set(xticks=[], yticks=[])\nplt.tight_layout()\n</code></pre> <p></p> <p>We see that the alignment is indeed working, so new slices remain comparable with previously trained slices. As noted the overall alignments and progression is not as nice as the previous version, but it does have the significant benefit of allowing an update as you go approach.</p> <p>Note that right now this model keeps all the previous data, so it will only really work in a batch streaming approach where occasionally a fresh model is trained, dropping some of the historical data before continuing with updates.</p>"},{"location":"aligned_umap_basic_usage/#aligning-varying-parameters","title":"Aligning varying parameters","text":"<p>It is possible to align UMAP embedding that vary in the parameters used instead of the data. To demonstrate how this can work we\u2019ll continue to use the pendigits dataset, but instead of slicing the data as we did before, we\u2019ll use the full dataset. That means that our relations between datasets are simply constant relations. We can construct those ahead of time:</p> Python<pre><code>constant_dict = {i:i for i in range(digits.data.shape[0])}\nconstant_relations = [constant_dict for i in range(9)]\n</code></pre> <p>To run AlignedUMAP over a range of parameters you simply need to pass in a list of the sequence of parameters you wish to use. You can do this for several different parameters \u2013 just ensure that all the lists are the same length! In this case we\u2019ll try looking at how the embeddings change if we change <code>n_neighbors</code> and <code>min_dist</code>. This means that when we create the AlignedUMAP object we pass a list, instead of a single value, to each of those parameters. To make the visualization a little more interesting we\u2019ll also vary some of the alignment parameters (there are only two of major consequence). Specifically we\u2019ll adjust the <code>alignment_window_size</code>, which controls how far forward and backward across the datasets we look when doing alignment, and the <code>alignment_regularisation</code> which controls how heavily we weight the alignment aspect versus the UMAP layout. Larger values of <code>alignment_regularisation</code> will work harder to keep points aligned across embeddings (at the cost of the embedding quality at each slice), while smaller values will allow the optimisation to focus more on the individual embeddings and put less emphasis on aligning the embeddings with each other.</p> <p>Given a model we can then fit it. As before we need to hand it a list of datasets, and a list of relations. Since we are using the same data each time (and varying the parameters) we can just repeat the full pendigits dataset. Note that the number of datasets needs to match the number of parameter values being used. The same goes for the number of relations (one less than the number of parameter values).</p> Python<pre><code>neighbors_mapper = umap.AlignedUMAP(\n    n_neighbors=[3,4,5,7,11,16,22,29,37,45,54],\n    min_dist=[0.01,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45],\n    alignment_window_size=2,\n    alignment_regularisation=1e-3,\n).fit(\n    [digits.data for i in range(10)], relations=constant_relations\n)\n</code></pre> <p>As before we can look at the results by plotting each of the embeddings.</p> Python<pre><code>fig, axs = plt.subplots(5,2, figsize=(10, 20))\nax_bound = axis_bounds(np.vstack(neighbors_mapper.embeddings_))\nfor i, ax in enumerate(axs.flatten()):\n    ax.scatter(*neighbors_mapper.embeddings_[i].T, s=2, c=digits.target, cmap=\"Spectral\")\n    ax.axis(ax_bound)\n    ax.set(xticks=[], yticks=[])\nplt.tight_layout()\n</code></pre> <p></p> <p>To get a better feel for the evolution of the embedding over the change in parameter values we can plot the data in three dimensions, with the third dimension being the parameter value chosen. To better show how data points in the embedding move with respect to the changing parameters we can plot them not as points, but as curves connecting the same point in each sequential embedding. For three dimensional plots like this we\u2019ll make use of the plotly plotting library.</p> Python<pre><code>import plotly.graph_objects as go\nimport plotly.express as px\nimport pandas as pd\n</code></pre> <p>The first thing we\u2019ll have to do is wrangle the data into a suitable format for plotly. That\u2019s the reason we loaded up pandas as well \u2013 plotly likes dataframes. This involves stacking all the embeddings together, and then assigning an extra <code>z</code> value according to which embedding we are in. For the purposes of visualization we\u2019ll just have a linear scale from 0 to 1 of the appropriate length for the z coordinates.</p> Python<pre><code>n_embeddings = len(neighbors_mapper.embeddings_)\nes = neighbors_mapper.embeddings_\nembedding_df = pd.DataFrame(np.vstack(es), columns=('x', 'y'))\nembedding_df['z'] = np.repeat(np.linspace(0, 1.0, n_embeddings), es[0].shape[0])\nembedding_df['id'] = np.tile(np.arange(es[0].shape[0]), n_embeddings)\nembedding_df['digit'] = np.tile(digits.target, n_embeddings)\n</code></pre> <p>The next thing we can do to improve the visualization is to smooth out the curves rather than leaving them as piecewise linear lines. To to this we can use the <code>scipy.interpolate</code> functionality to create smooth cubic splines that pass through all the points of the curve we wish to create.</p> Python<pre><code>import scipy.interpolate\n</code></pre> <p>The interpolate module has a function <code>interp1d</code> that generates a (vector of) smooth function given a one dimensional set of datapoints that it needs to pass through. We can generate separate functions for the x and y coordinates for each pendigit sample, allowing us to generate smooth curves in three dimensions.</p> Python<pre><code>fx = scipy.interpolate.interp1d(\n    embedding_df.z[embedding_df.id == 0], embedding_df.x.values.reshape(n_embeddings, digits.data.shape[0]).T, kind=\"cubic\"\n)\nfy = scipy.interpolate.interp1d(\n    embedding_df.z[embedding_df.id == 0], embedding_df.y.values.reshape(n_embeddings, digits.data.shape[0]).T, kind=\"cubic\"\n)\nz = np.linspace(0, 1.0, 100)\n</code></pre> <p>With that in hand it is just a matter of plotting all the curves. In plotly parlance each curve is a \u201ctrace\u201d and we generate each one separately (along with a suitable colour given by the digit the sample represents). We then add all the traces to a figure, and display the figure.</p> Python<pre><code>palette = px.colors.diverging.Spectral\ninterpolated_traces = [fx(z), fy(z)]\ntraces = [\n    go.Scatter3d(\n        x=interpolated_traces[0][i],\n        y=interpolated_traces[1][i],\n        z=z*3.0,\n        mode=\"lines\",\n        line=dict(\n            color=palette[digits.target[i]],\n            width=3.0\n        ),\n        opacity=1.0,\n    )\n    for i in range(digits.data.shape[0])\n]\nfig = go.Figure(data=traces)\nfig.update_layout(\n    width=800,\n    height=700,\n    autosize=False,\n    showlegend=False,\n)\nfig.show()\n</code></pre> <p></p> <p>Since it is tricky to get the interactive plotly figure embedded in documentation we have a static image here, but if you run this yourself you will have a fully interactive view of the data.</p> <p>Alternatively, we can visualize the third dimension as an evolution of the embeddings through time by rendering each z-slice as a frame in an animated GIF. To do this, we'll first need to import some notebook display tools and matplotlib's animation module.</p> Python<pre><code>from IPython.display import display, Image, HTML\nfrom matplotlib import animation\n</code></pre> <p>Next, we'll create a new figure, initialize a blank scatter plot, then use <code>FuncAnimation</code> to update the point positions (called \"offsets\") one frame at a time.</p> Python<pre><code>fig = plt.figure(figsize=(4, 4), dpi=150)\nax = fig.add_subplot(1, 1, 1)\n\nscat = ax.scatter([], [], s=2)\nscat.set_array(digits.target)\nscat.set_cmap('Spectral')\ntext = ax.text(ax_bound[0] + 0.5, ax_bound[2] + 0.5, '')\nax.axis(ax_bound)\nax.set(xticks=[], yticks=[])\nplt.tight_layout()\n\noffsets = np.array(interpolated_traces).T\nnum_frames = offsets.shape[0]\n\ndef animate(i):\n    scat.set_offsets(offsets[i])\n    text.set_text(f'Frame {i}')\n    return scat\n\nanim = animation.FuncAnimation(\n    fig,\n    init_func=None,\n    func=animate,\n    frames=num_frames,\n    interval=40)\n</code></pre> <p>Then we can save the animation as a GIF and close our animation. Depending on your machine, you may need to change which writer the save method uses.</p> Python<pre><code>anim.save(\"aligned_umap_pendigits_anim.gif\", writer=\"pillow\")\nplt.close(anim._fig)\n</code></pre> <p>Finally, we can read in our rendered GIF and display it in the notebook.</p> Python<pre><code>with open(\"aligned_umap_pendigits_anim.gif\", \"rb\") as f:\n    display(Image(f.read()))\n</code></pre> <p></p>"},{"location":"aligned_umap_politics_demo/","title":"AlignedUMAP for Time Varying Data","text":"<p>It is not uncommon to have datasets that can be partitioned into segments, often with respect to time, where we want to understand not only the structure of each segment, but how that structure changes over the different segments. An example of this is the relative political leanings of the US congress over time. In determining the relative political leanings we can look at the representatives voting record on roll call votes, with the presumption that representatives with similar political principles will have similar voting records. We can, of course, look at such data for any given congress, but since representatives are commonly re-elected we can also consider how their relative position in congress changes with time \u2013 an ideal use case for AlignedUMAP.</p> <p>First we\u2019ll need a selection of libraries. Aside from UMAP we will need to do a little bit of data wrangling; for that we\u2019ll need pandas, and also for matching up names of representatives we\u2019ll make use of the library <code>fuzzywuzzy</code> which provides easy to use fuzzy string matching.</p> Python<pre><code>import umap\nimport umap.utils as utils\nimport umap.aligned_umap\nimport sklearn.decomposition\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom fuzzywuzzy import fuzz, process\nimport re\n</code></pre> Python<pre><code>sns.set(style=\"darkgrid\", color_codes=True)\n</code></pre> <p>Next we\u2019ll need to voting records for the representatives, along with the associated metadata from the roll call vote records. You can obtain the data https://clerk.house.gov; a notebook demonstrating how to pull down the data and parse it into the csv files used here is available here.</p>"},{"location":"aligned_umap_politics_demo/#processing-congressional-voting-records","title":"Processing Congressional Voting Records","text":"<p>The voting records provide a row for each representative with a -1 for \u201cNo\u201d, 0 for \u201cPresent\u201d or \u201cNot Voting\u201d, and 1 for \u201cAye\u201d. A separate csv file contains the raw data of all the votes with a row for each legislators vote on each roll-call item. We really just need some metadata \u2013 which state they represent and the party they represent so we can decorate the results with this kind of information later. For that we just need to extra the names, states, and parties for each year. We can grab those columns and then drop duplicates. A catch: the party is very occasionally entered incorrectly, and occasionally representatives switch parties, making duplicated rows. We\u2019ll just take the first entry of such duplciates for now.</p> Python<pre><code>votes = [pd.read_csv(f\"house_votes/{year}_voting_record.csv\", index_col=0).sort_index()\n         for year in range(1990,2021)]\nmetadata = [pd.read_csv(\n    f\"house_votes/{year}_full.csv\",\n    index_col=0\n)[[\"legislator\", \"state\", \"party\"]].drop_duplicates([\"legislator\", \"state\"]).sort_values('legislator')\n            for year in range(1990,2021)]\n</code></pre> <p>Let\u2019s take a look at the voting record for a single year to see what sort of data we are looking at:</p> Python<pre><code>votes[5]\n</code></pre> 104-1st-1 104-1st-10 104-1st-100 104-1st-101 104-1st-102 104-1st-103 104-1st-104 104-1st-105 104-1st-106 104-1st-107 ... 104-1st-90 104-1st-91 104-1st-92 104-1st-93 104-1st-94 104-1st-95 104-1st-96 104-1st-97 104-1st-98 104-1st-99 legislator Abercrombie 0.0 1.0 -1.0 -1.0 -1.0 -1.0 1.0 1.0 -1.0 1.0 ... -1.0 -1.0 1.0 -1.0 1.0 -1.0 -1.0 1.0 1.0 1.0 Ackerman 0.0 1.0 -1.0 1.0 -1.0 -1.0 1.0 1.0 -1.0 1.0 ... 1.0 -1.0 -1.0 1.0 1.0 -1.0 -1.0 1.0 1.0 1.0 Allard 0.0 1.0 1.0 1.0 -1.0 1.0 -1.0 -1.0 1.0 -1.0 ... -1.0 -1.0 -1.0 -1.0 1.0 1.0 1.0 1.0 0.0 -1.0 Andrews 0.0 1.0 0.0 -1.0 -1.0 1.0 -1.0 0.0 0.0 0.0 ... -1.0 1.0 -1.0 -1.0 1.0 1.0 -1.0 1.0 -1.0 -1.0 Archer 0.0 1.0 1.0 -1.0 -1.0 1.0 -1.0 -1.0 1.0 -1.0 ... -1.0 -1.0 -1.0 -1.0 -1.0 1.0 1.0 1.0 -1.0 0.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... Young (AK) 0.0 1.0 1.0 1.0 -1.0 1.0 -1.0 -1.0 1.0 -1.0 ... -1.0 -1.0 -1.0 -1.0 -1.0 1.0 1.0 1.0 -1.0 -1.0 Young (FL) 0.0 1.0 1.0 -1.0 -1.0 1.0 -1.0 -1.0 1.0 -1.0 ... -1.0 -1.0 -1.0 -1.0 -1.0 1.0 1.0 1.0 -1.0 -1.0 Zeliff 0.0 1.0 1.0 -1.0 -1.0 1.0 -1.0 -1.0 1.0 -1.0 ... -1.0 -1.0 -1.0 -1.0 -1.0 1.0 1.0 1.0 -1.0 -1.0 Zimmer 0.0 1.0 1.0 -1.0 -1.0 1.0 -1.0 -1.0 1.0 -1.0 ... -1.0 1.0 -1.0 -1.0 -1.0 1.0 1.0 1.0 -1.0 -1.0 de la Garza 0.0 1.0 1.0 1.0 -1.0 1.0 1.0 1.0 -1.0 1.0 ... 0.0 -1.0 -1.0 1.0 1.0 -1.0 1.0 1.0 -1.0 1.0 <p>438 rows \u00d7 885 columns</p> <p>We can examine the associated metadata for the same year.</p> Python<pre><code>metadata[5]\n</code></pre> legislator state party 0 Abercrombie HI D 1 Ackerman NY D 2 Allard CO R 3 Andrews NJ D 4 Archer TX R ... ... ... ... 430 Young (AK) AK R 431 Young (FL) FL R 432 Zeliff NH R 433 Zimmer NJ R 89 de la Garza TX D <p>438 rows \u00d7 3 columns</p> <p>You may note that sometimes representatives names list a state in parenthesis afterwards. This is to provide disambiguation for representatives that happen to have the last name. This actually complicates matters for us since the disambiguation is only applied in those cases where there is a name collision in that sitting of congress. That means that for several years a representative may have simply their last name, but then switch to being disambiguated, before potentially switching back again. This would make it much harder to consistently treck representatives over their entire career in congress. To fix this up we\u2019ll simply re-index by a unique representative ID that has their last name, party, and state all listed over all the voting dataframes. We\u2019ll need a function to generate those from the metadata, and then we\u2019ll need to apply it to all the records. Importantly we\u2019ll have to finesse those situations where representatives are listed twice (under un-ambiguous and disambiguated names) with some groupby tricks.</p> Python<pre><code>def unique_legislator(row):\n    name, state, party = row.legislator, row.state, row.party\n    # Strip of disambiguating state designators\n    if re.search(r'(\\w+) \\([A-Z]{2}\\)', name) is not None:\n        name = name[:-5]\n    return f\"{name} ({party}, {state})\"\n</code></pre> Python<pre><code>for i, _ in enumerate(votes):\n    votes[i].index = pd.Index(metadata[i].apply(unique_legislator, axis=1), name=\"legislator_index\")\n    votes[i] = votes[i].groupby(level=0).sum()\n    metadata[i].index = pd.Index(metadata[i].apply(unique_legislator, axis=1), name=\"legislator_index\")\n    metadata[i] = metadata[i].groupby(level=0).first()\n</code></pre> <p>Now that we have the data at least a little wrangled into order there is the question of ensuring some degree of continuity fore representatives. To make this a little easier we\u2019ll use voting records over four year spans instead of over single years. Equally importantly we\u2019ll do this in a sliding window fashion so that we consider the record for 1990-1994 and then the record for 1991-1995 and so on. By overlapping the windows in this way we can ensure a little greater continuity of political stance through the years. To make this happen we just have to merge data frames in a sliding set of pairs, and then merge the pairs via the same approach:</p> Python<pre><code>votes = [\n    pd.merge(\n        v1, v2, how=\"outer\", on=\"legislator_index\"\n    ).fillna(0.0).sort_index()\n    for v1, v2 in zip(votes[:-1], votes[1:])\n] + votes[-1:]\n\nmetadata = [\n    pd.concat([m1, m2]).groupby(\"legislator_index\").first().sort_index()\n    for m1, m2 in zip(metadata[:-1], metadata[1:])\n] + metadata[-1:]\n</code></pre> <p>That\u2019s the pairs of years; now we merge these pairwise to get sets of four years worth of votes.</p> Python<pre><code>votes = [\n    pd.merge(\n        v1, v2, how=\"outer\", on=\"legislator_index\"\n    ).fillna(0.0).sort_index()\n    for v1, v2 in zip(votes[:-1], votes[1:])\n] + votes[-1:]\n\nmetadata = [\n    pd.concat([m1, m2]).groupby(level=0).first().sort_index()\n    for m1, m2 in zip(metadata[:-1], metadata[1:])\n] + metadata[-1:]\n</code></pre>"},{"location":"aligned_umap_politics_demo/#applying-alignedumap","title":"Applying AlignedUMAP","text":"<p>To make use of AlignedUMAP we need to generate relations between consecutive dataset slices. In this case that means we need to have a relation describing row from one four year slice corresponds to a row from the following four year slice for the same representative. For AlignedUMAP to work this should be formatted as a list of dictionaries; each dictionary gives a mapping from indices of one slice to indices of the next. Importantly this mapping can be partial \u2013 it only has to relate indices for which there is a match between the two slices.</p> <p>The vote dataframes that we are using for slices are already indexed with unique identifiers for representatives, so to make relations we simply have to match them up, creating a dictionary of indices from one to the other. In practice we can do this relatively efficiently by using pandas to merge dataframes on the pandas indexes of the two vote dataframes with the data being simply the numeric indices of the rows. The resulting dictionary is then just the dictionary of pairs given by the inner join.</p> Python<pre><code>def make_relation(from_df, to_df):\n    left = pd.DataFrame(data=np.arange(len(from_df)), index=from_df.index)\n    right = pd.DataFrame(data=np.arange(len(to_df)), index=to_df.index)\n    merge = pd.merge(left, right, left_index=True, right_index=True)\n    return dict(merge.values)\n</code></pre> <p>With a function for relation creation in place we simply need to apply it to each consecutive pair of vote dataframes.</p> Python<pre><code>relations = [make_relation(x,y) for x, y in zip(votes[:-1], votes[1:])]\n</code></pre> <p>If you are still unsure of what these relations are it might be beneficial to look at a few of the dictionaries, along with the corresponding pairs of vote dataframes. Here is (part of) the first relation dictionary:</p> Python<pre><code>relations[0]\n</code></pre> Text Only<pre><code>{0: 0,\n 1: 1,\n 3: 2,\n 4: 3,\n 5: 4,\n 6: 5,\n 7: 6,\n 8: 7,\n 9: 8,\n 10: 9,\n 11: 10,\n 12: 11,\n 13: 12,\n 14: 13,\n 15: 14,\n ...\n 475: 547,\n 476: 549,\n 477: 550,\n 478: 552,\n 479: 553,\n 480: 554,\n 481: 555,\n 482: 556,\n 483: 557,\n 484: 559}\n</code></pre> <p>Now we are finally in a position to run AlignedUMAP. Most of the standard UMAP parameters are available for use, including choosing a metric and a number of neighbors. Here we will also make use of the extra AlignedUMAP parameters <code>alignment_regularisation</code> and <code>alignment_window_size</code>. The first is a value that weights how important retaining alignment is. Typically the value is much smaller than this (the default is 0.01), but given the relatively high volatility in voting records we are going to increase it here. The second parameter, <code>alignment_window_size</code> determines how far out on either side AlignedUMAP will look when aligning embeddings \u2013 even though the relations are specified only between consecutive slices it will chain them together to construct relations reaching further. In this case we\u2019ll have it look as far out as 5 slices either side.</p> Python<pre><code>%%time\naligned_mapper = umap.aligned_umap.AlignedUMAP(\n    metric=\"cosine\",\n    n_neighbors=20,\n    alignment_regularisation=0.1,\n    alignment_window_size=5,\n    n_epochs=200,\n    random_state=42,\n).fit(votes, relations=relations)\nembeddings = aligned_mapper.embeddings_\n</code></pre> Text Only<pre><code>CPU times: user 6min 7s, sys: 30.6 s, total: 6min 37s\nWall time: 5min 57s\n</code></pre>"},{"location":"aligned_umap_politics_demo/#visualizing-the-results","title":"Visualizing the Results","text":"<p>Now we need to plot the data somehow. To make the visualization interesting it would be beneficial to have some colour variation \u2013 ideally showing a different view of the relative political stance. For that we want to attempt to get an idea of the position of each candidate from an alternative source. To do this we can try to extract the vote margin that the representative won by. The catch here is that while the election data can be collected and processed, the names don\u2019t match perfectly as they come from a different source. That means we need to do our best to get a name match for each candidate. We\u2019ll use fuzzy string matching restricted to the relevant year and state to try to get a good match. A notebook providing details for obtaining and processing the election winners data can be found here.</p> Python<pre><code>election_winners = pd.read_csv('election_winners_1976-2018.csv', index_col=0)\nelection_winners.head()\n</code></pre> year state district winner party winning_ratio 0 1976 AK 0 Don Young republican 0.289986 0 1976 AL 1 Jack Edwards republican 0.374808 0 1976 AL 2 William L. \\\\\"Bill\\\"\\\" Dickinson\" republican 0.423953 0 1976 AL 3 Bill Nichols democrat 1.000000 0 1976 AL 4 Tom Bevill democrat 0.803825 <p>Now we need to simply go through the metadata and fill it out with the extra information we can glean from the election winners data. Since we can\u2019t do exact name matching (the data for both is somewhat messy when it comes to text fields like names) we can\u2019t simply perform a join, but must instead process things year by year and representative by representative, finding the best string match on name that we can for the given year and state election. In practice we are undoubtedly going to get some of these wrong, and if the goal was a rigorous analysis based on this data a lot more care would need to be taken. Since this is just a demonstration and we\u2019ll only be using this extra information as a colour channel in plots we can excuise a few errors here and there from in-exact data processing.</p> Python<pre><code>n_name_misses = 0\nfor year, df in enumerate(metadata, 1990):\n    df[\"partisan_lean\"] = 0.5\n    df[\"district\"] = np.full(len(df), -1, dtype=np.int8)\n    for idx, (loc, row) in enumerate(df.iterrows()):\n        name, state, party = row.legislator, row.state, row.party\n        # Strip of disambiguating state designators\n        if re.search(r'(\\w+) \\([A-Z]{2}\\)', name) is not None:\n            name = name[:-5]\n        # Get a party designator matching the election_winners data\n        party = \"republican\" if party == \"R\" else \"democrat\"\n        # Restrict to the right state and time-frame\n        state_election_winners = election_winners[(election_winners.state == state)\n                                                  &amp; (election_winners.year &lt;= year + 4)\n                                                  &amp; (election_winners.year &gt;= year - 4)]\n        # Try to match a name; and fail \"gracefully\"\n        try:\n            matched_name = process.extractOne(\n                name,\n                state_election_winners.winner.tolist(),\n                scorer=fuzz.partial_token_sort_ratio,\n                score_cutoff=50,\n            )\n        except:\n            matched_name = None\n\n        # If we got a unique match, get the election data\n        if matched_name is not None:\n            winner = state_election_winners[state_election_winners.winner == matched_name[0]]\n        else:\n            winner = []\n\n        # We either have none, one, or *several* match elections. Take a best guess.\n        if len(winner) &lt; 1:\n            df.loc[loc, [\"partisan_lean\"]] = 0.25 if party == \"republican\" else 0.75\n            n_name_misses += 1\n        elif len(winner) &gt; 1:\n            df.iloc[idx, 4] = int(winner.district.values[-1])\n            df.iloc[idx, 3] = float(winner.winning_ratio.values[-1])\n        else:\n            df.iloc[idx, 4] = int(winner.district.values)\n            df.iloc[idx, 3] = float(winner.winning_ratio.values[0])\n\nprint(f\"Failed to match a name {n_name_misses} times\")\n</code></pre> Text Only<pre><code>Failed to match a name 100 times\n</code></pre> <p>Now that we have the relative partisan leanings based on district election margins we can color the plot. We can obviously label the plot with the representatives names. The last remaining catch (when using matplotlib for the plotting) is the get the plot bounds (since we will be placing text markers directly into the plot, and thus not autogenerating bounds). This is a simple enough matter of computing some bounds as an adjustment a little outside the data limits.</p> Python<pre><code>def axis_bounds(embedding):\n    left = embedding.T[0].min()\n    right = embedding.T[0].max()\n    bottom = embedding.T[1].min()\n    top = embedding.T[1].max()\n    width = right - left\n    height = top - bottom\n    adj_h = width * 0.1\n    adj_v = height * 0.05\n    return [left - adj_h, right + adj_h, bottom - adj_v, top + adj_v]\n</code></pre> <p>Now for the plot. Let\u2019s pick a random time slice (you are welcome to try others) and draw the representatives names in their embedded locations for that slice, coloured by their relative election victory margin.</p> Python<pre><code>fig, ax = plt.subplots(figsize=(12,12))\ne = 5\nax.axis(axis_bounds(embeddings[e]))\nax.set_aspect('equal')\nfor i in range(embeddings[e].shape[0]):\n    ax.text(embeddings[e][i, 0],\n            embeddings[e][i, 1],\n            metadata[e].index.values[i],\n            color=plt.cm.RdBu(np.float32(metadata[e][\"partisan_lean\"].values[i])),\n            fontsize=8,\n            horizontalalignment='center',\n            verticalalignment='center',\n           )\n</code></pre> <p></p> <p>This gives a good idea of the layout in a single time slices, and by plotting different time slices we can get some idea of how things have evolved. We can go further, however, by plotting a representative as curve through time as their relative political position in congress changes. For that we will need a 3D plot \u2013 we need both the UMAP x and y coordinates, as well as a third coordinate giving the year. I found this easiest to do in plotly, so let\u2019s import that. To make nice smooth curves through time we will also import the <code>scipy.interpolate</code> module which will let is interpolate a smooth curve from the discrete positions that a representatives appears in over time.</p> Python<pre><code>import plotly.graph_objects as go\nimport scipy.interpolate\n</code></pre> <p>Wrangling the data into shape for this is the next step; first let\u2019s get everything in a single dataframe that we can extract relevant data from on an as-needed basis.</p> Python<pre><code>df = pd.DataFrame(np.vstack(embeddings), columns=('x', 'y'))\ndf['z'] = np.concatenate([[year] * len(embeddings[i]) for i, year in enumerate(range(1990, 2021))])\ndf['representative_id'] = np.concatenate([v.index for v in votes])\ndf['partisan_lean'] = np.concatenate([m[\"partisan_lean\"].values for m in metadata])\n</code></pre> <p>Next we\u2019ll need that interpolation of the curve for a given representative. We\u2019ll write a function to handle that as there is a little bit of case-based logic that makes it non-trivial. We are going to get handed year data and want to interpolate the UMAP x and y coordinates for a single representative.</p> <p>The first major catch is that many representatives don\u2019t have a single contiguous block of years for which they were in congress: they were elected for several years, missed re-election, and then came back to congress several years later (possibly in another district). Each such block of contiguous years needs to be a separate path, and we shouldn\u2019t connect them. We therefore need some logic to find the contiguous blocks and generate smooth paths for each of them.</p> <p>Another catch is that some representatives have only been in office for a year or two (special elections and so forth) and we can\u2019t do a cubic spline interpolation for that; we can devolve to linear interpolation or quadratic splines for those cases, so simply add the point itself for the odd single year cases.</p> <p>With those issues in hand we can then simply use the scipy <code>interp1d</code> function to generate smooth curves through the points.</p> Python<pre><code>INTERP_KIND = {2:\"linear\", 3:\"quadratic\", 4:\"cubic\"}\n\ndef interpolate_paths(z, x, y, c, rep_id):\n    consecutive_year_blocks = np.where(np.diff(z) != 1)[0] + 1\n    z_blocks = np.split(z, consecutive_year_blocks)\n    x_blocks = np.split(x, consecutive_year_blocks)\n    y_blocks = np.split(y, consecutive_year_blocks)\n    c_blocks = np.split(c, consecutive_year_blocks)\n\n    paths = []\n\n    for block_idx, zs in enumerate(z_blocks):\n\n        text = f\"{rep_id} -- partisan_lean: {np.mean(c_blocks[block_idx]):.2f}\"\n\n        if len(zs) &gt; 1:\n            kind = INTERP_KIND.get(len(zs), \"cubic\")\n        else:\n            paths.append(\n                (zs, x_blocks[block_idx], y_blocks[block_idx], c_blocks[block_idx], text)\n            )\n            continue\n\n        z = np.linspace(np.min(zs), np.max(zs), 100)\n        x = scipy.interpolate.interp1d(zs, x_blocks[block_idx], kind=kind)(z)\n        y = scipy.interpolate.interp1d(zs, y_blocks[block_idx], kind=kind)(z)\n        c = scipy.interpolate.interp1d(zs, c_blocks[block_idx], kind=\"linear\")(z)\n\n        paths.append((z, x, y, c, text))\n\n    return paths\n</code></pre> <p>And now we can use plotly to draw the resulting curves. For plotly we use the <code>Scatter3D</code> method, which supports a \u201clines\u201d mode that can draw curves in 3D space. We can colour the curves by the partisan lean score we derived from the election data \u2013 in fact the colour can vary through the trace as the election margins vary. Since this is a plotly plot it is interactive, so you can rotate it around and view it from all angles.</p> <p>Unfortunately the interactive plotly plot does not embed into the documentation well, so we present here a static image. If you run this yourself, however, you will get the interactive version.</p> Python<pre><code>traces = []\nfor rep in df.representative_id.unique():\n    z = df.z[df.representative_id == rep].values\n    x = df.x[df.representative_id == rep].values\n    y = df.y[df.representative_id == rep].values\n    c = df.partisan_lean[df.representative_id == rep]\n\n    for z, x, y, c, text in interpolate_paths(z, x, y, c, rep):\n        trace = go.Scatter3d(\n            x=x, y=z, z=y,\n            mode=\"lines\",\n            hovertext=text,\n            hoverinfo=\"text\",\n            line=dict(\n                color=c,\n                cmin=0.0,\n                cmid=0.5,\n                cmax=1.0,\n                cauto=False,\n                colorscale=\"RdBu\",\n                colorbar=dict(),\n                width=2.5,\n            ),\n            opacity=1.0,\n        )\n        traces.append(trace)\n\nfig = go.Figure(data=traces)\nfig.update_layout(\n    width=800,\n    height=600,\n    scene=dict(\n        aspectratio = dict( x=0.5, y=1.25, z=0.5 ),\n        yaxis_title=\"Year\",\n        xaxis_title=\"UMAP-X\",\n        zaxis_title=\"UMAP-Y\",\n    ),\n    scene_camera=dict(eye=dict( x=0.5, y=0.8, z=0.75 )),\n    autosize=False,\n    showlegend=False,\n)\nfig_widget = go.FigureWidget(fig)\nfig_widget\n</code></pre> <p></p> <p>.. View html file</p> <p>This concludes our exploration for now.</p>"},{"location":"api/","title":"UMAP API Guide","text":"<p>UMAP has only two classes, <code>UMAP</code>, and <code>ParametricUMAP</code>, which inherits from it.</p>"},{"location":"api/#umap","title":"UMAP","text":""},{"location":"api/#umapumap_umap","title":"umap.umap_.UMAP","text":"<p>API Reference: <code>umap.umap_.UMAP</code></p> <p>This is an auto-generated API reference. See the Python docstrings for details.</p>"},{"location":"api/#parametricumap","title":"ParametricUMAP","text":""},{"location":"api/#umapparametric_umapparametricumap","title":"umap.parametric_umap.ParametricUMAP","text":"<p>API Reference: <code>umap.parametric_umap.ParametricUMAP</code></p> <p>This is an auto-generated API reference. See the Python docstrings for details.</p> <p>A number of internal functions can also be accessed separately for more fine tuned work.</p>"},{"location":"api/#useful-functions","title":"Useful Functions","text":""},{"location":"api/#module-umapumap_","title":"Module: umap.umap_","text":"<p>API Reference: Module <code>umap.umap_</code></p> <p>This is an auto-generated API reference. See the Python docstrings for details.</p>"},{"location":"basic_usage/","title":"How to Use UMAP","text":"<p>UMAP is a general purpose manifold learning and dimension reduction algorithm. It is designed to be compatible with scikit-learn, making use of the same API and able to be added to sklearn pipelines. If you are already familiar with sklearn you should be able to use UMAP as a drop in replacement for t-SNE and other dimension reduction classes. If you are not so familiar with sklearn this tutorial will step you through the basics of using UMAP to transform and visualise data.</p> <p>First we'll need to import a bunch of useful tools. We will need numpy obviously, but we'll use some of the datasets available in sklearn, as well as the <code>train_test_split</code> function to divide up data. Finally we'll need some plotting tools (matplotlib and seaborn) to help us visualise the results of UMAP, and pandas to make that a little easier.</p> Python<pre><code>import numpy as np\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n%matplotlib inline\n</code></pre> Python<pre><code>sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)})\n</code></pre>"},{"location":"basic_usage/#penguin-data","title":"Penguin data","text":"<p>The next step is to get some data to work with. To ease us into things we'll start with the <code>penguin dataset &lt;https://github.com/allisonhorst/penguins&gt;</code>__. It isn't very representative of what real data would look like, but it is small both in number of points and number of features, and will let us get an idea of what the dimension reduction is doing.</p> Python<pre><code>penguins = pd.read_csv(\"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/c19a904462482430170bfe2c718775ddb7dbb885/inst/extdata/penguins.csv\")\npenguins.head()\n</code></pre> species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year 0 Adelie Torgersen 39.1 18.7 181.0 3750.0 male 2007 1 Adelie Torgersen 39.5 17.4 186.0 3800.0 female 2007 2 Adelie Torgersen 40.3 18.0 195.0 3250.0 female 2007 3 Adelie Torgersen NaN NaN NaN NaN NaN 2007 4 Adelie Torgersen 36.7 19.3 193.0 3450.0 female 2007 <p>Since this is for demonstration purposes we will get rid of the NAs in the data; in a real world setting one would wish to take more care with proper handling of missing data.</p> Python<pre><code>penguins = penguins.dropna()\npenguins.species.value_counts()\n</code></pre> Text Only<pre><code>Adelie       146\nGentoo       119\nChinstrap     68\nName: species, dtype: int64\n</code></pre> <p></p> <p>See the github repostiory for more details about the dataset itself. It consists of measurements of bill (culmen) and flippers and weights of three species of penguins, along with some other metadata about the penguins. In total we have 333 different penguins measured. Visualizing this data is a little bit tricky since we can't plot in 4 dimensions easily. Fortunately four is not that large a number, so we can just to a pairwise feature scatterplot matrix to get an ideas of what is going on. Seaborn makes this easy.</p> Python<pre><code>sns.pairplot(penguins.drop(\"year\", axis=1), hue='species');\n</code></pre> <p></p> <p>This gives us some idea of what the data looks like by giving as all the 2D views of the data. Four dimensions is low enough that we can (sort of) reconstruct what the full dimensional data looks like in our heads. Now that we sort of know what we are looking at, the question is what can a dimension reduction technique like UMAP do for us? By reducing the dimension in a way that preserves as much of the structure of the data as possible we can get a visualisable representation of the data allowing us to \"see\" the data and its structure and begin to get some intuition about the data itself.</p> <p>To use UMAP for this task we need to first construct a UMAP object that will do the job for us. That is as simple as instantiating the class. So let's import the umap library and do that.</p> Python<pre><code>import umap\n</code></pre> Python<pre><code>reducer = umap.UMAP()\n</code></pre> <p>Before we can do any work with the data it will help to clean up it a little. We won't need NAs, we just want the measurement columns, and since the measurements are on entirely different scales it will be helpful to convert each feature into z-scores (number of standard deviations from the mean) for comparability.</p> Python<pre><code>penguin_data = penguins[\n    [\n        \"bill_length_mm\",\n        \"bill_depth_mm\",\n        \"flipper_length_mm\",\n        \"body_mass_g\",\n    ]\n].values\nscaled_penguin_data = StandardScaler().fit_transform(penguin_data)\n</code></pre> <p>Now we need to train our reducer, letting it learn about the manifold. For this UMAP follows the sklearn API and has a method <code>fit</code> which we pass the data we want the model to learn from. Since, at the end of the day, we are going to want to reduced representation of the data we will use, instead, the <code>fit_transform</code> method which first calls <code>fit</code> and then returns the transformed data as a numpy array.</p> Python<pre><code>embedding = reducer.fit_transform(scaled_penguin_data)\nembedding.shape\n</code></pre> Text Only<pre><code>(333, 2)\n</code></pre> <p>The result is an array with 333 samples, but only two feature columns (instead of the four we started with). This is because, by default, UMAP reduces down to 2D. Each row of the array is a 2-dimensional representation of the corresponding penguin. Thus we can plot the <code>embedding</code> as a standard scatterplot and color by the target array (since it applies to the transformed data which is in the same order as the original).</p> Python<pre><code>plt.scatter(\n    embedding[:, 0],\n    embedding[:, 1],\n    c=[sns.color_palette()[x] for x in penguins.species.map({\"Adelie\":0, \"Chinstrap\":1, \"Gentoo\":2})])\nplt.gca().set_aspect('equal', 'datalim')\nplt.title('UMAP projection of the Penguin dataset', fontsize=24);\n</code></pre> <p></p> <p>This does a useful job of capturing the structure of the data, and as can be seen from the matrix of scatterplots this is relatively accurate. Of course we learned at least this much just from that matrix of scatterplots -- which we could do since we only had four different dimensions to analyse. If we had data with a larger number of dimensions the scatterplot matrix would quickly become unwieldy to plot, and far harder to interpret. So moving on from the Penguin dataset, let's consider the digits dataset.</p>"},{"location":"basic_usage/#digits-data","title":"Digits data","text":"<p>First we will load the dataset from sklearn.</p> Python<pre><code>digits = load_digits()\nprint(digits.DESCR)\n</code></pre> Text Only<pre><code>.. _digits_dataset:\n\nOptical recognition of handwritten digits dataset\n--------------------------------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 5620\n    :Number of Attributes: 64\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n    :Missing Attribute Values: None\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n    :Date: July; 1998\n\nThis is a copy of the test set of the UCI ML hand-written digits datasets\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n\nThe data set contains images of hand-written digits: 10 classes where\neach class refers to a digit.\n\nPreprocessing programs made available by NIST were used to extract\nnormalized bitmaps of handwritten digits from a preprinted form. From a\ntotal of 43 people, 30 contributed to the training set and different 13\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n4x4 and the number of on pixels are counted in each block. This generates\nan input matrix of 8x8 where each element is an integer in the range\n0..16. This reduces dimensionality and gives invariance to small\ndistortions.\n\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n1994.\n\n.. topic:: References\n\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n    Graduate Studies in Science and Engineering, Bogazici University.\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n    Linear dimensionalityreduction using relevance weighted LDA. School of\n    Electrical and Electronic Engineering Nanyang Technological University.\n    2005.\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\n    Algorithm. NIPS. 2000.\n</code></pre> <p>We can plot a number of the images to get an idea of what we are looking at. This just involves matplotlib building a grid of axes and then looping through them plotting an image into each one in turn.</p> Python<pre><code>fig, ax_array = plt.subplots(20, 20)\naxes = ax_array.flatten()\nfor i, ax in enumerate(axes):\n    ax.imshow(digits.images[i], cmap='gray_r')\nplt.setp(axes, xticks=[], yticks=[], frame_on=False)\nplt.tight_layout(h_pad=0.5, w_pad=0.01)\n</code></pre> <p></p> <p>As you can see these are quite low resolution images -- for the most part they are recognisable as digits, but there are a number of cases that are sufficiently blurred as to be questionable even for a human to guess at. The zeros do stand out as the easiest to pick out as notably different and clearly zeros. Beyond that things get a little harder: some of the squashed thing eights look awfully like ones, some of the threes start to look a little like crossed sevens when drawn badly, and so on.</p> <p>Each image can be unfolded into a 64 element long vector of grayscale values. It is these 64 dimensional vectors that we wish to analyse: how much of the digits structure can we discern? At least in principle 64 dimensions is overkill for this task, and we would reasonably expect that there should be some smaller number of \"latent\" features that would be sufficient to describe the data reasonably well. We can try a scatterplot matrix -- in this case just of the first 10 dimensions so that it is at least plottable, but as you can quickly see that approach is not going to be sufficient for this data.</p> Python<pre><code>digits_df = pd.DataFrame(digits.data[:,1:11])\ndigits_df['digit'] = pd.Series(digits.target).map(lambda x: 'Digit {}'.format(x))\nsns.pairplot(digits_df, hue='digit', palette='Spectral');\n</code></pre> <p></p> <p>In contrast we can try using UMAP again. It works exactly as before: construct a model, train the model, and then look at the transformed data. To demonstrate more of UMAP we'll go about it differently this time and simply use the <code>fit</code> method rather than the <code>fit_transform</code> approach we used for Penguins.</p> Python<pre><code>reducer = umap.UMAP(random_state=42)\nreducer.fit(digits.data)\n</code></pre> Text Only<pre><code>UMAP(a=None, angular_rp_forest=False, b=None,\n     force_approximation_algorithm=False, init='spectral', learning_rate=1.0,\n     local_connectivity=1.0, low_memory=False, metric='euclidean',\n     metric_kwds=None, min_dist=0.1, n_components=2, n_epochs=None,\n     n_neighbors=15, negative_sample_rate=5, output_metric='euclidean',\n     output_metric_kwds=None, random_state=42, repulsion_strength=1.0,\n     set_op_mix_ratio=1.0, spread=1.0, target_metric='categorical',\n     target_metric_kwds=None, target_n_neighbors=-1, target_weight=0.5,\n     transform_queue_size=4.0, transform_seed=42, unique=False, verbose=False)\n</code></pre> <p>Now, instead of returning an embedding we simply get back the reducer object, now having trained on the dataset we passed it. To access the resulting transform we can either look at the <code>embedding_</code> attribute of the reducer object, or call transform on the original data.</p> Python<pre><code>embedding = reducer.transform(digits.data)\n# Verify that the result of calling transform is\n# idenitical to accessing the embedding_ attribute\nassert(np.all(embedding == reducer.embedding_))\nembedding.shape\n</code></pre> Text Only<pre><code>(1797, 2)\n</code></pre> <p>We now have a dataset with 1797 rows (one for each hand-written digit sample), but only 2 columns. As with the Penguins example we can now plot the resulting embedding, coloring the data points by the class that they belong to (i.e. the digit they represent).</p> Python<pre><code>plt.scatter(embedding[:, 0], embedding[:, 1], c=digits.target, cmap='Spectral', s=5)\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\nplt.title('UMAP projection of the Digits dataset', fontsize=24);\n</code></pre> <p></p> <p>We see that UMAP has successfully captured the digit classes. There are also some interesting effects as some digit classes blend into one another (see the eights, ones, and sevens, with some nines in between), and also cases where digits are pushed away as clearly distinct (the zeros on the right, the fours at the top, and a small subcluster of ones at the bottom come to mind). To get a better idea of why UMAP chose to do this it is helpful to see the actual digits involve. One can do this using bokeh and mouseover tooltips of the images.</p> <p>First we'll need to encode all the images for inclusion in a dataframe.</p> Python<pre><code>from io import BytesIO\nfrom PIL import Image\nimport base64\n</code></pre> Python<pre><code>def embeddable_image(data):\n    img_data = 255 - 15 * data.astype(np.uint8)\n    image = Image.fromarray(img_data, mode='L').resize((64, 64), Image.Resampling.BICUBIC)\n    buffer = BytesIO()\n    image.save(buffer, format='png')\n    for_encoding = buffer.getvalue()\n    return 'data:image/png;base64,' + base64.b64encode(for_encoding).decode()\n</code></pre> <p>Next we need to load up bokeh and the various tools from it that will be needed to generate a suitable interactive plot.</p> Python<pre><code>from bokeh.plotting import figure, show, output_notebook\nfrom bokeh.models import HoverTool, ColumnDataSource, CategoricalColorMapper\nfrom bokeh.palettes import Spectral10\n\noutput_notebook()\n</code></pre> Loading BokehJS ... <p>Finally we generate the plot itself with a custom hover tooltip that embeds the image of the digit in question in it, along with the digit class that the digit is actually from (this can be useful for digits that are hard even for humans to classify correctly).</p> Python<pre><code>digits_df = pd.DataFrame(embedding, columns=('x', 'y'))\ndigits_df['digit'] = [str(x) for x in digits.target]\ndigits_df['image'] = list(map(embeddable_image, digits.images))\n\ndatasource = ColumnDataSource(digits_df)\ncolor_mapping = CategoricalColorMapper(factors=[str(9 - x) for x in digits.target_names],\n                                       palette=Spectral10)\n\nplot_figure = figure(\n    title='UMAP projection of the Digits dataset',\n    width=600,\n    height=600,\n    tools=('pan, wheel_zoom, reset')\n)\n\nplot_figure.add_tools(HoverTool(tooltips=\"\"\"\n&lt;div&gt;\n    &lt;div&gt;\n        &lt;img src='@image' style='float: left; margin: 5px 5px 5px 5px'/&gt;\n    &lt;/div&gt;\n    &lt;div&gt;\n        &lt;span style='font-size: 16px; color: #224499'&gt;Digit:&lt;/span&gt;\n        &lt;span style='font-size: 18px'&gt;@digit&lt;/span&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n\"\"\"))\n\nplot_figure.scatter(\n    'x',\n    'y',\n    source=datasource,\n    color=dict(field='digit', transform=color_mapping),\n    line_alpha=0.6,\n    fill_alpha=0.6,\n    size=4\n)\nshow(plot_figure)\n</code></pre> <p>View html file</p> <p>As can be seen, the nines that blend between the ones and the sevens are odd looking nines (that aren't very rounded) and do, indeed, interpolate surprisingly well between ones with hats and crossed sevens. In contrast the small disjoint cluster of ones at the bottom of the plot is made up of ones with feet (a horizontal line at the base of the one) which are, indeed, quite distinct from the general mass of ones.</p> <p>This concludes our introduction to basic UMAP usage -- hopefully this has given you the tools to get started for yourself. Further tutorials, covering UMAP parameters and more advanced usage are also available when you wish to dive deeper.</p>   Penguin data information        <p>Peguin data are from:</p> <p>Gorman KB, Williams TD, Fraser WR (2014) Ecological Sexual Dimorphism and Environmental Variability within a Community of Antarctic Penguins (Genus Pygoscelis). PLoS ONE 9(3): e90081. doi:10.1371/journal.pone.0090081</p> <p>See the full paper HERE.</p>   Original data access and use        <p>From Gorman et al.: \"Data reported here are publicly available within the PAL-LTER data system (datasets #219, 220, and 221): http://oceaninformatics.ucsd.edu/datazoo/data/pallter/datasets. These data are additionally archived within the United States (US) LTER Network's Information System Data Portal: https://portal.lternet.edu/. Individuals interested in using these data are therefore expected to follow the US LTER Network's Data Access Policy, Requirements and Use Agreement: https://lternet.edu/data-access-policy/.\"</p> <p>Anyone interested in publishing the data should contact <code>Dr.\u00a0Kristen Gorman &lt;https://www.uaf.edu/cfos/people/faculty/detail/kristen-gorman.php&gt;</code>__ about analysis and working together on any final products.</p> <p>Penguin images by Alison Horst.</p>"},{"location":"benchmarking/","title":"Performance Comparison of Dimension Reduction Implementations","text":"<p>Different dimension reduction techniques can have quite different computational complexity. Beyond the algorithm itself there is also the question of how exactly it is implemented. These two factors can have a significant role in how long it actually takes to run a given dimension reduction. Furthermore the nature of the data you are trying to reduce can also matter -- mostly the involves the dimensionality of the original data. Here we will take a brief look at the performance characterstics of a number of dimension reduction implementations.</p> <p>To start let's get the basic tools we'll need loaded up -- numpy and pandas obviously, but also tools to get and resample the data, and the time module so we can perform some basic benchmarking.</p> IPython3<pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.utils import resample\nimport time\n</code></pre> <p>Next we'll need the actual dimension reduction implementations. For the purposes of this explanation we'll mostly stick with scikit-learn, but for the sake of comparison we'll also include the MulticoreTSNE implementation of t-SNE, which has significantly better performance than the current scikit-learn t-SNE.</p> IPython3<pre><code>from sklearn.manifold import TSNE, LocallyLinearEmbedding, Isomap, MDS, SpectralEmbedding\nfrom sklearn.decomposition import PCA\nfrom MulticoreTSNE import MulticoreTSNE\nfrom umap import UMAP\n</code></pre> <p>Next we'll need out plotting tools, and, of course, some data to work with. For this performance comparison we'll default to the now standard benchmark of manifold learning: the MNIST digits dataset. We can use scikit-learn's <code>fetch_mldata</code> to grab it for us.</p> IPython3<pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n</code></pre> IPython3<pre><code>sns.set(context='notebook',\n        rc={'figure.figsize':(12,10)},\n        palette=sns.color_palette('tab10', 10))\n</code></pre> IPython3<pre><code>mnist = fetch_openml('Fashion-MNIST', version=1)\n</code></pre> <p>Now it is time to start looking at performance. To start with let's look at how performance scales with increasing dataset size.</p>"},{"location":"benchmarking/#performance-scaling-by-dataset-size","title":"Performance scaling by dataset size","text":"<p>As the size of a dataset increases the runtime of a given dimension reduction algorithm will increase at varying rates. If you ever want to run your algorithm on larger datasets you will care not just about the comparative runtime on a single small dataset, but how the performance scales out as you move to larger datasets. We can similate this by subsampling from MNIST digits (via scikit-learn's convenient <code>resample</code> utility) and looking at the runtime for varying sized subsamples. Since there is some randomness involved here (both in the subsample selection, and in some of the algorithms which have stochastic aspects) we will want to run a few examples for each dataset size. We can easily package all of this up in a simple function that will return a convenient pandas dataframe of dataset sizes and runtimes given an algorithm.</p> IPython3<pre><code>def data_size_scaling(algorithm, data, sizes=[100, 200, 400, 800, 1600], n_runs=5):\n    result = []\n    for size in sizes:\n        for run in range(n_runs):\n            subsample = resample(data, n_samples=size)\n            start_time = time.time()\n            algorithm.fit(subsample)\n            elapsed_time = time.time() - start_time\n            del subsample\n            result.append((size, elapsed_time))\n    return pd.DataFrame(result, columns=('dataset size', 'runtime (s)'))\n</code></pre> <p>Now we just want to run this for each of the various dimension reduction implementations so we can look at the results. Since we don't know how long these runs might take we'll start off with a very small set of samples, scaling up to only 1600 samples.</p> IPython3<pre><code>all_algorithms = [\n    PCA(),\n    UMAP(),\n    MulticoreTSNE(),\n    LocallyLinearEmbedding(),\n    SpectralEmbedding(),\n    Isomap(),\n    TSNE(),\n    MDS(),\n]\nperformance_data = {}\nfor algorithm in all_algorithms:\n    alg_name = str(algorithm)\n    if 'MulticoreTSNE' in alg_name:\n        alg_name = 'MulticoreTSNE'\n    else:\n        alg_name = alg_name.split('(')[0]\n\n    performance_data[alg_name] = data_size_scaling(algorithm, mnist.data, n_runs=3)\n</code></pre> <p>Now let's plot the results so we can see what is going on. We'll use seaborn's regression plot to interpolate the effective scaling.</p> IPython3<pre><code>for alg_name, perf_data in performance_data.items():\n    sns.regplot('dataset size', 'runtime (s)', perf_data, order=2, label=alg_name)\nplt.legend();\n</code></pre> <p></p> <p>We can see straight away that there are some outliers here. The scikit-learn t-SNE is clearly much slower than most of the other algorithms. It does not have the scaling properties of MDS however; for larger dataset sizes MDS is going to quickly become completely unmanageable. At the same time MulticoreTSNE demonstrates that t-SNE can run fairly efficiently. It is hard to tell much about the other implementations other than the fact that PCA is far and away the fastest option. To see more we'll have to look at runtimes on larger dataset sizes. Both MDS and scikit-learn's t-SNE are going to take too long to run so let's restrict ourselves to the fastest performing implementations and see what happens as we extend out to larger dataset sizes.</p> IPython3<pre><code>fast_algorithms = [\n    PCA(),\n    UMAP(),\n    MulticoreTSNE(),\n    LocallyLinearEmbedding(),\n    SpectralEmbedding(),\n    Isomap(),\n]\nfast_performance_data = {}\nfor algorithm in fast_algorithms:\n    alg_name = str(algorithm)\n    if 'MulticoreTSNE' in alg_name:\n        alg_name = 'MulticoreTSNE'\n    else:\n        alg_name = alg_name.split('(')[0]\n\n    fast_performance_data[alg_name] = data_size_scaling(algorithm, mnist.data,\n                                                   sizes=[800, 1600, 3200, 6400, 12800], n_runs=3)\n</code></pre> IPython3<pre><code>for alg_name, perf_data in fast_performance_data.items():\n    sns.regplot('dataset size', 'runtime (s)', perf_data, order=2, label=alg_name)\nplt.legend();\n</code></pre> <p></p> <p>At this point we begin to see some significant differentiation among the different implementations. In the earlier plot MulticoreTSNE looked to be slower than some of the other algorithms, but as we scale out to larger datasets we see that its relative scaling performance is far superior to the scikit-learn implementations of Isomap, spectral embedding, and locally linear embedding.</p> <p>It is probably worth extending out further -- up to the full MNIST digits dataset. To manage to do that in any reasonable amount of time we'll have to restrict out attention to an even smaller subset of implementations. We will pare things down to just MulticoreTSNE, PCA and UMAP.</p> IPython3<pre><code>very_fast_algorithms = [\n    PCA(),\n    UMAP(),\n    MulticoreTSNE(),\n]\nvfast_performance_data = {}\nfor algorithm in very_fast_algorithms:\n    alg_name = str(algorithm)\n    if 'MulticoreTSNE' in alg_name:\n        alg_name = 'MulticoreTSNE'\n    else:\n        alg_name = alg_name.split('(')[0]\n\n    vfast_performance_data[alg_name] = data_size_scaling(algorithm, mnist.data,\n                                                    sizes=[3200, 6400, 12800, 25600, 51200, 70000], n_runs=2)\n</code></pre> IPython3<pre><code>for alg_name, perf_data in vfast_performance_data.items():\n    sns.regplot('dataset size', 'runtime (s)', perf_data, order=2, label=alg_name)\nplt.legend();\n</code></pre> <p></p> <p>Here we see UMAP's advantages over t-SNE really coming to the forefront. While UMAP is clearly slower than PCA, its scaling performance is dramatically better than MulticoreTSNE, and for even larger datasets the difference is only going to grow.</p> <p>This concludes our look at scaling by dataset size. The short summary is that PCA is far and away the fastest option, but you are potentially giving up a lot for that speed. UMAP, while not competitive with PCA, is clearly the next best option in terms of performance among the implementations explored here. Given the quality of results that UMAP can provide we feel it is clearly a good option for dimension reduction.</p>"},{"location":"clustering/","title":"Using UMAP for Clustering","text":"<p>UMAP can be used as an effective preprocessing step to boost the performance of density based clustering. This is somewhat controversial, and should be attempted with care. For a good discussion of some of the issues involved in this, please see the various answers <code>in this stackoverflow thread &lt;https://stats.stackexchange.com/questions/263539/clustering-on-the-output-of-t-sne&gt;</code>__ on clustering the results of t-SNE. Many of the points of concern raised there are salient for clustering the results of UMAP. The most notable is that UMAP, like t-SNE, does not completely preserve density. UMAP, like t-SNE, can also create false tears in clusters, resulting in a finer clustering than is necessarily present in the data. Despite these concerns there are still valid reasons to use UMAP as a preprocessing step for clustering. As with any clustering approach one will want to do some exploration and evaluation of the clusters that come out to try to validate them if possible.</p> <p>With all of that said, let's work through an example to demonstrate the difficulties that can face clustering approaches and how UMAP can provide a powerful tool to help overcome them.</p> <p>First we'll need a selection of libraries loaded up. Obviously we'll need data, and we can use sklearn's <code>fetch_openml</code> to get it. We'll also need the usual tools of numpy, and plotting. Next we'll need umap, and some clustering options. Finally, since we'll be working with labeled data, we can make use of strong cluster evaluation metrics <code>Adjusted Rand Index &lt;https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index&gt;</code>__ and <code>Adjusted Mutual Information &lt;https://en.wikipedia.org/wiki/Adjusted_mutual_information&gt;</code>__.</p> Python<pre><code>from sklearn.datasets import fetch_openml\nfrom sklearn.decomposition import PCA\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Dimension reduction and clustering libraries\nimport umap\nimport hdbscan\nimport sklearn.cluster as cluster\nfrom sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n</code></pre> <p>Now let's set up the plotting and grab the data we'll be using -- in this case the MNIST handwritten digits dataset. MNIST consists of 28x28 pixel grayscale images of handwritten digits (0 through 9). These can be unraveled such that each digit is described by a 784 dimensional vector (the gray scale value of each pixel in the image). Ideally we would like the clustering to recover the digit structure.</p> Python<pre><code>mnist = fetch_openml('mnist_784', version=1)\nmnist.target = mnist.target.astype(int)\n</code></pre> <p>For visualization purposes we can reduce the data to 2-dimensions using UMAP. When we cluster the data in high dimensions we can visualize the result of that clustering. First, however, we'll view the data colored by the digit that each data point represents -- we'll use a different color for each digit. This will help frame what follows.</p> Python<pre><code>standard_embedding = umap.UMAP(random_state=42).fit_transform(mnist.data)\nplt.scatter(standard_embedding[:, 0], standard_embedding[:, 1], c=mnist.target.astype(int), s=0.1, cmap='Spectral');\n</code></pre> <p></p>"},{"location":"clustering/#traditional-clustering","title":"Traditional clustering","text":"<p>Now we would like to cluster the data. As a first attempt let's try the traditional approach: K-Means. In this case we can solve one of the hard problems for K-Means clustering -- choosing the right k value, giving the number of clusters we are looking for. In this case we know the answer is exactly 10. We will use sklearns K-Means implementation looking for 10 clusters in the original 784 dimensional data.</p> Python<pre><code>kmeans_labels = cluster.KMeans(n_clusters=10).fit_predict(mnist.data)\n</code></pre> <p>And how did the clustering do? We can look at the results by coloring out UMAP embedded data by cluster membership.</p> Python<pre><code>plt.scatter(standard_embedding[:, 0], standard_embedding[:, 1], c=kmeans_labels, s=0.1, cmap='Spectral');\n</code></pre> <p></p> <p>This is not really the result we were looking for (though it does expose interesting properties of how K-Means chooses clusters in high dimensional space, and how UMAP unwraps manifolds by finding manifold boundaries). While K-Means gets some cases correct, such as the two clusters on the right side which are mostly correct, most of the rest of the data looks somewhat arbitrarily carved up among the remaining clusters. We can put this impression to the test by evaluating the adjusted Rand score and adjusted mutual information for this clustering as compared with the true labels.</p> Python<pre><code>(\n    adjusted_rand_score(mnist.target, kmeans_labels),\n    adjusted_mutual_info_score(mnist.target, kmeans_labels)\n)\n</code></pre> Text Only<pre><code>(0.36675295135972552, 0.49614118437750965)\n</code></pre> <p>As might be expected, we have not done a particularly good job -- both scores take values in the range 0 to 1, with 0 representing a bad (essentially random) clustering and 1 representing perfectly recovering the true labels. K-Means definitely was not random, but it was also quite a long way from perfectly recovering the true labels. Part of the problem is the way K-Means works, based on centroids with an assumption of largely spherical clusters -- this is responsible for some of the sharp divides that K-Means puts across digit classes. We can potentially improve on this by using a smarter density based algorithm. In this case we've chosen to try HDBSCAN, which we believe to be among the most advanced density based techniques. For the sake of performance we'll reduce the dimensionality of the data down to 50 dimensions via PCA (this recovers most of the variance), since HDBSCAN scales somewhat poorly with the dimensionality of the data it will work on.</p> Python<pre><code>lowd_mnist = PCA(n_components=50).fit_transform(mnist.data)\nhdbscan_labels = hdbscan.HDBSCAN(min_samples=10, min_cluster_size=500).fit_predict(lowd_mnist)\n</code></pre> <p>We can now inspect the results. Before we do, however, it should be noted that one of the features of HDBSCAN is that it can refuse to cluster some points and classify them as \"noise\". To visualize this aspect we will color points that were classified as noise gray, and then color the remaining points according to the cluster membership.</p> Python<pre><code>clustered = (hdbscan_labels &gt;= 0)\nplt.scatter(standard_embedding[~clustered, 0],\n            standard_embedding[~clustered, 1],\n            color=(0.5, 0.5, 0.5),\n            s=0.1,\n            alpha=0.5)\nplt.scatter(standard_embedding[clustered, 0],\n            standard_embedding[clustered, 1],\n            c=hdbscan_labels[clustered],\n            s=0.1,\n            cmap='Spectral');\n</code></pre> <p></p> <p>This looks somewhat underwhelming. It meets HDBSCAN's approach of \"not being wrong\" by simply refusing to classify the majority of the data. The result is a clustering that almost certainly fails to recover all the labels. We can verify this by looking at the clustering validation scores.</p> Python<pre><code>(\n    adjusted_rand_score(mnist.target, hdbscan_labels),\n    adjusted_mutual_info_score(mnist.target, hdbscan_labels)\n)\n</code></pre> Text Only<pre><code>(0.053830107882840102, 0.19756104096566332)\n</code></pre> <p>These scores are far worse than K-Means! Partially this is due to the fact that these scores assume that the noise points are simply an extra cluster. We can instead only look at the subset of the data that HDBSCAN was actually confident enough to assign to clusters -- a simple sub-selection will let us recompute the scores for only that data.</p> Python<pre><code>clustered = (hdbscan_labels &gt;= 0)\n(\n    adjusted_rand_score(mnist.target[clustered], hdbscan_labels[clustered]),\n    adjusted_mutual_info_score(mnist.target[clustered], hdbscan_labels[clustered])\n)\n</code></pre> Text Only<pre><code>(0.99843407988303912, 0.99405521087764015)\n</code></pre> <p>And here we see that where HDBSCAN was willing to cluster it got things almost entirely correct. This is what it was designed to do -- be right for what it can, and defer on anything that it couldn't have sufficient confidence in. Of course the catch here is that it deferred clustering a lot of the data. How much of the data did HDBSCAN actually assign to clusters? We can compute that easily enough.</p> Python<pre><code>np.sum(clustered) / mnist.data.shape[0]\n</code></pre> Text Only<pre><code>0.17081428571428572\n</code></pre> <p>It seems that less than 18% of the data was clustered. While HDBSCAN did a great job on the data it could cluster it did a poor job of actually managing to cluster the data. The problem here is that, as a density based clustering algorithm, HDBSCAN tends to suffer from the curse of dimensionality: high dimensional data requires more observed samples to produce much density. If we could reduce the dimensionality of the data more we would make the density more evident and make it far easier for HDBSCAN to cluster the data. The problem is that trying to use PCA to do this is going to become problematic. While reducing the 50 dimensions still explained a lot of the variance of the data, reducing further is going to quickly do a lot worse. This is due to the linear nature of PCA. What we need is strong manifold learning, and this is where UMAP can come into play.</p>"},{"location":"clustering/#umap-enhanced-clustering","title":"UMAP enhanced clustering","text":"<p>Our goal is to make use of UMAP to perform non-linear manifold aware dimension reduction so we can get the dataset down to a number of dimensions small enough for a density based clustering algorithm to make progress. One advantage of UMAP for this is that it doesn't require you to reduce to only two dimensions -- you can reduce to 10 dimensions instead since the goal is to cluster, not visualize, and the performance cost with UMAP is minimal. As it happens MNIST is such a simple dataset that we really can push it all the way down to only two dimensions, but in general you should explore different embedding dimension options.</p> <p>The next thing to be aware of is that when using UMAP for dimension reduction you will want to select different parameters than if you were using it for visualization. First of all we will want a larger <code>n_neighbors</code> value -- small values will focus more on very local structure and are more prone to producing fine grained cluster structure that may be more a result of patterns of noise in the data than actual clusters. In this case we'll double it from the default 15 up to 30. Second it is beneficial to set <code>min_dist</code> to a very low value. Since we actually want to pack points together densely (density is what we want after all) a low value will help, as well as making cleaner separations between clusters. In this case we will simply set <code>min_dist</code> to be 0.</p> Python<pre><code>clusterable_embedding = umap.UMAP(\n    n_neighbors=30,\n    min_dist=0.0,\n    n_components=2,\n    random_state=42,\n).fit_transform(mnist.data)\n</code></pre> <p>We can visualize the results of this so see how it compares with more visualization attuned parameters:</p> Python<pre><code>plt.scatter(clusterable_embedding[:, 0], clusterable_embedding[:, 1],\n            c=mnist.target, s=0.1, cmap='Spectral');\n</code></pre> <p></p> <p>As you can see we still have the general global structure, but we are packing points together more tightly within clusters, and consequently we can see larger gaps between the clusters. Ultimately this embedding was for clustering purposes only, and we will go back to the original embedding for visualization purposes from here on out.</p> <p>The next step is to cluster this data. We'll use HDBSCAN again, with the same parameter setting as before.</p> Python<pre><code>labels = hdbscan.HDBSCAN(\n    min_samples=10,\n    min_cluster_size=500,\n).fit_predict(clusterable_embedding)\n</code></pre> <p>And now we can visualize the results, just as before.</p> Python<pre><code>clustered = (labels &gt;= 0)\nplt.scatter(standard_embedding[~clustered, 0],\n            standard_embedding[~clustered, 1],\n            color=(0.5, 0.5, 0.5),\n            s=0.1,\n            alpha=0.5)\nplt.scatter(standard_embedding[clustered, 0],\n            standard_embedding[clustered, 1],\n            c=labels[clustered],\n            s=0.1,\n            cmap='Spectral');\n</code></pre> <p></p> <p>We can see that we have done a much better job of finding clusters rather than merely assigning the majority of data as noise. This is because we no longer have to try to cope with the relative lack of density in 50 dimensional space and now HDBSCAN can more cleanly discern the clusters.</p> <p>We can also make a quantitative assessment by using the clustering quality measures as before.</p> Python<pre><code>adjusted_rand_score(mnist.target, labels), adjusted_mutual_info_score(mnist.target, labels)\n</code></pre> Text Only<pre><code>(0.9239306564265013, 0.90302671641133736)\n</code></pre> <p>Where before HDBSCAN performed very poorly, we now have scores of 0.9 or better. This is because we actually clustered far more of the data. As before we can also look at how the clustering did on just the data that HDBSCAN was confident in clustering.</p> Python<pre><code>clustered = (labels &gt;= 0)\n(\n    adjusted_rand_score(mnist.target[clustered], labels[clustered]),\n    adjusted_mutual_info_score(mnist.target[clustered], labels[clustered])\n)\n</code></pre> Text Only<pre><code>(0.93240371696811541, 0.91912906363537572)\n</code></pre> <p>This is a little worse than the original HDBSCAN, but it is unsurprising that you are going to be wrong more often if you make more predictions. The question is how much more of the data is HDBSCAN actually clustering? Previously we were clustering only 17% of the data.</p> Python<pre><code>np.sum(clustered) / mnist.data.shape[0]\n</code></pre> Text Only<pre><code>0.99164285714285716\n</code></pre> <p>Now we are clustering over 99% of the data! And our results in terms of adjusted Rand score and adjusted mutual information are in line with the current state of the art techniques using convolutional autoencoder techniques. That's not bad for an approach that is simply viewing the data as arbitrary 784 dimensional vectors.</p> <p>Hopefully this has outlined how UMAP can be beneficial for clustering. As with all things care must be taken, but clearly UMAP can provide significantly better clustering results when used judiciously.</p>"},{"location":"composing_models/","title":"Combining multiple UMAP models","text":"<p>It is possible to combine together multiple UMAP models, assuming that they are operating on the same underlying data. To get an idea of how this works recall that UMAP uses an intermediate fuzzy topological representation (see <code>how_umap_works</code>). Given different views of the same underlying data this will generate different fuzzy topological representations. We can apply intersections or unions to these representations to get a new composite fuzzy topological representation which we can then embed into low dimensional space in the standard UMAP way. The key is that, to be able to sensibly intersect or union these representations, there must be one-to-one correspondences between the data samples from the two different views.</p> <p>To get an idea of how this might work it is useful to see it in practice. Let\u2019s load some libraries and get started.</p> Python<pre><code>import sklearn.datasets\nfrom sklearn.preprocessing import RobustScaler\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport umap\nimport umap.plot\n</code></pre>"},{"location":"composing_models/#mnist-digits-example","title":"MNIST digits example","text":"<p>To begin with let\u2019s use a relatively familiar dataset \u2013 the MNIST digits dataset that we\u2019ve used in other sections of this tutorial. The data is (grayscale) 28x28 pixel images of handwritten digits (0 through 9); in total there are 70,000 such images, and each image is unrolled into a 784 element vector.</p> Python<pre><code>mnist = sklearn.datasets.fetch_openml(\"mnist_784\")\n</code></pre> <p>To ensure we have an idea of what this dataset looks like through the lens of UMAP we can run UMAP on the full dataset.</p> Python<pre><code>mapper = umap.UMAP(random_state=42).fit(mnist.data)\n</code></pre> Python<pre><code>umap.plot.points(mapper, labels=mnist.target, width=500, height=500)\n</code></pre> <p></p> <p>To make the problem more interesting let\u2019s carve the dataset in two \u2013 not into two sets of 35,000 samples, but instead carve each image in half. That is, we\u2019ll end up with 70,000 samples each of which is the top half of the image of the handwritten digit, and another 70,000 samples each of which is the bottom half of the image of the handwritten digit.</p> Python<pre><code>top = mnist.data[:, :28 * 14]\nbottom = mnist.data[:, 28 * 14:]\n</code></pre> <p>This is a little artificial, but it provides us with an example dataset where we have two distinct views of the data which we can still well understand. In practice this situation would be more likely to arise when there are two different data collection processes sampling from the same underlying population. In our case we could simply glue the data back together (hstack the numpy arrays for example), but potentially this isn\u2019t feasible as the different data views may have different scales or modalities. So, despite the fact that we could glue things back together in this case, we will proceed as if we can\u2019t \u2013 as may be the case for many real world problems.</p> <p>Let\u2019s first look at what UMAP does individually on each dataset. We\u2019ll start with the top halves of the digits:</p> Python<pre><code>top_mapper = umap.UMAP(random_state=42).fit(top)\n</code></pre> Python<pre><code>umap.plot.points(top_mapper, labels=mnist.target, width=500, height=500)\n</code></pre> <p></p> <p>While UMAP still manages to mostly separate the different digit classes we can see the results are quite different from UMAP on the full standard MNIST dataset. The twos and threes are blurred together (as we would expect given that we don\u2019t have the bottom half of the image wich would let us tell them apart); The twos and threes are also in a large grouping that pulls together all of the eights, sevens and nines (again, what we would expect given only the top half of the digit), while the fives and sixes are somewhat distinct, but clearly are similar to each other. It is only the ones, fours and zeros that are very clearly discernible.</p> <p>Now let\u2019s see what sorts of results we get with the bottom halves of the digits:</p> Python<pre><code>bot_mapper = umap.UMAP(random_state=42).fit(bottom)\n</code></pre> Python<pre><code>umap.plot.points(bot_mapper, labels=mnist.target, width=500, height=500)\n</code></pre> <p></p> <p>This is clearly a very different view of the data. Now it is the fours and nines that blur together (presumably many of the nines are drawn with straight rather than curved stems), with sevens nearby. The twos and the threes are very distinct from each other, but the threes and the fives are combined (as one might expect given that the bottom halves should look similar). Zeros and sixes are distinct, but close to each other. Ones, eights and twos are the most distinctive digits in this view.</p> <p>So, assuming we can\u2019t just glue the raw data together and stick a reasonable metric on it, what can we do? We can perform intersections or unions on the fuzzy topological representations. There is also some work to be done re-asserting UMAP\u2019s theoretical assumptions (local connectivity, approximately uniform distributions). Fortunately UMAP makes this relatively easy as long as you have a copy of fitted UMAP models on hand (which we do in this case). To intersect two models simply use the <code>*</code> operator; to union them use the <code>+</code> operator. Note that this will actually take some time since we need to compute the 2D embedding of the combined model.</p> Python<pre><code>intersection_mapper = top_mapper * bot_mapper\nunion_mapper = top_mapper + bot_mapper\n</code></pre> <p>With that complete we can visualize the results. First let\u2019s look at the intersection:</p> Python<pre><code>umap.plot.points(intersection_mapper, labels=mnist.target, width=500, height=500)\n</code></pre> <p></p> <p>As you can see, while this isn\u2019t as good as a UMAP plot for the full MNIST dataset it has recovered the individual digits quite well. The worst of the remaining overlap is between the threes and fives in the center, which is it still struggling to fully distinguish. But note, also, that we have recovered more of the overall structure than either of the two different individual views, with the layout of different digit classes more closely resembling that of the UMAP run on the full dataset.</p> <p>Now let\u2019s look at the union.</p> Python<pre><code>umap.plot.points(union_mapper, labels=mnist.target, width=500, height=500)\n</code></pre> <p></p> <p>Given that UMAP is agnostic to rotation or reflection of the final layout, this is essentially the same result as the intersection since it is almost the reflection of it in the y-axis. This sort of result (intersection and union being similar) is not always the case (in fact it is not that common), but since the underlying structure of the digits dataset is so clear we find that either way of piecing it together from the two half datasets manage to find the same core underlying structure.</p> <p>If you are willing to try something a little more experimental there is also a third option using the <code>-</code> operator which effectively intersects with the fuzzy set complement (and is thus not commutative, just as <code>-</code> implies). The goal here is to try to provide a sense of what the data looks like when we contrast it against a second view.</p> Python<pre><code>contrast_mapper = top_mapper - bot_mapper\n</code></pre> Python<pre><code>umap.plot.points(contrast_mapper, labels=mnist.target, width=500, height=500)\n</code></pre> <p></p> <p>In this case the result is not overly dissimilar from the embedding of just the top half, so the contrast has perhaps not shown is as much as we might have hoped.</p>"},{"location":"composing_models/#diamonds-dataset-example","title":"Diamonds dataset example","text":"<p>Now let\u2019s try the same approach on a different dataset where the option of just running UMAP on the full dataset is not available. For this we\u2019ll use the diamonds dataset. In this dataset each row represents a different diamond and provides details on the weight (carat), cut, color, clarity, size (depth, table, x, y, z) and price of the given diamond. How these different factors interplay is somewhat complicated.</p> Python<pre><code>diamonds = sns.load_dataset('diamonds')\ndiamonds.head()\n</code></pre> carat cut color clarity depth table price x y z 0 0.23 Ideal E SI2 61.5 55.0 326 3.95 3.98 2.43 1 0.21 Premium E SI1 59.8 61.0 326 3.89 3.84 2.31 2 0.23 Good E VS1 56.9 65.0 327 4.05 4.07 2.31 3 0.29 Premium I VS2 62.4 58.0 334 4.20 4.23 2.63 4 0.31 Good J SI2 63.3 58.0 335 4.34 4.35 2.75 <p>For our purposes let\u2019s take \u201cprice\u201d as a \u201ctarget\u201d variable (as is often the case when the dataset is used in machine learning contexts). What we would like to do is provide a UMAP embedding of the data using the remaining features. This is tricky since we can\u2019t exactly use a euclidean metric over the whole thing. What we can do, however, is split the data into two distinct types: the purely numeric features relating to size and weight, and the categorical features of color, cut and clarity. Let\u2019s pull each of those feature sets out so we can work with them independently.</p> Python<pre><code>numeric = diamonds[[\"carat\", \"table\", \"x\", \"y\", \"z\"]].copy()\nordinal = diamonds[[\"cut\", \"color\", \"clarity\"]].copy()\n</code></pre> <p>Now we have a new problem: the numeric features are not at all on the same scales, so any sort of standard distance metric across them will be dominated by those features with the largest ranges. We can correct for that by performing feature scaling. To do that we\u2019ll make use of sklearn\u2019s <code>RobustScaler</code> which uses robust statistics (such as the median and interquartile range) to center and rescale the data feature by feature. If we look at the results on the first five rows we see that the different features are all now reasonably comparable, and it is reasonable to apply something like euclidean distance across them.</p> Python<pre><code>scaled_numeric = RobustScaler().fit_transform(numeric)\nscaled_numeric[:5]\n</code></pre> Text Only<pre><code>array([[-0.734375  , -0.66666667, -0.95628415, -0.95054945, -0.97345133],\n       [-0.765625  ,  1.33333333, -0.98907104, -1.02747253, -1.07964602],\n       [-0.734375  ,  2.66666667, -0.90163934, -0.9010989 , -1.07964602],\n       [-0.640625  ,  0.33333333, -0.81967213, -0.81318681, -0.79646018],\n       [-0.609375  ,  0.33333333, -0.7431694 , -0.74725275, -0.69026549]])\n</code></pre> <p>What is the best way to handle the categorical features? If they are purely categorical it would make sense to one-hot encode the categories and use \u201cdice\u201d distance between them. A downside of that is that, with so few categories, it is a very coarse metric which will fail to provide much differentiation. For the diamonds dataset, however, the categories come with a strict order: Ideal cut is better than Premium cut, which is better than Very Good cut and so on. Color grades work similarly, and there is a distinct grading scheme for clarity as well. We can use an ordinal encoding on these categories. Now, while the ranges of values may vary, the differences between them are all comparable \u2013 a difference of 1 for each grade level. That means we don\u2019t need to rescale this data after the ordinal coding.</p> Python<pre><code>ordinal[\"cut\"] = ordinal.cut.map({\"Fair\":0, \"Good\":1, \"Very Good\":2, \"Premium\":3, \"Ideal\":4})\nordinal[\"color\"] = ordinal.color.map({\"D\":0, \"E\":1, \"F\":2, \"G\":3, \"H\":4, \"I\":5, \"J\":6})\nordinal[\"clarity\"] = ordinal.clarity.map({\"I1\":0, \"SI2\":1, \"SI1\":2, \"VS2\":3, \"VS1\":4, \"VVS2\":5, \"VVS1\":6, \"IF\":7})\n</code></pre> Python<pre><code>ordinal\n</code></pre> cut color clarity 0 4 1 1 1 3 1 2 2 1 1 4 3 3 5 3 4 1 6 1 ... ... ... ... 53935 4 0 2 53936 1 0 2 53937 2 0 2 53938 3 4 1 53939 4 0 1 <p>53940 rows \u00d7 3 columns</p> <p>As noted we can use euclidean as a sensible distance on the rescaled numeric data. On the other hand since the different ordinal categories are entirelty independent of each other, and we have a strict ordinal codin, the socalled \u201cmanhattan\u201d metric makes more sense here \u2013 it is simply the sum of the absolute differences in each category. As before we can now train UMAP models on each dataset \u2013 this time, however, since the datasets are different we need different metrics and even different values of <code>n_neighbors</code>.</p> Python<pre><code>numeric_mapper = umap.UMAP(n_neighbors=15, random_state=42).fit(scaled_numeric)\nordinal_mapper = umap.UMAP(metric=\"manhattan\", n_neighbors=150, random_state=42).fit(ordinal.values)\n</code></pre> <p>We can look at the results of each of these independent views of the dataset reduced to 2D using UMAP. Let\u2019s first look at the numeric data on size and weight of the diamonds. We can colour by the price to get some idea of how the dataset fits together.</p> Python<pre><code>umap.plot.points(numeric_mapper, values=diamonds[\"price\"], cmap=\"viridis\")\n</code></pre> <p></p> <p>We see that while the data generally correlates somewhat with the price of the diamonds there are distinctly different threads in the data, presumably corresponding to different styles of cut, and how that results in different sizing of diamonds in the various dimensions, depending on the weight.</p> <p>In contrast we ca look at the ordinal data. In this case we\u2019ll colour it by the different categories as well as by price.</p> Python<pre><code>fig, ax = umap.plot.plt.subplots(2, 2, figsize=(12,12))\numap.plot.points(ordinal_mapper, labels=diamonds[\"color\"], ax=ax[0,0])\numap.plot.points(ordinal_mapper, labels=diamonds[\"clarity\"], ax=ax[0,1])\numap.plot.points(ordinal_mapper, labels=diamonds[\"cut\"], ax=ax[1,0])\numap.plot.points(ordinal_mapper, values=diamonds[\"price\"], cmap=\"viridis\", ax=ax[1,1])\n</code></pre> <p></p> <p>As you can see this is a markedly different result! The ordinal data has a relatively coarse metric, since the different categories can only take on a small range of discrete values. This means that, with respect to the trio of color, cut, and clarity, diamonds are largely either almost identical, or quite distinct. The result is very tight groupings which have very high density. You can see a gradient of color from left to right in the plot; colouring by cut or clarity show different stratifications. The combination of these very distinct statifications results in this highly clustered embedding. It is exactly for this reason that we need such a high <code>n_neighbors</code> value: the very local structure of the data is merely clusters of identical categories; we need to see wider to learn more structure.</p> <p>Given these radically different views of the data, what do we get if we try to integrate them together? As before we can use the intersection and union operators to simply combine the models. As noted before this is a somewhat time-consuming operation as a new 2D representation for the combined models needs to be optimized.</p> Python<pre><code>intersection_mapper = numeric_mapper * ordinal_mapper\nunion_mapper = numeric_mapper + ordinal_mapper\n</code></pre> <p>Let\u2019s start by looking at the intersection; here we are only really decreasing connectivity since edges are assigned the probability of existing in both data views (before re-asserting local connectivity and uniform distribution assumptions).</p> Python<pre><code>umap.plot.points(intersection_mapper, values=diamonds[\"price\"], cmap=\"viridis\")\n</code></pre> <p></p> <p>What we get most closely represents the numeric data view. Why is this? Because the categorical data view has points either connected with certainty (because they are, or are nearly, identical) or very loosely. The points connected with near certainty are very dense clusters \u2013 almost points in the plot \u2013 and mostly what we are doing with the intersection is breaking up those clusters with the more fine-grained and variable connectivity provided by the numerical data. At th esame time we have shifted the result significantly from the numerical data view on its own; the categorical information has made each cluster more uniform (rather than being a gradient) in its price.</p> <p>Given this result, what would you expect of the union?</p> Python<pre><code>umap.plot.points(union_mapper, labels=diamonds[\"color\"])\n</code></pre> <p></p> <p>What we get in practice looks a lot more like the categorical view of the data. This time we are only increasing the connectivity (prior to re-asserting local connectivity and uniform distribution assumptions); thus we retain most of the structure of the high-connectivity categorical view. Note, however, that we have created more connected and coherent clusters in the center of the plot, showing a range of diamond colors, and the introduction of the numerical size and weight information has induced a rearrangement of the individual clusters around the fringes.</p> <p>We can go a step further and experiment with the contrast composition method.</p> Python<pre><code>contrast_mapper = numeric_mapper - ordinal_mapper\n</code></pre> Python<pre><code>umap.plot.points(contrast_mapper, values=diamonds[\"price\"], cmap=\"viridis\")\n</code></pre> <p></p> <p>Here we see that we\u2019ve retained a lot of the structure of the numeric data view, but have refined and broken it down further into clear clusters with price gradients running through each of them.</p> <p>To further demonstrate the power of this approach we can go a step further and intersect a higher <code>n_neighbors</code> based embedding of the numeric data view with our existing union of numeric and categorical data \u2013 providing a model that is a composition of three simpler models.</p> Python<pre><code>intersect_union_mapper = umap.UMAP(random_state=42, n_neighbors=60).fit(numeric) * union_mapper\n</code></pre> Python<pre><code>umap.plot.points(intersect_union_mapper, values=diamonds[\"price\"], cmap=\"viridis\")\n</code></pre> <p></p> <p>Here the greater global structure from the larger <code>n_neighbors</code> value glues together longer strands and we get an interesting result out. In this case it is not necessarily particularly informative, but it is included as a demonstration that even composed models can be composed with each other, stacking together potentially many different views.</p>"},{"location":"densmap_demo/","title":"Better Preserving Local Density with DensMAP","text":"<p>A notable assumption in UMAP is that the data is uniformly distributed on some manifold and that it is ultimately this manifold that we would like to present. This is highly effective for many use cases, but it can be the case that one would like to preserve more information about the relative local density of data. A recent paper presented a technique called DensMAP that computes estimates of the local density and uses those estimates as a regularizer in the optimization of the low dimensional representation. The details are well explained in <code>the paper &lt;https://www.biorxiv.org/content/10.1101/2020.05.12.077776v1&gt;</code>__ and we encourage those curious about the details to read it. The result is a low dimensional representation that preserves information about the relative local density of the data. To see what this means in practice let\u2019s load some modules and try it out on some familiar data.</p> Python<pre><code>import sklearn.datasets\nimport umap\nimport umap.plot\n</code></pre> <p>For test data we will make use of the now familiar (see earlier tutorial sections) MNIST and Fashion-MNIST datasets. MNIST is a collection of 70,000 gray-scale images of hand-written digits. Fashion-MNIST is a collection of 70,000 gray-scale images of fashion items.</p> Python<pre><code>mnist = sklearn.datasets.fetch_openml(\"mnist_784\")\nfmnist = sklearn.datasets.fetch_openml(\"Fashion-MNIST\")\n</code></pre> <p>Before we try out DensMAP let\u2019s run standard UMAP so we have a baseline to compare to. We\u2019ll start with MNIST digits.</p> Python<pre><code>%%time\nmapper = umap.UMAP(random_state=42).fit(mnist.data)\n</code></pre> Text Only<pre><code>CPU times: user 2min, sys: 15 s, total: 2min 15s\nWall time: 1min 43s\n</code></pre> Python<pre><code>umap.plot.points(mapper, labels=mnist.target, width=500, height=500)\n</code></pre> <p></p> <p>Now let\u2019s try running DensMAP instead. In practice this is as easy as adding the parameter <code>densmap=True</code> to the UMAP constructor \u2013 this will cause UMAP to use DensMAP regularization with the default DensMAP parameters.</p> Python<pre><code>%%time\ndens_mapper = umap.UMAP(densmap=True, random_state=42).fit(mnist.data)\n</code></pre> Text Only<pre><code>CPU times: user 3min 42s, sys: 12.9 s, total: 3min 55s\nWall time: 2min 20s\n</code></pre> <p>Note that this is a little slower than standard UMAP \u2013 there is a little more work to be done. It is worth noting, however, that the DensMAP overhead is relatively constant, so the difference in runtime won\u2019t increase much as you scale out DensMAP to larger datasets.</p> <p>Now let\u2019s see what sort of results we get:</p> Python<pre><code>umap.plot.points(dens_mapper, labels=mnist.target, width=500, height=500)\n</code></pre> <p></p> <p>This is a significantly different result \u2013 although notably the same groupings of digits and overall structure have resulted. The most striking aspects are that the ones cluster has be compressed into a very narrow and dense stripe, while other digit clusters, most notably the zeros and the twos have expanded out to fill more space in the plot. This is due to the fact that in the high dimensional space the ones are indeed more densely packed together, with largely only variation along one dimension (the angle with which the stroke of the one is drawn). In contrast a digit like the zero has a lot more variation (rounder, narrower, taller, shorter, sloping one way or another); this results in less local density in high dimensional space, and this lack of local density has been preserved by DensMAP.</p> <p>Let\u2019s now look at the Fashion-MNIST dataset; as before we\u2019ll start by reminding ourselves what the default UMAP results look like:</p> Python<pre><code>%%time\nmapper = umap.UMAP(random_state=42).fit(fmnist.data)\n</code></pre> Text Only<pre><code>CPU times: user 1min 6s, sys: 8.66 s, total: 1min 15s\nWall time: 49.8 s\n</code></pre> Python<pre><code>umap.plot.points(mapper, labels=fmnist.target, width=500, height=500)\n</code></pre> <p></p> <p>Now let\u2019s try running DensMAP. As before that is as simple as setting the <code>densmap=True</code> flag.</p> Python<pre><code>%%time\ndens_mapper = umap.UMAP(densmap=True, random_state=42).fit(fmnist.data)\n</code></pre> Text Only<pre><code>CPU times: user 3min 48s, sys: 8.07 s, total: 3min 56s\nWall time: 2min 21s\n</code></pre> Python<pre><code>umap.plot.points(dens_mapper, labels=fmnist.target, width=500, height=500)\n</code></pre> <p></p> <p>Again we see that DensMAP provides a plot similar to UMAP broadly, but with striking differences. Here we get to see that the cluster of bags (label 8 in blue) is actually quite sparse, while the cluster of pants (label 1 in red) is actually quite dense with little variation compared to other categories. We even see information internal to clusters. Consider the cluster of boots (label 9 in violet): at the top end it is quite dense, but it fades out into a much sparse region.</p> <p>So far we have used DensMAP with default parameters, but the implementation provides several parameters for adjusting exactly how the local density regularisation is handled. We encourage readers to consult the paper for the details of the many parameters available. For general use the main parameter of interest is called <code>dens_lambda</code> and it controls how strongly the local density regularisation acts. Larger values of <code>dens_lambda</code> with make preserving the local density a priority over the the standard UMAP objective, while smaller values lean more towards classical UMAP. The default value is 2.0. Let\u2019s play with it a little so we can see the effects of varying it. To start we\u2019ll use a higher <code>dens_lambda</code> of 5.0:</p> Python<pre><code>%%time\ndens_mapper = umap.UMAP(densmap=True, dens_lambda=5.0, random_state=42).fit(fmnist.data)\n</code></pre> Text Only<pre><code>CPU times: user 3min 47s, sys: 5.04 s, total: 3min 52s\nWall time: 2min 18s\n</code></pre> Python<pre><code>umap.plot.points(dens_mapper, labels=fmnist.target, width=500, height=500)\n</code></pre> <p></p> <p>This looks kind of like what we had before, but blurrier. And also \u2026 smaller? The plot bounds are set by the data, so the fact that it is smaller represents the fact that there are some points right out to the edges of the plot. These are likely points that are in locally very sparse regions of the high dimensional space and are thus pushed well away from everything else. We can see this better if we use raw matplotlib and a scatter plot with larger point size:</p> Python<pre><code>fig, ax = umap.plot.plt.subplots(figsize=(7,7))\nax.scatter(*dens_mapper.embedding_.T, c=fmnist.target.astype('int8'), cmap=\"Spectral\", s=1)\n</code></pre> <p></p> <p>Aside from seeing the issues with overplotting we can see that there are, in fact, quite a few points that create a very soft halo of of sparse points around the fringes.</p> <p>Now let\u2019s try going the other way and reduce <code>dens_lambda</code> to a small value, so that in principle we can recover something quite close to the default UMAP plot, with just a hint of local density information encoded.</p> Python<pre><code>%%time\ndens_mapper = umap.UMAP(densmap=True, dens_lambda=0.1, random_state=42).fit(fmnist.data)\n</code></pre> Text Only<pre><code>CPU times: user 3min 47s, sys: 3.78 s, total: 3min 51s\nWall time: 2min 16s\n</code></pre> Python<pre><code>umap.plot.points(dens_mapper, labels=fmnist.target, width=500, height=500)\n</code></pre> <p></p> <p>And indeed, this looks very much like the original plot, but the bags (label 8 in blue) are slightly more diffused, and the pants (label 1 in red) are a little denser. This is very much the default UMAP with just a tweak to better reflect some notion of local density.</p>"},{"location":"densmap_demo/#supervised-densmap-on-the-galaxy10sdss-dataset","title":"Supervised DensMAP on the Galaxy10SDSS dataset","text":"<p>The Galaxy10SDSS dataset is a crowd sourced human labelled dataset of galaxy images, which have been separated in to ten classes. DensMAP can learn an embedding that partially separates the data. To keep runtime small, DensMAP is applied to a subset of the data.</p> Python<pre><code>import numpy as np\nimport h5py\nimport matplotlib.pyplot as plt\nimport umap\nimport os\nimport math\nimport requests\n\nif not os.path.isfile(\"Galaxy10.h5\"):\n    url = \"http://astro.utoronto.ca/~bovy/Galaxy10/Galaxy10.h5\"\n    r = requests.get(url, allow_redirects=True)\n    open(\"Galaxy10.h5\", \"wb\").write(r.content)\n\n# To get the images and labels from file\nwith h5py.File(\"Galaxy10.h5\", \"r\") as F:\n    images = np.array(F[\"images\"])\n    labels = np.array(F[\"ans\"])\n\nX_train = np.empty([math.floor(len(labels) / 100), 14283], dtype=np.float64)\ny_train = np.empty([math.floor(len(labels) / 100)], dtype=np.float64)\nX_test = X_train\ny_test = y_train\n# Get a subset of the data\nfor i in range(math.floor(len(labels) / 100)):\n    X_train[i, :] = np.array(np.ndarray.flatten(images[i, :, :, :]), dtype=np.float64)\n    y_train[i] = labels[i]\n    X_test[i, :] = np.array(\n        np.ndarray.flatten(images[i + math.floor(len(labels) / 100), :, :, :]),\n        dtype=np.float64,\n    )\n    y_test[i] = labels[i + math.floor(len(labels) / 100)]\n\n# Plot distribution\nclasses, frequency = np.unique(y_train, return_counts=True)\nfig = plt.figure(1, figsize=(4, 4))\nplt.clf()\nplt.bar(classes, frequency)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Data Subset\")\nplt.savefig(\"galaxy10_subset.svg\")\n</code></pre> <p></p> <p>The figure shows that the selected subset of the data set is unbalanced, but the entire dataset is also unbalanced, so this experiment will still use this subset. The next step is to examine the output of the standard DensMAP algorithm.</p> Python<pre><code>reducer = umap.UMAP(\n    densmap=True, n_components=2, random_state=42, verbose=False\n)\nreducer.fit(X_train)\n\ngalaxy10_densmap = reducer.transform(X_train)\nfig = plt.figure(1, figsize=(4, 4))\nplt.clf()\nplt.scatter(\n    galaxy10_densmap[:, 0],\n    galaxy10_densmap[:, 1],\n    c=y_train,\n    cmap=plt.cm.nipy_spectral,\n    edgecolor=\"k\",\n    label=y_train,\n)\nplt.colorbar(boundaries=np.arange(11) - 0.5).set_ticks(np.arange(10))\nplt.savefig(\"galaxy10_2D_densmap.svg\")\n</code></pre> <p></p> <p>The standard DensMAP algorithm does not separate the galaxies according to their type. Supervised DensMAP can do better.</p> Python<pre><code>reducer = umap.UMAP(\n    densmap=True, n_components=2, random_state=42, verbose=False\n)\nreducer.fit(X_train, y_train)\n\ngalaxy10_densmap_supervised = reducer.transform(X_train)\nfig = plt.figure(1, figsize=(4, 4))\nplt.clf()\nplt.scatter(\n    galaxy10_densmap_supervised[:, 0],\n    galaxy10_densmap_supervised[:, 1],\n    c=y_train,\n    cmap=plt.cm.nipy_spectral,\n    edgecolor=\"k\",\n    label=y_train,\n)\nplt.colorbar(boundaries=np.arange(11) - 0.5).set_ticks(np.arange(10))\nplt.savefig(\"galaxy10_2D_densmap_supervised.svg\")\n</code></pre> <p></p> <p>Supervised DensMAP does indeed do better. There is a litle overlap between some of the classes, but the original dataset also has some ambiguities in the classification.  The best check of this method is to project the testing data onto the learned embedding.</p> Python<pre><code>galaxy10_densmap_supervised_prediction = reducer.transform(X_test)\nfig = plt.figure(1, figsize=(4, 4))\nplt.clf()\nplt.scatter(\n    galaxy10_densmap_supervised_prediction[:, 0],\n    galaxy10_densmap_supervised_prediction[:, 1],\n    c=y_test,\n    cmap=plt.cm.nipy_spectral,\n    edgecolor=\"k\",\n    label=y_test,\n)\nplt.colorbar(boundaries=np.arange(11) - 0.5).set_ticks(np.arange(10))\nplt.savefig(\"galaxy10_2D_densmap_supervised_prediction.svg\")\n</code></pre> <p></p> <p>This shows that the learned embedding can be used on new data sets, and so this method may be helpful for examining images of galaxies. Try out this method on the full 200 Mb dataset as well as the newer 2.54 Gb Galaxy 10 DECals dataset</p>"},{"location":"development_roadmap/","title":"Development Roadmap","text":"<p>================ Development Roadmap ================</p> <p>This document outlines planned improvements and modernizations for the UMAP codebase.</p>"},{"location":"development_roadmap/#major-initiatives","title":"Major Initiatives","text":""},{"location":"development_roadmap/#1-parametric-umap-tensorflow-pytorch-migration","title":"1. Parametric UMAP: TensorFlow \u2192 PyTorch Migration","text":"<p>Status: Planned</p> <p>Motivation: - PyTorch has become the standard in modern machine learning - Better community support and ecosystem for deep learning - Easier to integrate with modern training frameworks - TensorFlow 2.x dependency (currently &gt;= 2.1) is outdated and should be &gt;= 2.10+ - PyTorch provides better control over computation graphs and optimization</p> <p>Scope: - Replace TensorFlow/Keras-based parametric UMAP implementation with PyTorch - Update parametric_umap.py and related modules - Migrate all Keras models to PyTorch equivalents - Update all Parametric UMAP example notebooks to use PyTorch</p> <p>Implementation Plan: 1. Create PyTorch equivalents of all TensorFlow/Keras models 2. Implement PyTorch training loops with equivalent loss functions 3. Add PyTorch as an optional dependency 4. Maintain TensorFlow version temporarily for backward compatibility 5. Add comprehensive testing for PyTorch implementation 6. Update all documentation and examples 7. Deprecate TensorFlow implementation in a future release 8. Remove TensorFlow support in a major version bump</p> <p>Files to Modify: - umap/parametric_umap.py (core implementation) - notebooks/Parametric_UMAP/*.ipynb (all examples) - doc/parametric_umap.rst (documentation) - pyproject.toml (dependencies) - tests/ (add PyTorch tests)</p> <p>Estimated Effort: Large (multiple weeks)</p> <p>Priority: High</p>"},{"location":"development_roadmap/#2-remove-conda-from-installation-and-cicd","title":"2. Remove Conda from Installation and CI/CD","text":"<p>Status: Planned</p> <p>Motivation: - pip and modern Python packaging (PyPI) is the standard - Removes redundant installation method - Simplifies CI/CD pipelines - Reduces maintenance burden - conda-forge is maintained by volunteers; we should focus on PyPI</p> <p>Scope: - Remove conda installation instructions from README and documentation - Remove conda badges from README - Remove conda-based CI/CD scripts and configurations - Update all installation documentation - Remove references to anaconda and conda in setup instructions - Clean up ci_scripts/install.sh (currently uses conda)</p> <p>Implementation Plan: 1. Update README.rst to remove conda install instructions 2. Update doc/index.rst to remove conda installation section 3. Remove conda references from ci_scripts/ 4. Remove .travis.yml and appveyor.yml (legacy CI configurations) 5. Ensure all installation docs reference pip/PyPI only 6. Add documentation for development setup using pip and venv/virtualenv 7. Update any notebooks that show conda install commands</p> <p>Files to Modify: - README.rst - doc/index.rst - doc/parameters.rst - ci_scripts/install.sh - .travis.yml (remove or archive) - appveyor.yml (remove or archive) - Notebooks showing conda instructions</p> <p>Estimated Effort: Small (1-2 days)</p> <p>Priority: Medium</p>"},{"location":"development_roadmap/#3-remove-pynndescent-dependency","title":"3. Remove pynndescent Dependency","text":"<p>Status: Planned</p> <p>Motivation: - pynndescent is an external dependency created for UMAP but not tightly integrated - Adds a required external dependency that complicates the dependency tree - Reduces coupling and external dependencies improves maintainability - pynndescent can remain optional for users who want advanced features, but core UMAP should work with standard libraries - Industry-standard vector databases like Qdrant use alternative approaches (HNSW, etc.)</p> <p>Reference Implementations: - Qdrant: Production vector database using HNSW   - See architecture: https://github.com/qdrant/qdrant/tree/master/src   - Uses HNSW (Hierarchical Navigable Small World) for production-grade nearest neighbor search   - 10x+ faster than alternatives on large datasets</p> <ul> <li>HGG (Hierarchical Greedy Graph): Alternative to HNSW from rust-cv</li> <li>See implementation: https://github.com/rust-cv/hgg</li> <li>Data-dependent hierarchy (adapts to local dimensionality)</li> <li>Fully deterministic (unlike HNSW which uses randomness)</li> <li>More efficient edge management and freshening process</li> <li> <p>Written in Rust, but algorithm principles apply to Python implementation</p> </li> <li> <p>ANN-Benchmarks: Comprehensive benchmark suite for nearest neighbor algorithms</p> </li> <li>See benchmarks: https://ann-benchmarks.com/index.html</li> <li>See source &amp; README: https://github.com/erikbern/ann-benchmarks/?tab=readme-ov-file</li> <li>Real-world datasets and performance comparisons</li> <li>Tracks speed vs recall tradeoffs across all major algorithms</li> <li>Essential resource for validating implementation choices</li> </ul> <p>Scope: - Replace pynndescent with pluggable ANN backend system supporting multiple algorithms - Integrate many ANN algorithms to discover the fastest options for different use cases - Reference all algorithms from ann-benchmarks.com: HNSW, Annoy, FAISS, NGT, NSW, Vespa, Milvus, etc. - Implement missing distance metrics that pynndescent provides - Handle both dense and sparse nearest neighbor computation - Make pynndescent optional for advanced/performance-critical use cases - Achieve equivalent or better performance than current pynndescent implementation - Goal: Find the optimal ANN algorithm for different scenarios (speed, memory, accuracy, determinism)</p> <p>Alternative Implementations Comparison:</p> <p>================ ==================== ==================== ==================== ==================== ==================== Criteria         scikit-learn         HNSWlib              FAISS                Annoy                HGG ================ ==================== ==================== ==================== ==================== ==================== Type             KDTree/BallTree      Graph (HNSW)         Clustering/HNSW      Random projections    Graph (Greedy) Speed            Good (small data)    Excellent            Excellent            Good                  Excellent GPU Support      No                   No                   Yes (major feature)  No                    No Dynamic Updates  Yes                  Yes                  Limited              No (rebuild)          Yes Distance Metrics Many                 Common               Many                 Limited               Common Memory Efficient Yes                  Moderate             Yes                  Yes                   Moderate Deterministic    Yes                  No (randomized)      Yes                  Yes                   Yes (fully) Maintenance      Well-maintained      Active               Well-maintained      Active                Active Dependencies     NumPy only           Minimal              Minimal              Minimal               Rust (native) ================ ==================== ==================== ==================== ==================== ====================</p> <p>Recommended Approach: 1. Default (no new deps): Use scikit-learn's KDTree/BallTree for standard cases    - Sufficient for small to medium datasets    - No additional dependencies    - Good for most use cases</p> <ol> <li>Optional (with deps): Allow users to opt-in to HNSWlib, FAISS, or HGG for large-scale deployments</li> <li>HNSWlib: Best for production use (Qdrant uses this, industry-standard)</li> <li>FAISS: Best for GPU-accelerated workloads</li> <li>HGG: Best for reproducibility and data-dependent optimization (fully deterministic)</li> <li> <p>All have minimal Python dependencies</p> </li> <li> <p>Fallback Strategy: Graceful degradation</p> </li> <li>Use sklearn by default</li> <li>Warn if user tries to use pynndescent features</li> <li>Auto-upgrade to HNSWlib if available and beneficial for large datasets</li> <li>Consider HGG for users requiring deterministic behavior</li> </ol> <p>Implementation Plan: 1. Audit all pynndescent usage in the codebase 2. Identify distance metrics that pynndescent provides that sklearn doesn't 3. Implement missing distance metrics using scipy.spatial.distance or native code 4. Create pluggable ANN backend abstraction layer (umap/nndescent.py)    - Design clean interface for swapping different ANN algorithms    - Support dynamic algorithm selection at runtime    - Handle algorithm-specific parameters and tuning 5. Implement support for multiple ANN algorithms (from ann-benchmarks.com):    - Default: scikit-learn KDTree/BallTree (no new deps)    - Optional: HNSWlib, FAISS, Annoy, NGT, NSW, HGG    - Design: Start with top performers, expand as needed 6. Implement fallback and distance metric handling for all backends 7. Implement comprehensive benchmark suite with automation:    - Set up script \u2192 JSON \u2192 graph pipeline (rust-analyzer-metrics model)    - Run full algorithm suite benchmarks on every commit    - Compare: speed, memory usage, recall accuracy, construction time    - Validate against ann-benchmarks.com reference datasets and results    - Track algorithm performance across different data characteristics    - Generate comparative performance graphs (updated with each commit)    - Identify optimal algorithms for different use cases    - Ensure no performance regressions during development 8. Update tests to work without pynndescent as required dependency 9. Make pynndescent optional in pyproject.toml (keep for users who want it) 10. Add algorithm selection guide in documentation     - Recommendations based on: dataset size, speed requirements, reproducibility needs     - Performance profiles from benchmarks     - Trade-off analysis: speed vs memory vs accuracy 11. Add runtime configuration for algorithm selection (config file or environment variables)</p> <p>Files to Modify: - umap/umap_.py (main implementation - remove pynndescent imports) - umap/distances.py (implement missing distance metrics) - umap/sparse.py (handle sparse nearest neighbors) - umap/nndescent.py (new abstraction layer for NN backends) - pyproject.toml (move pynndescent to optional, add HNSWlib as optional) - README.rst (update requirements and performance notes) - tests/ (ensure all tests pass without pynndescent) - doc/ (add backend selection guide) - benchmarks/ (new: benchmark suite infrastructure)</p> <p>Benchmark Infrastructure (Ideal for Benchmark-on-Commit Discovery Approach): This task is an excellent candidate for implementing automated benchmarking infrastructure to discover optimal algorithms:</p> <ul> <li>Algorithm Suite: Test all major ANN algorithms from ann-benchmarks.com</li> <li>Start with: sklearn, HNSWlib, FAISS, Annoy, HGG</li> <li>Expand to: NGT, NSW, Vespa, Milvus, others based on availability</li> <li>Metrics Tracking (JSON output):</li> <li>Speed: query time, construction time</li> <li>Memory: index size, working memory during search</li> <li>Accuracy: recall, precision on different datasets</li> <li>Scalability: performance vs dataset size</li> <li>Automated Benchmarking on Every Commit:</li> <li>Run full algorithm comparison on each commit to benchmark branch</li> <li>Compare against ann-benchmarks.com reference results</li> <li>JSON \u2192 CSV \u2192 automatic graph generation</li> <li>Track performance trends over time</li> <li>Discovery &amp; Optimization:</li> <li>Identify which algorithms are fastest for different scenarios</li> <li>Visualize speed vs memory vs accuracy trade-offs</li> <li>Find algorithm \"sweet spots\" for common UMAP use cases</li> <li>Auto-suggest best algorithm based on data characteristics</li> </ul> <p>This enables: - Real-time algorithm discovery: See which algorithms perform best as implementations improve - Data-driven decisions: No guessing - use actual performance data - Regression detection: Catch performance regressions immediately - Optimization targeting: Focus effort on algorithms that matter most - Historical analysis: Track which algorithms win under different conditions - Publication-ready data: Comprehensive benchmarks for academic papers</p> <p>Estimated Effort: Large (2-3 weeks for implementation + 2 weeks for benchmarking infrastructure &amp; algorithm integration)</p> <p>Priority: High (reduces coupling and external dependencies)</p> <p>Key Considerations: - Algorithm Trade-offs: Different algorithms excel in different scenarios   - HNSWlib: 10x+ faster for large datasets, randomized   - HGG: Fully deterministic, comparable speed, data-dependent   - FAISS: GPU acceleration, excellent for massive scale   - Annoy: Memory efficient, fast construction   - sklearn: Zero new dependencies, good for small-medium data   - Find optimal algorithm per use case through benchmarking - Determinism: HGG fully deterministic (unlike HNSW which uses randomization), important for reproducible research - Sparse matrices: scipy.sparse has limited distance metric support (critical for sparse UMAP) - Distance metrics: Not all custom metrics supported by all backends - design abstraction carefully - Pluggable Architecture: Design abstraction layer to make adding new algorithms trivial   - Clean interface for algorithm implementations   - Dynamic algorithm selection at runtime   - Per-algorithm configuration and tuning - Benchmarking Against Industry Standard:   - Use https://ann-benchmarks.com/index.html as reference   - Validate UMAP results match ann-benchmarks.com reference implementations   - Ensure UMAP is among fastest implementations for each algorithm - Rust-based Options: HGG and others could be wrapped with PyO3 for native performance - Research Reproducibility: Deterministic algorithms (sklearn, HGG) valuable for scientific use cases - Publication Potential: Comprehensive benchmarks comparing all algorithms is valuable for ML community - Performance Goals:   - Match or exceed pynndescent speed with fewer dependencies   - Enable users to find fastest algorithm for their specific use case   - Make UMAP a reference implementation for ANN algorithm evaluation</p>"},{"location":"development_roadmap/#4-code-modernization-and-technical-debt-reduction","title":"4. Code Modernization and Technical Debt Reduction","text":"<p>Status: In Progress</p> <p>Completed: - Identified outdated patterns (see analysis below)</p> <p>Pending:</p>"},{"location":"development_roadmap/#41-remove-python-2-artifacts","title":"4.1 Remove Python 2 Artifacts","text":"<ul> <li>Remove <code>from __future__ import print_function</code> from:</li> <li>umap/umap_.py</li> <li>umap/sparse.py</li> <li>Remove explicit <code>object</code> inheritance from old-style classes</li> <li>Update any remaining Python 2 compatibility code</li> </ul> <p>Estimated Effort: Small (&lt; 1 day)</p> <p>Priority: Medium</p>"},{"location":"development_roadmap/#42-fix-numpy-deprecated-type-aliases","title":"4.2 Fix NumPy Deprecated Type Aliases","text":"<ul> <li>Replace <code>np.bool_</code> with <code>bool</code> or <code>np.bool</code> in:</li> <li>umap/sparse.py</li> <li>umap/aligned_umap.py</li> <li>Replace other deprecated NumPy type aliases (np.int_, np.float_, etc.)</li> <li>Ensure compatibility with NumPy &gt;= 1.20</li> </ul> <p>Estimated Effort: Small (&lt; 1 day)</p> <p>Priority: High</p>"},{"location":"development_roadmap/#43-fix-bitwise-operator-misuse","title":"4.3 Fix Bitwise Operator Misuse","text":"<ul> <li>Replace bitwise AND (<code>&amp;</code>) with logical AND (<code>and</code>) in boolean contexts</li> <li>Affected files:</li> <li>umap/umap_.py (lines 119, 124, 426)</li> <li>This improves code clarity and prevents subtle bugs</li> </ul> <p>Estimated Effort: Small (&lt; 1 day)</p> <p>Priority: High</p>"},{"location":"development_roadmap/#44-optimize-data-structures","title":"4.4 Optimize Data Structures","text":"<ul> <li>Replace list-based queue with <code>collections.deque</code> in breadth_first_search()</li> <li>This changes O(n) queue.pop(0) operations to O(1) deque.popleft()</li> <li>Location: umap/umap_.py:93</li> </ul> <p>Estimated Effort: Small (&lt; 1 day)</p> <p>Priority: Medium</p>"},{"location":"development_roadmap/#45-resolve-technical-debt-fixmetodo-comments","title":"4.5 Resolve Technical Debt (FIXME/TODO Comments)","text":"<ul> <li>Address FIXME comments in umap/layouts.py regarding uninitialized variables</li> <li>Review TODO comments about performance optimizations</li> <li>Either implement the optimizations or update comments with justification</li> </ul> <p>Estimated Effort: Medium (2-3 days)</p> <p>Priority: Medium</p>"},{"location":"development_roadmap/#46-remove-dead-code","title":"4.6 Remove Dead Code","text":"<ul> <li>Remove sklearn.externals.joblib fallback code (dead since sklearn &gt;= 1.0)</li> <li>Location: umap/umap_.py</li> </ul> <p>Estimated Effort: Minimal (&lt; 1 hour)</p> <p>Priority: Low</p>"},{"location":"development_roadmap/#47-update-dependency-specifications","title":"4.7 Update Dependency Specifications","text":"<ul> <li>Review and update minimum version constraints in pyproject.toml</li> <li>scipy: currently &gt;= 1.3.1 (2019); should be &gt;= 1.8.0 or higher</li> <li>numba: currently &gt;= 0.51.2 (2020); should be &gt;= 0.55.0 or higher</li> <li>TensorFlow (if keeping): update to &gt;= 2.10.0</li> <li>Ensure minimum versions are tested in CI</li> </ul> <p>Estimated Effort: Small (&lt; 1 day)</p> <p>Priority: Low</p>"},{"location":"development_roadmap/#5-documentation-updates","title":"5. Documentation Updates","text":"<p>Status: Pending</p>"},{"location":"development_roadmap/#51-update-installation-instructions","title":"5.1 Update Installation Instructions","text":"<ul> <li>Remove all conda references</li> <li>Clarify Python 3.9+ requirement (currently lists 3.6+)</li> <li>Add virtual environment setup instructions</li> <li>Document development setup</li> </ul> <p>Estimated Effort: Small (1 day)</p> <p>Priority: Medium</p>"},{"location":"development_roadmap/#52-clarify-optional-dependencies","title":"5.2 Clarify Optional Dependencies","text":"<ul> <li>Separate optional dependencies clearly:</li> <li>Plotting: matplotlib, datashader, holoviews</li> <li>Parametric UMAP: PyTorch/TensorFlow (post-migration: PyTorch only)</li> <li>Development: pytest, typing-extensions, etc.</li> </ul> <p>Estimated Effort: Small (1 day)</p> <p>Priority: Medium</p>"},{"location":"development_roadmap/#53-update-development-status-classifier","title":"5.3 Update Development Status Classifier","text":"<ul> <li>pyproject.toml currently lists \"Development Status :: 3 - Alpha\"</li> <li>Should be updated to \"Development Status :: 5 - Production/Stable\"</li> <li>UMAP has been stable for years</li> </ul> <p>Estimated Effort: Minimal (&lt; 1 hour)</p> <p>Priority: Low</p>"},{"location":"development_roadmap/#6-testing-improvements","title":"6. Testing Improvements","text":"<p>Status: Pending</p>"},{"location":"development_roadmap/#61-add-type-hints","title":"6.1 Add Type Hints","text":"<ul> <li>Add Python type hints throughout the codebase</li> <li>Enables better IDE support and static type checking</li> <li>Use tools like mypy or pyright for validation</li> <li>Priority: Core modules first (umap/umap_.py, umap/sparse.py)</li> </ul> <p>Estimated Effort: Large (2-3 weeks for full coverage)</p> <p>Priority: Medium</p>"},{"location":"development_roadmap/#62-expand-test-coverage","title":"6.2 Expand Test Coverage","text":"<ul> <li>Add tests for edge cases and error conditions</li> <li>Add performance regression tests</li> <li>Ensure PyTorch implementation has equivalent test coverage to current</li> </ul> <p>Estimated Effort: Medium</p> <p>Priority: Medium after PyTorch migration</p>"},{"location":"development_roadmap/#long-term-considerations","title":"Long-term Considerations","text":"<ol> <li>GPU Acceleration: Consider native GPU support for core UMAP algorithm (not just Parametric UMAP)</li> <li>API Stability: Consider stabilizing the public API with semantic versioning</li> <li>Performance: Profile and optimize hot paths, especially in KNN computation</li> <li>Sparse Matrix Support: Expand and optimize sparse matrix handling</li> <li>Distributed Computing: Explore support for large-scale distributed embedding</li> </ol>"},{"location":"development_roadmap/#timeline","title":"Timeline","text":"<p>Near-term (1-2 months): - Complete code modernization (items 4.1-4.7) - Remove conda from documentation and CI (item 2) - COMPLETED - Remove pynndescent dependency (item 3) - Update documentation (item 5)</p> <p>Medium-term (2-3 months): - Begin PyTorch implementation of Parametric UMAP (item 1) - Add type hints to core modules (item 6.1)</p> <p>Long-term (3+ months): - Complete PyTorch migration (item 1) - Full type hint coverage (item 6.1) - Deprecate TensorFlow support</p>"},{"location":"development_roadmap/#contributing","title":"Contributing","text":"<p>Interested in helping with any of these initiatives? Please open an issue or pull request on the GitHub repository.</p> <p>For major work like the PyTorch migration, please discuss your approach with the maintainers first.</p>"},{"location":"document_embedding/","title":"Document embedding using UMAP","text":"<p>This is a tutorial of using UMAP to embed text (but this can be extended to any collection of tokens). We are going to use the <code>20 newsgroups dataset &lt;http://qwone.com/~jason/20Newsgroups/&gt;</code>__ which is a collection of forum posts labelled by topic. We are going to embed these documents and see that similar documents (i.e.\u00a0posts in the same subforum) will end up close together. You can use this embedding for other downstream tasks, such as visualizing your corpus, or run a clustering algorithm (e.g.\u00a0HDBSCAN). We will use a bag of words model and use UMAP on the count vectors as well as the TF-IDF vectors.</p> <p>To start with let's load the relevant libraries. This requires UMAP version &gt;= 0.4.0.</p> Python<pre><code>import pandas as pd\nimport umap\nimport umap.plot\n\n# Used to get the data\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n# Some plotting libraries\nimport matplotlib.pyplot as plt\n%matplotlib notebook\nfrom bokeh.plotting import show, save, output_notebook, output_file\nfrom bokeh.resources import INLINE\noutput_notebook(resources=INLINE)\n</code></pre> <p>Next let's download and explore the 20 newsgroups dataset.</p> Python<pre><code>%%time\ndataset = fetch_20newsgroups(subset='all',\n                             shuffle=True, random_state=42)\n</code></pre> Text Only<pre><code>CPU times: user 280 ms, sys: 52 ms, total: 332 ms\nWall time: 460 ms\n</code></pre> <p>Let's see the size of the corpus:</p> Python<pre><code>print(f'{len(dataset.data)} documents')\nprint(f'{len(dataset.target_names)} categories')\n</code></pre> Text Only<pre><code>18846 documents\n20 categories\n</code></pre> <p>Here are the categories of documents. As you can see many are related to one another (e.g. \u2018comp.sys.ibm.pc.hardware\u2019 and \u2018comp.sys.mac.hardware\u2019) but they are not all correlated (e.g. \u2018sci.med\u2019 and \u2018rec.sport.baseball\u2019).</p> Python<pre><code>dataset.target_names\n</code></pre> Text Only<pre><code>['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']\n</code></pre> <p>Let\u2019s look at a couple of sample documents:</p> Python<pre><code>for idx, document in enumerate(dataset.data[:3]):\n    category = dataset.target_names[dataset.target[idx]]\n\n    print(f'Category: {category}')\n    print('---------------------------')\n    # Print the first 500 characters of the post\n    print(document[:500])\n    print('---------------------------')\n</code></pre> Text Only<pre><code>Category: rec.sport.hockey\n---------------------------\nFrom: Mamatha Devineni Ratnam &lt;mr47+@andrew.cmu.edu&gt;\nSubject: Pens fans reactions\nOrganization: Post Office, Carnegie Mellon, Pittsburgh, PA\nLines: 12\nNNTP-Posting-Host: po4.andrew.cmu.edu\n\n\n\nI am sure some bashers of Pens fans are pretty confused about the lack\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\nare killin\n---------------------------\nCategory: comp.sys.ibm.pc.hardware\n---------------------------\nFrom: mblawson@midway.ecn.uoknor.edu (Matthew B Lawson)\nSubject: Which high-performance VLB video card?\nSummary: Seek recommendations for VLB video card\nNntp-Posting-Host: midway.ecn.uoknor.edu\nOrganization: Engineering Computer Network, University of Oklahoma, Norman, OK, USA\nKeywords: orchid, stealth, vlb\nLines: 21\n\n  My brother is in the market for a high-performance video card that supports\nVESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:\n\n  - Diamond Stealth Pro Local\n---------------------------\nCategory: talk.politics.mideast\n---------------------------\nFrom: hilmi-er@dsv.su.se (Hilmi Eren)\nSubject: Re: ARMENIA SAYS IT COULD SHOOT DOWN TURKISH PLANES (Henrik)\nLines: 95\nNntp-Posting-Host: viktoria.dsv.su.se\nReply-To: hilmi-er@dsv.su.se (Hilmi Eren)\nOrganization: Dept. of Computer and Systems Sciences, Stockholm University\n\n\n\n\n\\|&gt;The student of \"regional killings\" alias Davidian (not the Davidian religios sect) writes:\n\n\n\\|&gt;Greater Armenia would stretch from Karabakh, to the Black Sea, to the\n\\|&gt;Mediterranean, so if you use the term \"Greater Armenia\n---------------------------\n</code></pre> <p>Now we will create a dataframe with the target labels to be used in plotting. This will allow us to see the newsgroup when we hover over the plotted points (if using interactive plotting). This will help us evaluate (by eye) how good the embedding looks.</p> Python<pre><code>category_labels = [dataset.target_names[x] for x in dataset.target]\nhover_df = pd.DataFrame(category_labels, columns=['category'])\n</code></pre>"},{"location":"document_embedding/#using-raw-counts","title":"Using raw counts","text":"<p>Next, we are going to use a bag-of-words approach (i.e.\u00a0word order doesn\u2019t matter) and construct a word document matrix. In this matrix the rows will correspond to a document (i.e.\u00a0post) and each column will correspond to a particular word. The values will be the counts of how many times a given word appeared in a particular document.</p> <p>We will use sklearns CountVectorizer function to do this for us along with a couple other preprocessing steps:</p> <p>1) Split the text into tokens (i.e.\u00a0words) by splitting on whitespace</p> <p>2) Remove english stopwords (the, and, etc)</p> <p>3) Remove all words which occur less than 5 times in the entire corpus    (via the min_df parameter)</p> Python<pre><code>vectorizer = CountVectorizer(min_df=5, stop_words='english')\nword_doc_matrix = vectorizer.fit_transform(dataset.data)\n</code></pre> <p>This gives us a 18846x34880 matrix where there are 18846 documents (same as above) and 34880 unique tokens. This matrix is sparse since most words do not appear in most documents.</p> Python<pre><code>word_doc_matrix\n</code></pre> Text Only<pre><code>&lt;18846x34880 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 1939023 stored elements in Compressed Sparse Row format&gt;\n</code></pre> <p>Now we are going to do dimension reduction using UMAP to reduce the matrix from 34880 dimensions to 2 dimensions (since n_components=2). We need a distance metric and will use <code>Hellinger distance &lt;https://en.wikipedia.org/wiki/Hellinger_distance&gt;</code>__ which measures the similarity between two probability distributions. Each document has a set of counts generated by a <code>multinomial distribution &lt;https://en.wikipedia.org/wiki/Multinomial_distribution&gt;</code>__ where we can use Hellinger distance to measure the similarity of these distributions.</p> Python<pre><code>%%time\nembedding = umap.UMAP(n_components=2, metric='hellinger').fit(word_doc_matrix)\n</code></pre> Text Only<pre><code>CPU times: user 2min 24s, sys: 1.18 s, total: 2min 25s\nWall time: 2min 3s\n</code></pre> <p>Now we have an embedding of 18846x2.</p> Python<pre><code>embedding.embedding_.shape\n</code></pre> Text Only<pre><code>(18846, 2)\n</code></pre> <p>Let\u2019s plot the embedding. If you are running this in a notebook, you should use the interactive plotting method as it lets you hover over your points and see what category they belong to.</p> Python<pre><code># For interactive plotting use\n# f = umap.plot.interactive(embedding, labels=dataset.target, hover_data=hover_df, point_size=1)\n# show(f)\nf = umap.plot.points(embedding, labels=hover_df['category'])\n</code></pre> <p></p> <p>As you can see this does reasonably well. There is some separation and groups that you would expect to be similar (such as \u2018rec.sport.baseball\u2019 and \u2018rec.sport.hockey\u2019) are close together. The big clump in the middle corresponds to a lot of extremely similar newsgroups like \u2018comp.sys.ibm.pc.hardware\u2019 and \u2018comp.sys.mac.hardware\u2019.</p>"},{"location":"document_embedding/#using-tf-idf","title":"Using TF-IDF","text":"<p>We will now do the same pipeline with the only change being the use of TF-IDF weighting. TF-IDF gives less weight to words that appear frequently across a large number of documents since they are more popular in general. It asserts a higher weight to words that appear frequently in a smaller subset of documents since they are probably important words for those documents.</p> <p>To do the TF-IDF weighting we will use sklearns TfidfVectorizer with the same parameters as CountVectorizer above.</p> Python<pre><code>tfidf_vectorizer = TfidfVectorizer(min_df=5, stop_words='english')\ntfidf_word_doc_matrix = tfidf_vectorizer.fit_transform(dataset.data)\n</code></pre> <p>We get a matrix of the same size as before:</p> Python<pre><code>tfidf_word_doc_matrix\n</code></pre> Text Only<pre><code>&lt;18846x34880 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 1939023 stored elements in Compressed Sparse Row format&gt;\n</code></pre> <p>Again we use Hellinger distance and UMAP to embed the documents</p> Python<pre><code>%%time\ntfidf_embedding = umap.UMAP(metric='hellinger').fit(tfidf_word_doc_matrix)\n</code></pre> Text Only<pre><code>CPU times: user 2min 19s, sys: 1.27 s, total: 2min 20s\nWall time: 1min 57s\n</code></pre> Python<pre><code># For interactive plotting use\n# fig = umap.plot.interactive(tfidf_embedding, labels=dataset.target, hover_data=hover_df, point_size=1)\n# show(fig)\nfig = umap.plot.points(tfidf_embedding, labels=hover_df['category'])\n</code></pre> <p></p> <p>The results look fairly similar to before but this can be a useful trick to have in your toolbox.</p>"},{"location":"document_embedding/#potential-applications","title":"Potential applications","text":"<p>Now that we have an embedding, what can we do with it?</p> <ul> <li>Explore/visualize your corpus to identify topics/trends</li> <li>Cluster the embedding to find groups of related documents</li> <li>Look for nearest neighbours to find related documents</li> <li>Look for anomalous documents</li> </ul>"},{"location":"embedding_space/","title":"Embedding to non-Euclidean spaces","text":"<p>By default UMAP embeds data into Euclidean space. For 2D visualization that means that data is embedded into a 2D plane suitable for a scatterplot. In practice, however, there aren't really any major constraints that prevent the algorithm from working with other more interesting embedding spaces. In this tutorial we'll look at how to get UMAP to embed into other spaces, how to embed into your own custom space, and why this sort of approach might be useful.</p> <p>To start we'll load the usual selection of libraries. In this case we will not be using the <code>umap.plot</code> functionality, but working with matplotlib directly since we'll be generating some custom visualizations for some of the more unique embedding spaces.</p> Python<pre><code>import numpy as np\nimport numba\nimport sklearn.datasets\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\nimport umap\n%matplotlib inline\n</code></pre> Python<pre><code>sns.set(style='white', rc={'figure.figsize':(10,10)})\n</code></pre> <p>As a test dataset we'll use the PenDigits dataset from sklearn -- embedding into exotic spaces can be considerably more computationally taxing, so a simple relatively small dataset is going to be useful.</p> Python<pre><code>digits = sklearn.datasets.load_digits()\n</code></pre>"},{"location":"embedding_space/#plane-embeddings","title":"Plane embeddings","text":"<p>Plain old plane embeddings are simple enough -- it is the default for UMAP. Here we'll run through the example again, just to ensure you are familiar with how this works, and what the result of a UMAP embedding of the PenDigits dataset looks like in the simple case of embedding in the plane.</p> Python<pre><code>plane_mapper = umap.UMAP(random_state=42).fit(digits.data)\n</code></pre> Python<pre><code>plt.scatter(plane_mapper.embedding_.T[0], plane_mapper.embedding_.T[1], c=digits.target, cmap='Spectral')\n</code></pre> <p></p>"},{"location":"embedding_space/#spherical-embeddings","title":"Spherical embeddings","text":"<p>What if we wanted to embed data onto a sphere rather than a plane? This might make sense, for example, if we have reason to expect some sort of periodic behaviour or other reasons to expect that no point can be infinitely far from any other. To make UMAP embed onto a sphere we need to make use of the <code>output_metric</code> parameter, which specifies what metric to use for the output space. By default UMAP uses a Euclidean <code>output_metric</code> (and even has a special faster code-path for this case), but you can pass in other metrics. Among the metrics UMAP supports is the Haversine metric, used for measuring distances on a sphere, given in latitude and longitude (in radians). If we set the <code>output_metric</code> to <code>\"haversine\"</code> then UMAP will use that to measure distance in the embedding space.</p> Python<pre><code>sphere_mapper = umap.UMAP(output_metric='haversine', random_state=42).fit(digits.data)\n</code></pre> <p>The result is the pendigits data embedded with respect to haversine distance on a sphere. The catch is that if we visualize this naively then we will get nonsense.</p> Python<pre><code>plt.scatter(sphere_mapper.embedding_.T[0], sphere_mapper.embedding_.T[1], c=digits.target, cmap='Spectral')\n</code></pre> <p></p> <p>What has gone astray is that under the embedding distance metric a point at :math:<code>(0, \\pi)</code> is distance zero from a point at :math:<code>(0, 3\\pi)</code> since that will wrap all the way around the equator. You'll note that the scales on the x and y axes of the above plot go well outside the ranges :math:<code>(-\\pi, \\pi)</code> and :math:<code>(0, 2\\pi)</code>, so this isn't the right representation of the data. We can, however, use straightforward formulas to map this data onto a sphere embedded in 3d-space.</p> Python<pre><code>x = np.sin(sphere_mapper.embedding_[:, 0]) * np.cos(sphere_mapper.embedding_[:, 1])\ny = np.sin(sphere_mapper.embedding_[:, 0]) * np.sin(sphere_mapper.embedding_[:, 1])\nz = np.cos(sphere_mapper.embedding_[:, 0])\n</code></pre> <p>Now <code>x</code>, <code>y</code>, and <code>z</code> give 3d coordinates for each embedding point that lies on the surface of a sphere. We can visualize this using matplotlib's 3d plotting capabilities, and see that we have in fact induced a quite reasonable embedding of the data onto the surface of a sphere.</p> Python<pre><code>fig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z, c=digits.target, cmap='Spectral')\n</code></pre> <p></p> <p>If you prefer a 2d plot we can convert these into lat/long coordinates in the appropriate ranges and get the equivalent of a map projection of the sphere data.</p> Python<pre><code>x = np.arctan2(x, y)\ny = -np.arccos(z)\n</code></pre> Python<pre><code>plt.scatter(x, y, c=digits.target.astype(np.int32), cmap='Spectral')\n</code></pre> <p></p>"},{"location":"embedding_space/#embedding-on-a-custom-metric-space","title":"Embedding on a Custom Metric Space","text":"<p>What if you have some other custom notion of a metric space that you would like to embed data into? In the same way that UMAP can support custom written distance metrics for the input data (as long as they can be compiled with numba), the <code>output_metric</code> parameter can accept custom distance functions. One catch is that, to support gradient descent optimization, the distance function needs to return both the distance, and a vector for the gradient of the distance. This latter point may require a little bit of calculus on the users part. A second catch is that it is highly beneficial to parameterize the embedding space in a way that has no coordinate constraints -- otherwise the gradient descent may step a point outside the embedding space, resulting in bad things happening. This is why, for example, the sphere example simply has points wrap around rather than constraining coordinates to be in the appropriate ranges.</p> <p>Let's work through an example where we construct a distance metric and gradient for a different sort of space: a torus. A torus is essentially just the outer surface of a donut. We can parameterize the torus in terms of x, y coordinates with the caveat that we can <code>\"wrap around\" (similar to the sphere) &lt;https://en.wikipedia.org/wiki/Torus#Flat_torus&gt;</code>__. In such a model distances are mostly just euclidean distances, we just have to check for which is the shorter direction -- across or wrapping around -- and ensure we account for the equivalence of wrapping around several times. We can write a simple function to calculate that.</p> Python<pre><code>@numba.njit(fastmath=True)\ndef torus_euclidean_grad(x, y, torus_dimensions=(2*np.pi,2*np.pi)):\n    \"\"\"Standard euclidean distance.\n\n    ..math::\n        D(x, y) = \\sqrt{\\sum_i (x_i - y_i)^2}\n    \"\"\"\n    distance_sqr = 0.0\n    g = np.zeros_like(x)\n    for i in range(x.shape[0]):\n        a = abs(x[i] - y[i])\n        if 2*a &lt; torus_dimensions[i]:\n            distance_sqr += a ** 2\n            g[i] = (x[i] - y[i])\n        else:\n            distance_sqr += (torus_dimensions[i]-a) ** 2\n            g[i] = (x[i] - y[i]) * (a - torus_dimensions[i]) / a\n    distance = np.sqrt(distance_sqr)\n    return distance, g/(1e-6 + distance)\n</code></pre> <p>Note that the gradient just derives from the standard euclidean gradient, we just have to check the direction according to the way we've wrapped around to compute the distance. We can now plug that function directly in to the <code>output_metric</code> parameter and end up embedding data on a torus.</p> Python<pre><code>torus_mapper = umap.UMAP(output_metric=torus_euclidean_grad, random_state=42).fit(digits.data)\n</code></pre> <p>As with the sphere case, a naive visualisation will look strange, due the the wrapping around and equivalence of looping several times. But, also just like the torus, we can construct a suitable visualization by computing the 3d coordinates for the points using a little bit of straightforward geometry (yes, I still had to look it up to check).</p> Python<pre><code>R = 3 # Size of the doughnut circle\nr = 1 # Size of the doughnut cross-section\n\nx = (R + r * np.cos(torus_mapper.embedding_[:, 0])) * np.cos(torus_mapper.embedding_[:, 1])\ny = (R + r * np.cos(torus_mapper.embedding_[:, 0])) * np.sin(torus_mapper.embedding_[:, 1])\nz = r * np.sin(torus_mapper.embedding_[:, 0])\n</code></pre> <p>Now we can visualize the result using matplotlib and see that, indeed, the data has been suitably embedded onto a torus.</p> Python<pre><code>fig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z, c=digits.target, cmap='Spectral')\nax.set_zlim3d(-3, 3)\nax.view_init(35, 70)\n</code></pre> <p></p> <p>And as with the torus we can do a little geometry and unwrap the torus into a flat plane with the appropriate bounds.</p> Python<pre><code>u = np.arctan2(x,y)\nv = np.arctan2(np.sqrt(x**2 + y**2) - R, z)\n</code></pre> Python<pre><code>plt.scatter(u, v, c=digits.target, cmap='Spectral')\n</code></pre> <p></p>"},{"location":"embedding_space/#a-practical-example","title":"A Practical Example","text":"<p>While the examples given so far may have some use (because some data does have suitable periodic or looping structures that we expect will be better represented in a sphere or a torus), most data doesn't really fall in the realm of something that a user can, apriori, expect to lie on an exotic manifold. Are there more practical uses for the ability to embed in other spaces? It turns out that there are. One interesting example to consider is the space formed by 2d-Gaussian distributions. We can measure the distance between two Gaussians (parameterized by a 2d vector for the mean, and 2x2 matrix giving the covariance) by the negative log of the inner product between the PDFs (since this has a nice closed form solution, and is reasonably computable). That gives us a metric space to embed into where samples are represented not as points in 2d, but as Gaussian distributions in 2d, encoding some uncertainty in how each sample in the high dimensional space is to be embedded.</p> <p>Of course we still have the issues of parameterizations that are suitable for SGD -- requiring that the covariance matrix be symmetric and positive definite is challenging. Instead we can parameterize the covariance in terms of a width, height and angle, and recover the covariance matrix from these if required. That gives us a total of 5 components to embed into (two for the mean, 3 for parameters describing the covariance). We can simply do this since the appropriate metric is defined already. Note that we have to specifically pass <code>n_components=5</code> since we need to explicitly embed into a 5 dimensional space to support all the covariance parameters associated to 2d Gaussians.</p> Python<pre><code>gaussian_mapper = umap.UMAP(output_metric='gaussian_energy',\n                            n_components=5,\n                            random_state=42).fit(digits.data)\n</code></pre> <p>Since we have embedded the data into a 5 dimensional space visualization is not as trivial as it was earlier. We can get a start on visualizing the results by looking at just the means, which are the 2d locations of the modes of the Gaussians. A traditional scatter plot will suffice for this.</p> Python<pre><code>plt.scatter(gaussian_mapper.embedding_.T[0], gaussian_mapper.embedding_.T[1], c=digits.target, cmap='Spectral')\n</code></pre> <p></p> <p>We see that we have gotten a result similar to a standard embedding into euclidean space, but with less clear clustering, and more points between clusters. To get a clearer idea of what is going on it will be necessary to devise a means to display some of the extra information contained in the extra 3 dimensions providing covariance data. To do this it will be helpful to be able to draw ellipses corresponding to super-level sets of the PDF of the 2d Gaussian. We can start on this by writing a simple function to draw ellipses on a plot accoriding to a position, a width, a height, and an angle (since this is the format the embedding computed the data).</p> Python<pre><code>from matplotlib.patches import Ellipse\n\ndef draw_simple_ellipse(position, width, height, angle,\n                        ax=None, from_size=0.1, to_size=0.5, n_ellipses=3,\n                        alpha=0.1, color=None,\n                        **kwargs):\n    ax = ax or plt.gca()\n    angle = (angle / np.pi) * 180\n    width, height = np.sqrt(width), np.sqrt(height)\n    # Draw the Ellipse\n    for nsig in np.linspace(from_size, to_size, n_ellipses):\n        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n                             angle, alpha=alpha, lw=0, color=color, **kwargs))\n</code></pre> <p>Now we can plot the data by providing a scatterplot of the centers (as before), but overlaying that over a super-level-set ellipses of the associated Gaussians. The obvious catch is that this will induce a lot of over-plotting, but it will at least provide a way to start understanding the embedding we have produced.</p> Python<pre><code>fig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(111)\ncolors = plt.get_cmap('Spectral')(np.linspace(0, 1, 10))\nfor i in range(gaussian_mapper.embedding_.shape[0]):\n    pos = gaussian_mapper.embedding_[i, :2]\n    draw_simple_ellipse(pos, gaussian_mapper.embedding_[i, 2],\n                        gaussian_mapper.embedding_[i, 3],\n                        gaussian_mapper.embedding_[i, 4],\n                        ax, color=colors[digits.target[i]],\n                        from_size=0.2, to_size=1.0, alpha=0.05)\nax.scatter(gaussian_mapper.embedding_.T[0],\n           gaussian_mapper.embedding_.T[1],\n           c=digits.target, cmap='Spectral', s=3)\n</code></pre> <p></p> <p>Now we can see that the covariance structure for the points can vary greatly, both in absolute size, and in shape. We note that many of the points falling between clusters have much larger variances, in a sense representing the greater uncertainty of the location of the embedding. It is also worth noting that the shape of the ellipses can vary significantly -- there are several very stretched ellipses, quite distinct from many of the very round ellipses; in a sense this represents where the uncertainty falls more along a single line for example.</p> <p>While this plot highlights some of the covariance structure in the outlying points, in practice the overplotting here obscures a lot of the more interesting structure in the clusters themselves. We can try to see this structure better by plotting only a single ellipse per point and using a lower alpha channel value for the ellipses, making them more translucent.</p> Python<pre><code>fig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(111)\nfor i in range(gaussian_mapper.embedding_.shape[0]):\n    pos = gaussian_mapper.embedding_[i, :2]\n    draw_simple_ellipse(pos, gaussian_mapper.embedding_[i, 2],\n                        gaussian_mapper.embedding_[i, 3],\n                        gaussian_mapper.embedding_[i, 4],\n                        ax, n_ellipses=1,\n                        color=colors[digits.target[i]],\n                        from_size=1.0, to_size=1.0, alpha=0.01)\nax.scatter(gaussian_mapper.embedding_.T[0],\n           gaussian_mapper.embedding_.T[1],\n           c=digits.target, cmap='Spectral', s=3)\n</code></pre> <p></p> <p>This lets us see the variation of density of clusters with respect to the covariance structure -- some clusters have consistently very tight covariance, while others are more spread out (and hence have, in a sense, greater associated uncertainty. Of course we still have a degree of overplotting even here, and it will become increasingly difficult to tune alpha channels to make things visible. Instead what we would want is an actual density plot, showing the the density of the sum over all of these Gaussians.</p> <p>To do this we'll need to define some functions, whose execution will be accelerated using numba: the evaluation of the density of a 2d Gaussian at a given point; an evaluation of the density of a given point summing over a set of several Gaussians; and a function to generate the density for each point in some grid (summing only over nearby Gaussians to make this naive approach more computable).</p> Python<pre><code>from sklearn.neighbors import KDTree\n\n@numba.njit(fastmath=True)\ndef eval_gaussian(x, pos=np.array([0, 0]), cov=np.eye(2, dtype=np.float32)):\n    det = cov[0,0] * cov[1,1] - cov[0,1] * cov[1,0]\n    if det &gt; 1e-16:\n        cov_inv = np.array([[cov[1,1], -cov[0,1]], [-cov[1,0], cov[0,0]]]) * 1.0 / det\n        diff = x - pos\n        m_dist = cov_inv[0,0] * diff[0]**2 - \\\n            (cov_inv[0,1] + cov_inv[1,0]) * diff[0] * diff[1] + \\\n            cov_inv[1,1] * diff[1]**2\n        return (np.exp(-0.5 * m_dist)) / (2 * np.pi * np.sqrt(np.abs(det)))\n    else:\n        return 0.0\n\n@numba.njit(fastmath=True)\ndef eval_density_at_point(x, embedding):\n    result = 0.0\n    for i in range(embedding.shape[0]):\n        pos = embedding[i, :2]\n        t = embedding[i, 4]\n        U = np.array([[np.cos(t), np.sin(t)], [np.sin(t), -np.cos(t)]])\n        cov = U @ np.diag(embedding[i, 2:4]) @ U\n        result += eval_gaussian(x, pos=pos, cov=cov)\n    return result\n\ndef create_density_plot(X, Y, embedding):\n    Z = np.zeros_like(X)\n    tree = KDTree(embedding[:, :2])\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            nearby_points = embedding[tree.query_radius([[X[i,j],Y[i,j]]], r=2)[0]]\n            Z[i, j] = eval_density_at_point(np.array([X[i,j],Y[i,j]]), nearby_points)\n    return Z / Z.sum()\n</code></pre> <p>Now we simply need an appropriate grid of points. We can use the plot bounds seen above, and a grid size selected for the sake of computability. The numpy <code>meshgrid</code> function can supply the actual grid.</p> Python<pre><code>X, Y = np.meshgrid(np.linspace(-7, 9, 300), np.linspace(-8, 8, 300))\n</code></pre> <p>Now we can use the function defined above to compute the density at each point in the grid, given the Gaussians produced by the embedding.</p> Python<pre><code>Z = create_density_plot(X, Y, gaussian_mapper.embedding_)\n</code></pre> <p>Now we can view the result as a density plot using <code>imshow</code>.</p> Python<pre><code>plt.imshow(Z, origin='lower', cmap='Reds', extent=(-7, 9, -8, 8), vmax=0.0005)\nplt.colorbar()\n</code></pre> <p></p> <p>Here we see the finer structure within the various clusters, including some of the interesting linear structures, demonstrating that this Gaussian uncertainty based embedding has captured quite detailed and useful information about the inter-relationships among the PenDigits dataset.</p>"},{"location":"embedding_space/#bonus-embedding-in-hyperbolic-space","title":"Bonus: Embedding in Hyperbolic space","text":"<p>As a bonus example let's look at embedding data into hyperbolic space. The most popular model for this for visualization is <code>Poincare's disk model &lt;https://en.wikipedia.org/wiki/Poincar%C3%A9_disk_model&gt;</code>__. An example of a regular tiling of hyperbolic space in Poincare's disk model is shown below; you may note it is similar to famous images by M.C. Escher.</p> <p></p> <p>Ideally we would be able to embed directly into this Poincare disk model, but in practice this proves to be very difficult. The issue is that the disk has a \"line at infinity\" in a circle of radius one bounding the disk. Outside of that circle things are not well defined. As you may recall from the discussion of embedding onto spheres and toruses it is best if we can have a parameterisation of the embedding space that it is hard to move out of. The Poincare disk model is almost the opposite of this -- as soon as we move outside the unit circle we have moved off the manifold and further updates will be badly defined. We therefore instead need a different parameterisation of hyperbolic space that is less constrained. One option is the Poincare half-plane model, but this, again, has a boundary that it is easy to move beyond. The simplest option is the <code>hyperboloid model &lt;https://en.wikipedia.org/wiki/Hyperboloid_model&gt;</code>__. Under this model we can simply move in x and y coordinates, and solve for the corresponding z coordinate when we need to compute distances. This model has been implemented under the distance metric <code>\"hyperboloid\"</code> so we can simply use it out-of-the-box.</p> Python<pre><code>hyperbolic_mapper = umap.UMAP(output_metric='hyperboloid',\n                              random_state=42).fit(digits.data)\n</code></pre> <p>A straightforward visualization option is to simply view the x and y coordinates we have arrived at:</p> Python<pre><code>plt.scatter(hyperbolic_mapper.embedding_.T[0],\n            hyperbolic_mapper.embedding_.T[1],\n            c=digits.target, cmap='Spectral')\n</code></pre> <p></p> <p>We can also solve for the z coordinate and view the data lying on a hyperboloid in 3d space.</p> Python<pre><code>x = hyperbolic_mapper.embedding_[:, 0]\ny = hyperbolic_mapper.embedding_[:, 1]\nz = np.sqrt(1 + np.sum(hyperbolic_mapper.embedding_**2, axis=1))\n</code></pre> Python<pre><code>fig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z, c=digits.target, cmap='Spectral')\nax.view_init(35, 80)\n</code></pre> <p></p> <p>But we can do more -- since we have embedded the data successfully in hyperbolic space we can map the data into the Poincare disk model. This is, in fact, a straightforward computation.</p> Python<pre><code>disk_x = x / (1 + z)\ndisk_y = y / (1 + z)\n</code></pre> <p>Now we can visualize the data in a Poincare disk model embedding as we first wanted. For this we simply generate a scatterplot of the data, and then draw in the bounding circle of the line at infinity.</p> Python<pre><code>fig = plt.figure()\nax = fig.add_subplot(111)\nax.scatter(disk_x, disk_y, c=digits.target, cmap='Spectral')\nboundary = plt.Circle((0,0), 1, fc='none', ec='k')\nax.add_artist(boundary)\nax.axis('off');\n</code></pre> <p></p> <p>Hopefully this has provided a useful example of how to go about embedding into non-euclidean spaces. This last example ideally highlights the limitations of this approach (we really need a suitable parameterisation), and some potential approaches to get around this: we can use an alternative parameterisation for the embedding, and then transform the data into the desired representation.</p>"},{"location":"exploratory_analysis/","title":"Exploratory Analysis of Interesting Datasets","text":"<p>UMAP is a useful tool for general exploratory analysis of data -- it can provide a unique lens through which to view data that can highlight structures and properties hiding in data that are not as apparent when analysed with other techniques. Below is a selection of uses cases of UMAP being used for interesting explorations of intriguing datasets -- everything from pure math and outputs of neural networks, to philosophy articles, and scientific texts.</p>"},{"location":"exploratory_analysis/#prime-factorizations-of-numbers","title":"Prime factorizations of numbers","text":"<p>What would happen if we applied UMAP to the integers? First we would need a way to express an integer in a high dimensional space. That can be done by looking at the prime factorization of each number. Next you have to take enough numbers to actually generate an interesting visualization. John Williamson set about doing exactly this, and the results are fascinating. While they may not actually tell us anything new about number theory they do highlight interesting structures in prime factorizations, and demonstrate how UMAP can aid in interesting explorations of datasets that we might think we know well. It's worth visiting the linked article below as Dr. Williamson provides a rich and detailed exploration of UMAP as applied to prime factorizations of integers.</p> <p></p> <p>UMAP on prime factorizations</p> <p>Thanks to John Williamson.</p>"},{"location":"exploratory_analysis/#structure-of-recent-philosophy","title":"Structure of Recent Philosophy","text":"<p>Philosophy is an incredibly diverse subject, ranging from social and moral philosophy to logic and philosophy of math; from analysis of ancient Greek philosophy to modern business ethics. If we could get an overview of all the philosophy papers published in the last century what might it look like? Maximilian Noichl provides just such an exploration, looking at a large sampling of philosophy papers and comparing them according to their citations. The results are intriguing, and can be explored interactively in the viewer Maximilian built for it.</p> <p></p> <p>Structure of Recent Philosophy</p> <p>Thanks to Maximilian Noichl.</p>"},{"location":"exploratory_analysis/#language-context-and-geometry-in-neural-networks","title":"Language, Context, and Geometry in Neural Networks","text":"<p>Among recent developments in natural language processing is the BERT neural network based technique for analysis of language. Among many things that BERT can do one is context sensitive embeddings of words -- providing numeric vector representations of words that are sensitive to the context of how the word is used. Exactly what goes on inside the neural network to do this is a little mysterious (since the network is very complex with many many parameters). A tram of researchers from Google set out to explore the word embedding space generated by BERT, and among the tools used was UMAP. The linked blog post provides a detailed and inspiring analysis of what BERT's word embeddings look like, and how the different layers of BERT represent different aspects of language.</p> <p></p> <p>Language, Context, and Geometry in Neural Networks</p> <p>Thanks to Andy Coenen, Emily Reif, Ann Yuan, Been Kim, Adam Pearce, Fernanda Vi\u00e9gas, and Martin Wattenberg.</p>"},{"location":"exploratory_analysis/#activation-atlas","title":"Activation Atlas","text":"<p>Understanding the image processing capabilities (and deficits!) of modern convolutional neural networks is a challenge. Certainly these models are capable of amazing feats in, for example, image classification. They can also be brittle in unexpected ways, with carefully designed images able to induce otherwise baffling mis-classifications. To better understand this researchers from Google and OpenAI built the activation atlas -- analysing the space of activations of a neural network. Here UMAP provides a means to compress the activation landscape down to 2 dimensions for visualization. The result was an impressive interactive paper in the Distill journal, providing rich visualizations and new insights into the working of convolutional neural networks.</p> <p></p> <p>The Activation Atlas</p> <p>Thanks to Shan Carter, Zan Armstrong, Ludwig Schubert, Ian Johnson, and Chris Olah.</p>"},{"location":"exploratory_analysis/#open-syllabus-galaxy","title":"Open Syllabus Galaxy","text":"<p>Suppose you wanted to explore the space of commonly assigned texts from Open Syllabus? That gives you over 150,000 texts to consider. Since the texts are open you can actually analyse the text content involved. With some NLP and neural network wizardry David McClure build a network of such texts and then used node2vec and UMAP to generate a map of them. The result is a galaxy of textbooks showing inter-relationships between subjects, similar and related texts, and generally just a an interesting ladscape of science to be explored. As with some of the other projects here David made a great interactive viewer allowing for rich exploration of the results.</p> <p></p> <p>Open Syllabus Galaxy</p> <p>Thanks to David McClure.</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":"<p>Compiled here are a set of frequently asked questions, along with answers. If you don't find your question listed here then please feel free to add an issue on github. More questions are always welcome, and the authors will do their best to answer. If you feel you have a common question that isn't answered here then please suggest that the question (and answer) be added to the FAQ when you file the issue.</p>"},{"location":"faq/#should-i-normalise-my-features","title":"Should I normalise my features?","text":"<p>The default answer is yes, but, of course, the real answer is \"it depends\". If your features have meaningful relationships with one another (say, latitude and longitude values) then normalising per feature is not a good idea. For features that are essentially independent it does make sense to get all the features on (relatively) the same scale. The best way to do this is to use pre-processing tools from scikit-learn. All the advice given there applies as sensible preprocessing for UMAP, and since UMAP is scikit-learn compatible you can put all of this together into a scikit-learn pipeline.</p>"},{"location":"faq/#can-i-cluster-the-results-of-umap","title":"Can I cluster the results of UMAP?","text":"<p>This is hard to answer well, but essentially the answer is \"yes, with care\". To start with it matters what clustering algorithm you are going to use. Since UMAP does not necessarily produce clean spherical clusters something like K-Means is a poor choice. I would recommend HDBSCAN or similar. The catch here is that UMAP, with its uniform density assumption, does not preserve density well. What UMAP will do, however, is contract connected components of the manifold together. Providing you have enough data for UMAP to distinguish that information then you can get useful clustering results out since algorithms like HDBSCAN will easily pick out the components after applying UMAP.</p> <p>UMAP does offer significant improvements over algorithms like t-SNE for clustering. First, by preserving more global structure and creating meaningful separation between connected components of the manifold on which the data lies, UMAP offers more meaningful clusters. Second, because it supports arbitrary embedding dimensions, UMAP allows embedding to larger dimensional spaces that make it more amenable to clustering.</p>"},{"location":"faq/#the-clusters-are-all-squashed-together-and-i-cant-see-internal-structure","title":"The clusters are all squashed together and I can't see internal structure","text":"<p>One of UMAPs goals is to have distance between clusters of points be meaningful. This means that clusters can end up spread out with a fair amount of space between them. As a result the clusters themselves can end up more visually packed together than in, say, t-SNE. This is intended. A catch, however, is that many plots (for example matplotlib's scatter plot with default parameters) tend to show the clusters only as indistinct blobs with no internal structure. The solution for this is really a matter of tuning the plot more than anything else.</p> <p>If you are using matplotlib consider using the <code>s</code> parameter that specifies the glyph size in scatter plots. Depending on how much data you have reducing this to anything from 5 to 0.001 can have a notable effect. The <code>size</code> parameter in bokeh is similarly useful (but does not need to be quite so small).</p> <p>More generally the real solution, particular with large datasets, is to use datashader for plotting. Datashader is a plotting library that handles aggregation of large scale data in scatter plots in a way that can better show the underlying detail that can otherwise be lost. We highly recommend investing the time to learn datashader for UMAP plot particularly for larger datasets.</p>"},{"location":"faq/#i-ran-out-of-memory-help","title":"I ran out of memory. Help!","text":"<p>For some datasets the default options for approximate nearest neighbor search can result in excessive memory use. If your dataset is not especially large but you have found that UMAP runs out of memory when operating on it consider using the <code>low_memory=True</code> option, which will switch to a slower but less memory intensive approach to computing the approximate nearest neighbors. This may alleviate your issues.</p>"},{"location":"faq/#umap-is-eating-all-my-cores-help","title":"UMAP is eating all my cores. Help!","text":"<p>If run without a random seed UMAP will use numba's parallel implementation to do multithreaded work and use many cores. By default this will make use of as many cores as are available. If you are on a shared machine or otherwise don't wish to use all the cores at once you can restrict the number of threads that numba uses by making use of the environment variable <code>NUMBA_NUM_THREADS</code>; see the <code>numba documentation &lt;https://numba.pydata.org/numba-doc/dev/reference/envvars.html#threading-control&gt;</code>__ for more details.</p>"},{"location":"faq/#is-there-gpu-or-multicore-cpu-support","title":"Is there GPU or multicore-CPU support?","text":"<p>There is basic multicore support as of version 0.4. In the future it is possible that GPU support may be added.</p> <p>There is a UMAP implementation for GPU available in the NVIDIA RAPIDS cuML library, so if you need GPU support that is currently the best place to go.</p>"},{"location":"faq/#can-i-add-a-custom-loss-function","title":"Can I add a custom loss function?","text":"<p>To allow for fast performance the SGD phase of UMAP has been hand-coded for the specific needs of UMAP. This makes custom loss functions a little difficult to handle. Now that Numba (as of version 0.38) supports passing functions it is possible that future versions of UMAP may support such functionality. In the meantime you should definitely look into smallvis, a library for t-SNE, LargeVis, UMAP, and related algorithms. Smallvis only works for small datasets, but provides much greater flexibility and control.</p>"},{"location":"faq/#is-there-support-for-the-r-language","title":"Is there support for the R language?","text":"<p>Yes! A number of people have worked hard to make UMAP available to R users.</p> <p>If you want to use the reference implementation under the hood but want a nice R interface then we recommend umap, which wraps the python code with reticulate. Another reticulate interface is umapr, but it may not be under active development.</p> <p>If you want a pure R version then we recommend uwot at this time. umap also provides a pure R implementation in addition to its reticulate wrapper.</p> <p>Both umap and uwot are available on CRAN.</p>"},{"location":"faq/#is-there-a-cc-implementation","title":"Is there a C/C++ implementation?","text":"<p>Not that we are aware of. For now Numba has done a very admirable job of providing high performance and the developers of UMAP have not felt the need to move to lower level languages. At some point a multithreaded C++ implementation may be made available, but there are no time-frames for when that would happen.</p>"},{"location":"faq/#i-cant-get-umap-to-run-properly","title":"I can't get UMAP to run properly!","text":"<p>There are, inevitably, a number of issues and corner cases that can cause issues for UMAP. Some know issues that can cause problems are:</p> <ul> <li>UMAP doesn't currently support 32-bit Windows.   This is due to issues with Numba of that platform   and will not likely be resolved soon. Sorry :-(</li> <li>If you have pip installed an old package with a conflicting name   at any time, this can cause serious issues. You will want to purge/remove   everything umap related in your <code>site-packages</code>   directory and re-install <code>umap</code>.</li> <li>Having any files called <code>umap.py</code> in the current   directory you will have issues as that will be   loaded instead of the <code>umap</code> module.</li> </ul> <p>It is worth checking the issues page on github for potential solutions. If all else fails please add an issue on github.</p>"},{"location":"faq/#what-is-the-difference-between-pca-umap-vaes","title":"What is the difference between PCA / UMAP / VAEs?","text":"<p>This is an example of an embedding for a popular Fashion MNIST dataset.</p> <p> Comparison of PCA / UMAP / VAE embeddings</p> <p>Note that FMNIST is mostly a toy dataset (MNIST on steroids). On such a simplistic case UMAP shows distillation results (i.e. if we use its embedding in a downstream task like classification) comparable to VAEs, which are more computationally expensive.</p> <p>By definition:</p> <ul> <li>PCA is linear transformation, you can apply it   to mostly any kind of data in an unsupervised fashion.   Also it works really fast. For most real world tasks   its embeddings are mostly too simplistic / useless.</li> <li>VAE is a kind of encoder-decoder neural network,   trained with KLD loss and BCE (or MSE) loss   to enforce the resulting embedding to be continuous.   VAE is an extension of auto-encoder networks,   which by design should produce embeddings that are   not only relevant to actually encoding the data, but are   also smooth.</li> </ul> <p>From a more practical standpoint:</p> <ul> <li>PCA mostly works for any reasonable dataset on a modern machine.   (up to tens or hundreds of millions of rows);</li> <li>VAEs have been shown to work only for toy datasets   and to our knowledge there was no real life useful application to   a real world sized dataset (i.e. ImageNet);</li> <li>Applying UMAP to real world tasks usually provides a good starting   point for downstream tasks (data visualization, clustering, classification)   and works reasonably fast;</li> <li>Consider a typical pipeline: high-dimensional embedding (300+)   =&gt; PCA to reduce to 50 dimensions =&gt; UMAP to reduce to 10-20 dimensions   =&gt; HDBSCAN for clustering / some plain algorithm for classification;</li> </ul> <p>Which tool should I use?</p> <ul> <li>PCA for very large or high dimensional datasets (or maybe consider finding   a domain specific matrix factorization technique, e.g. topic modelling for texts);</li> <li>UMAP for smaller datasets;</li> <li>VAEs are mostly experimental;</li> </ul> <p>Where can I learn more?</p> <ul> <li>While PCA is ubiquitous, you may look   at this example comparing PCA / UMAP / VAEs;</li> </ul>"},{"location":"faq/#how-umap-can-go-wrong","title":"How UMAP can go wrong","text":"<p>One way UMAP can go wrong is the introduction of data points that are maximally far apart from all other points in your data set.  In other words, a points nearest neighbour is maximally far from it.  A common example of this could be a point which shares no features in common with any other point under a Jaccard distance or a point whose nearest neighbour is np.inf from it under a continuous distance function.  In both these cases UMAPs assumption of all points lying on a connected manifold can lead us astray.  From this points perspective all other points are equally valid nearest neighbours so its k-nearest neighbour query will return a random selection of neighbours all at this maximal distance.  Next we will normalize this distance by applying our UMAP kernel which says that a point should be maximally similar to it's nearest neighbour. Since all k-nearest neighbours are identically far apart they will all be considered maximally similar by our point in question.  When we try to embed our data into a low dimensional space our optimization will attempt to pull all these randomly selected points together.  Add a sufficiently large number of these points and our entire space gets pulled together destroying any of the structure we had hoped to identify.</p> <p>To circumvent this problem we've added a disconnection_distance parameter to UMAP which will cut any edge with a distance greater than the value passed in.  This parameter defaults to <code>None</code>. When set to <code>None</code> the disconnection_distance will be set to the maximal value for any of our supported bounded metrics and otherwise set to np.inf.  Removing these edges from the UMAP graph will disconnect our manifold and cause these points to start where they are initialized and get pushed away from all other points via the our optimization.</p> <p>If a user has a good understanding of their distance metric they can set this value by hand to prevent data in particularly sparse regions of their space from becoming connected to their manifold.</p> <p>If vertices in your graph are disconnected a warning message will be thrown.  At that point a user can make use of the umap.utils.disconnected_vertices() function to identify the disconnected points. This can be used either for filtering and retraining a new UMAP model or simple to bed used as a filter for visualization purposes as seen below.</p> Python<pre><code>umap_model = umap.UMAP().fit(data)\ndisconnected_points = umap.utils.disconnected_vertices(umap_model)\numap.plot.points(umap_model, subset_points=~disconnected_points)\n</code></pre>"},{"location":"faq/#successful-use-cases","title":"Successful use-cases","text":"<p>UMAP can be / has been successfully applied to the following domains:</p> <ul> <li>Single cell data visualization in biology;</li> <li>Mapping malware based on behavioural data;</li> <li>Pre-processing phrase vectors for clustering;</li> <li>Pre-processing image embeddings (Inception) for clustering;</li> </ul> <p>and many more -- if you have a successful use-case please submit a pull request adding it to this list!</p>"},{"location":"how_umap_works/","title":"How UMAP Works","text":""},{"location":"how_umap_works/#how-umap-works","title":"How UMAP Works","text":"<p>UMAP is an algorithm for dimension reduction based on manifold learning techniques and ideas from topological data analysis. It provides a very general framework for approaching manifold learning and dimension reduction, but can also provide specific concrete realizations. This article will discuss how the algorithm works in practice. There exist deeper mathematical underpinnings, but for the sake of readability by a general audience these will merely be referenced and linked. If you are looking for the mathematical description please see the <code>UMAP paper &lt;https://arxiv.org/abs/1802.03426&gt;</code>__.</p> <p>To begin making sense of UMAP we will need a little bit of mathematical background from algebraic topology and topological data analysis. This will provide a basic algorithm that works well in theory, but unfortunately not so well in practice. The next step will be to make use of some basic Riemannian geometry to bring real world data a little closer to the underlying assumptions of the topological data analysis algorithm. Unfortunately this will introduce new complications, which will be resolved through a combination of deep math (details of which will be elided) and fuzzy logic. We can then put the pieces back together again, and combine them with a new approach to finding a low dimensional representation more fitting to the new data structures at hand. Putting this all together we arrive at the basic UMAP algorithm.</p>"},{"location":"how_umap_works/#topological-data-analysis-and-simplicial-complexes","title":"Topological Data Analysis and Simplicial Complexes","text":"<p>Simplicial complexes are a means to construct topological spaces out of simple combinatorial components. This allows one to reduce the complexities of dealing with the continuous geometry of topological spaces to the task of relatively simple combinatorics and counting. This method of taming geometry and topology will be fundamental to our approach to topological data analysis in general, and dimension reduction in particular.</p> <p>The first step is to provide some simple combinatorial building blocks called simplices. Geometrically a simplex is a very simple way to build a :math:<code>k</code>-dimensional object. A :math:<code>k</code> dimensional simplex is called a :math:<code>k</code>-simplex, and it is formed by taking the convex hull of :math:<code>k+1</code> independent points. Thus a 0-simplex is a point, a 1-simplex is a line segment (between two zero simplices), a 2-simplex is a triangle (with three 1-simplices as \"faces\"), and a 3-simplex is a tetrahedron (with four 2-simplices as \"faces\"). Such a simple construction allows for easy generalization to arbitrary dimensions.</p> <p> Low dimensional simplices</p> <p>This has a very simple combinatorial underlying structure, and ultimately one can regard a :math:<code>k</code>-simplex as an arbitrary set of :math:<code>k+1</code> objects with faces (and faces of faces etc.) given by appropriately sized subsets -- one can always provide a \"geometric realization\" of this abstract set description by constructing the corresponding geometric simplex.</p> <p>Simplices can provide building blocks, but to construct interesting topological spaces we need to be able to glue together such building blocks. This can be done by constructing a <code>*simplicial complex* &lt;https://en.wikipedia.org/wiki/Simplicial_complex&gt;</code>__. Ostensibly a simplicial complex is a set of simplices glued together along faces. More explicitly a simplicial complex :math:<code>\\mathcal{K}</code> is a set of simplices such that any face of any simplex in :math:<code>\\mathcal{K}</code> is also in :math:<code>\\mathcal{K}</code> (ensuring all faces exist), and the intersection of any two simplices in :math:<code>\\mathcal{K}</code> is a face of both simplices. A large class of topological spaces can be constructed in this way -- just gluing together simplices of various dimensions along their faces. A little further abstraction will get to simplicial sets which are purely combinatorial, have a nice category theoretic presentation, and can generate a much broader class of topological spaces, but that will take us too far afield for this article. The intuition of simplicial complexes will be enough to illustrate the relevant ideas and motivation.</p> <p>How does one apply these theoretical tools from topology to finite sets of data points? To start we'll look at how one might construct a simplicial complex from a topological space. The tool we will consider is the construction of a <code>\u010cech complex &lt;https://en.wikipedia.org/wiki/%C4%8Cech_cohomology&gt;</code>__ given an open cover of a topological space. That's a lot of verbiage if you haven't done much topology, but we can break it down fairly easily for our use case. An open cover is essentially just a family of sets whose union is the whole space, and a \u010cech complex is a combinatorial way to convert that into a simplicial complex. It works fairly simply: let each set in the cover be a 0-simplex; create a 1-simplex between two such sets if they have a non-empty intersection; create a 2-simplex between three such sets if the triple intersection of all three is non-empty; and so on. Now, that doesn't sound very advanced -- just looking at intersections of sets. The key is that the background topological theory actually provides guarantees about how well this simple process can produce something that represents the topological space itself in a meaningful way (the <code>Nerve theorem &lt;https://en.wikipedia.org/wiki/Nerve_of_a_covering&gt;</code>__ is the relevant result for those interested). Obviously the quality of the cover is important, and finer covers provide more accuracy, but the reality is that despite its simplicity the process captures much of the topology.</p> <p>Next we need to understand how to apply that process to a finite set of data samples. If we assume that the data samples are drawn from some underlying topological space then to learn about the topology of that space we need to generate an open cover of it. If our data actually lie in a metric space (i.e. we can measure distance between points) then one way to approximate an open cover is to simply create balls of some fixed radius about each data point. Since we only have finite samples, and not the topological space itself, we cannot be sure it is truly an open cover, but it might be as good an approximation as we could reasonably expect. This approach also has the advantage that the \u010cech complex associated to the cover will have a 0-simplex for each data point.</p> <p>To demonstrate the process let's consider a test dataset like this</p> <p> Test data set of a noisy sine wave</p> <p>If we fix a radius we can then picture the open sets of our cover as circles (since we are in a nice visualizable two dimensional case). The result is something like this</p> <p> A basic open cover of the test data</p> <p>We can then depict the the simplicial complex of 0-, 1-, and 2-simplices as points, lines, and triangles</p> <p> A simplicial complex built from the test data</p> <p>It is harder to easily depict the higher dimensional simplices, but you can imagine how they would fit in. There are two things to note here: first, the simplicial complex does a reasonable job of starting to capture the fundamental topology of the dataset; second, most of the work is really done by the 0- and 1-simplices, which are easier to deal with computationally (it is just a graph, in the nodes and edges sense). The second observation motivates the <code>Vietoris-Rips complex &lt;https://en.wikipedia.org/wiki/Vietoris%E2%80%93Rips_complex&gt;</code>__, which is similar to the \u010cech complex but is entirely determined by the 0- and 1-simplices. Vietoris-Rips complexes are much easier to work with computationally, especially for large datasets, and are one of the major tools of topological data analysis.</p> <p>If we take this approach to get a topological representation then we can build a dimension reduction algorithm by finding a low dimensional representation of the data that has a similar topological representation. If we only care about the 0- and 1-simplices then the topological representation is just a graph, and finding a low dimensional representation can be described as a <code>graph layout problem &lt;https://en.wikipedia.org/wiki/Graph_drawing&gt;</code>. If one wants to use, for example, spectral methods for graph layout then we arrive at algorithms like <code>Laplacian eigenmaps &lt;https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Laplacian_eigenmaps&gt;</code> and Diffusion maps. Force directed layouts are also an option, and provide algorithms closer to MDS or <code>Sammon mapping &lt;https://en.wikipedia.org/wiki/Sammon_mapping&gt;</code>__ in flavour.</p> <p>I would not blame those who have read this far to wonder why we took such an abstract roundabout road to simply building a neighborhood-graph on the data and then laying out that graph. There are a couple of reasons. The first reason is that the topological approach, while abstract, provides sound theoretical justification for what we are doing. While building a neighborhood-graph and laying it out in lower dimensional space makes heuristic sense and is computationally tractable, it doesn't provide the same underlying motivation of capturing the underlying topological structure of the data faithfully -- for that we need to appeal to the powerful topological machinery I've hinted lies in the background. The second reason is that it is this more abstract topological approach that will allow us to generalize the approach and get around some of the difficulties of the sorts of algorithms described above. While ultimately we will end up with a process that is fairly simple computationally, understanding why various manipulations matter is important to truly understanding the algorithm (as opposed to merely computing with it).</p>"},{"location":"how_umap_works/#adapting-to-real-world-data","title":"Adapting to Real World Data","text":"<p>The approach described above provides a nice theory for why a neighborhood graph based approach should capture manifold structure when doing dimension reduction. The problem tends to come when one tries to put the theory into practice. The first obvious difficulty (and we can see it even our example above) is that choosing the right radius for the balls that make up the open cover is hard. If you choose something too small the resulting simplicial complex splits into many connected components. If you choose something too large the simplicial complex turns into just a few very high dimensional simplices (and their faces etc.) and fails to capture the manifold structure anymore. How should one solve this?</p> <p>The dilemma is in part due to the theorem (called the <code>Nerve theorem &lt;https://en.wikipedia.org/wiki/Nerve_of_a_covering&gt;</code>__) that provides our justification that this process captures the topology. Specifically, the theorem says that the simplicial complex will be (homotopically) equivalent to the union of the cover. In our case, working with finite data, the cover, for certain radii, doesn't cover the whole of the manifold that we imagine underlies the data -- it is that lack of coverage that results in the disconnected components. Similarly, where the points are too bunched up, our cover does cover \"too much\" and we end up with higher dimensional simplices than we might ideally like. If the data were uniformly distributed across the manifold then selecting a suitable radius would be easy -- the average distance between points would work well. Moreover with a uniform distribution we would be guaranteed that our cover would actually cover the whole manifold with no \"gaps\" and no unnecessarily disconnected components. Similarly, we would not suffer from those unfortunate bunching effects resulting in unnecessarily high dimensional simplices.</p> <p>If we consider data that is uniformly distributed along the same manifold it is not hard to pick a good radius (a little above half the average distance between points) and the resulting open cover looks pretty good:</p> <p> Open balls over uniformly_distributed_data</p> <p>Because the data is evenly spread we actually cover the underlying manifold and don't end up with clumping. In other words, all this theory works well assuming that the data is uniformly distributed over the manifold.</p> <p>Unsurprisingly this uniform distribution assumption crops up elsewhere in manifold learning. The proofs that Laplacian eigenmaps work well require the assumption that the data is uniformly distributed on the manifold. Clearly if we had a uniform distribution of points on the manifold this would all work a lot better -- but we don't! Real world data simply isn't that nicely behaved. How can we resolve this? By turning the problem on its head: assume that the data is uniformly distributed on the manifold, and ask what that tells us about the manifold itself. If the data looks like it isn't uniformly distributed that must simply be because the notion of distance is varying across the manifold -- space itself is warping: stretching or shrinking according to where the data appear sparser or denser.</p> <p>By assuming that the data is uniformly distributed we can actually compute (an approximation of) a local notion of distance for each point by making use of a little standard <code>Riemannian geometry &lt;https://en.wikipedia.org/wiki/Riemannian_geometry&gt;</code>__. In practical terms, once you push the math through, this turns out to mean that a unit ball about a point stretches to the k-th nearest neighbor of the point, where k is the sample size we are using to approximate the local sense of distance. Each point is given its own unique distance function, and we can simply select balls of radius one with respect to that local distance function!</p> <p> Open balls of radius one with a locally varying metric</p> <p>This theoretically derived result matches well with many traditional graph based algorithms: a standard approach for such algorithms is to use a k-neighbor graph instead of using balls of some fixed radius to define connectivity. What this means is that each point in the dataset is given an edge to each of its k nearest neighbors -- the effective result of our locally varying metric with balls of radius one. Now, however, we can explain why this works in terms of simplicial complexes and the Nerve theorem.</p> <p>Of course we have traded choosing the radius of the balls for choosing a value for k. However it is often easier to pick a resolution scale in terms of number of neighbors than it is to correctly choose a distance. This is because choosing a distance is very dataset dependent: one needs to look at the distribution of distances in the dataset to even begin to select a good value. In contrast, while a k value is still dataset dependent to some degree, there are reasonable default choices, such as the 10 nearest neighbors, that should work acceptably for most datasets.</p> <p>At the same time the topological interpretation of all of this gives us a more meaningful interpretation of k. The choice of k determines how locally we wish to estimate the Riemannian metric. A small choice of k means we want a very local interpretation which will more accurately capture fine detail structure and variation of the Riemannian metric. Choosing a large k means our estimates will be based on larger regions, and thus, while missing some of the fine detail structure, they will be more broadly accurate across the manifold as a whole, having more data to make the estimate with.</p> <p>We also get a further benefit from this Riemannian metric based approach: we actually have a local metric space associated to each point, and can meaningfully measure distance, and thus we could weight the edges of the graph we might generate by how far apart (in terms of the local metric) the points on the edges are. In slightly more mathematical terms we can think of this as working in a fuzzy topology where being in an open set in a cover is no longer a binary yes or no, but instead a fuzzy value between zero and one. Obviously the certainty that points are in a ball of a given radius will decay as we move away from the center of the ball. We could visualize such a fuzzy cover as looking something like this</p> <p> Fuzzy open balls of radius one with a locally varying metric</p> <p>None of that is very concrete or formal -- it is merely an intuitive picture of what we would like to have happen. It turns out that we can actually formalize all of this by stealing the <code>singular set &lt;https://en.wikipedia.org/wiki/Simplicial_set#Singular_set_for_a_space&gt;</code>__ and <code>geometric realization &lt;https://en.wikipedia.org/wiki/Simplicial_set#Geometric_realization&gt;</code>__ functors from algebraic topology and then adapting them to apply to metric spaces and fuzzy simplicial sets. The mathematics involved in this is outside the scope of this exposition, but for those interested you can look at the <code>original work on this by David Spivak &lt;http://math.mit.edu/~dspivak/files/metric_realization.pdf&gt;</code>__ and our paper. It will have to suffice to say that there is some mathematical machinery that lets us realize this intuition in a well defined way.</p> <p>This resolves a number of issues, but a new problem presents itself when we apply this sort of process to real data, especially in higher dimensions: a lot of points become essentially totally isolated. One would imagine that this shouldn't happen if the manifold the data was sampled from isn't pathological. So what property are we expecting that manifold to have that we are somehow missing with the current approach? What we need to add is the idea of local connectivity.</p> <p>Note that this is not a requirement that the manifold as a whole be connected -- it can be made up of many connected components. Instead it is a requirement that at any point on the manifold there is some sufficiently small neighborhood of the point that is connected (this \"in a sufficiently small neighborhood\" is what the \"local\" part means). For the practical problem we are working with, where we only have a finite approximation of the manifold, this means that no point should be completely isolated -- it should connect to at least one other point. In terms of fuzzy open sets what this amounts to is that we should have complete confidence that the open set extends as far as the closest neighbor of each point. We can implement this by simply having the fuzzy confidence decay in terms of distance beyond the first nearest neighbor. We can visualize the result in terms of our example dataset again.</p> <p> Local connectivity and fuzzy open sets</p> <p>Again this can be formalized in terms of the aforementioned mathematical machinery from algebraic topology. From a practical standpoint this plays an important role for high dimensional data -- in high dimensions distances tend to be larger, but also more similar to one another (see <code>the curse of dimensionality &lt;https://en.wikipedia.org/wiki/Curse_of_dimensionality&gt;</code>__). This means that the distance to the first nearest neighbor can be quite large, but the distance to the tenth nearest neighbor can often be only slightly larger (in relative terms). The local connectivity constraint ensures that we focus on the difference in distances among nearest neighbors rather than the absolute distance (which shows little differentiation among neighbors).</p> <p>Just when we think we are almost there, having worked around some of the issues of real world data, we run aground on a new obstruction: our local metrics are not compatible! Each point has its own local metric associated to it, and from point a's perspective the distance from point a to point b might be 1.5, but from the perspective of point b the distance from point b to point a might only be 0.6. Which point is right? How do we decide? Going back to our graph based intuition we can think of this as having directed edges with varying weights something like this.</p> <p> Edges with incompatible weights</p> <p>Between any two points we might have up to two edges and the weights on those edges disagree with one another. There are a number of options for what to do given two disagreeing weights -- we could take the maximum, the minimum, the arithmetic mean, the geometric mean, or something else entirely. What we would really like is some principled way to make the decision. It is at this point that the mathematical machinery we built comes into play. Mathematically we actually have a family of fuzzy simplicial sets, and the obvious choice is to take their union -- a well defined operation. There are a a few ways to define fuzzy unions, depending on the nature of the logic involved, but here we have relatively clear probabilistic semantics that make the choice straightforward. In graph terms what we get is the following: if we want to merge together two disagreeing edges with weight a and b then we should have a single edge with combined weight :math:<code>a + b - a \\cdot b</code>. The way to think of this is that the weights are effectively the probabilities that an edge (1-simplex) exists. The combined weight is then the probability that at least one of the edges exists.</p> <p>If we apply this process to union together all the fuzzy simplicial sets we end up with a single fuzzy simplicial complex, which we can again think of as a weighted graph. In computational terms we are simply applying the edge weight combination formula across the whole graph (with non-edges having a weight of 0). In the end we have something that looks like this.</p> <p> Graph with combined edge weights</p> <p>So in some sense in the end we have simply constructed a weighted graph (although we could make use of higher dimensional simplices if we wished, just at significant extra computational cost). What the mathematical theory lurking in the background did for us is determine why we should construct this graph. It also helped make the decisions about exactly how to compute things, and gives a concrete interpretation of what this graph means. So while in the end we just constructed a graph, the math answered the important questions to get us here, and can help us determine what to do next.</p> <p>So given that we now have a fuzzy topological representation of the data (which the math says will capture the topology of the manifold underlying the data), how do we go about converting that into a low dimensional representation?</p>"},{"location":"how_umap_works/#finding-a-low-dimensional-representation","title":"Finding a Low Dimensional Representation","text":"<p>Ideally we want the low dimensional representation to have as similar a fuzzy topological structure as possible. The first question is how do we determine the fuzzy topological structure of a low dimensional representation, and the second question is how do we find a good one.</p> <p>The first question is largely already answered -- we should presumably follow the same procedure we just used to find the fuzzy topological structure of our data. There is a quirk, however: this time around the data won't be lying on some manifold, we'll have a low dimensional representation that is lying on a very particular manifold. That manifold is, of course, just the low dimensional euclidean space we are trying to embed into. This means that all the effort we went to previously to make vary the notion of distance across the manifold is going to be misplaced when working with the low dimensional representation. We explicitly want the distance on the manifold to be standard euclidean distance with respect to the global coordinate system, not a varying metric. That saves some trouble. The other quirk is that we made use of the distance to the nearest neighbor, again something we computed given the data. This is also a property we would like to be globally true across the manifold as we optimize toward a good low dimensional representation, so we will have to accept it as a hyper-parameter <code>min_dist</code> to the algorithm.</p> <p>The second question, 'how do we find a good low dimensional representation', hinges on our ability to measure how \"close\" a match we have found in terms of fuzzy topological structures. Given such a measure we can turn this into an optimization problem of finding the low dimensional representation with the closest fuzzy topological structure. Obviously if our measure of closeness turns out to have various properties the nature of the optimization techniques we can apply will differ.</p> <p>Going back to when we were merging together the conflicting weights associated to simplices, we interpreted the weights as the probability of the simplex existing. Thus, since both topological structures we are comparing share the same 0-simplices, we can imagine that we are comparing the two vectors of probabilities indexed by the 1-simplices. Given that these are Bernoulli variables (ultimately the simplex either exists or it doesn't, and the probability is the parameter of a Bernoulli distribution), the right choice here is the cross entropy.</p> <p>Explicitly, if the set of all possible 1-simplices is :math:<code>E</code>, and we have weight functions such that :math:<code>w_h(e)</code> is the weight of the 1-simplex :math:<code>e</code> in the high dimensional case and :math:<code>w_l(e)</code> is the weight of :math:<code>e</code> in the low dimensional case, then the cross entropy will be</p> <p>\\sum_{e\\in E} w_h(e) \\log\\left(\\frac{w_h(e)}{w_l(e)}\\right) + (1 - w_h(e)) \\log\\left(\\frac{1 - w_h(e)}{1 - w_l(e)}\\right)</p> <p>This might look complicated, but if we go back to thinking in terms of a graph we can view minimizing the cross entropy as a kind of force directed graph layout algorithm.</p> <p>The first term, :math:<code>w_h(e) \\log\\left(\\frac{w_h(e)}{w_l(e)}\\right)</code>, provides an attractive force between the points :math:<code>e</code> spans whenever there is a large weight associated to the high dimensional case. This is because this term will be minimized when :math:<code>w_l(e)</code> is as large as possible, which will occur when the distance between the points is as small as possible.</p> <p>In contrast the second term, :math:<code>(1 - w_h(e)) \\log\\left(\\frac{1 - w_h(e)}{1 - w_l(e)}\\right)</code>, provides a repulsive force between the ends of :math:<code>e</code> whenever :math:<code>w_h(e)</code> is small. This is because the term will be minimized by making :math:<code>w_l(e)</code> as small as possible.</p> <p>On balance this process of pull and push, mediated by the weights on edges of the topological representation of the high dimensional data, will let the low dimensional representation settle into a state that relatively accurately represents the overall topology of the source data.</p>"},{"location":"how_umap_works/#the-umap-algorithm","title":"The UMAP Algorithm","text":"<p>Putting all these pieces together we can construct the UMAP algorithm. The first phase consists of constructing a fuzzy topological representation, essentially as described above. The second phase is simply optimizing the low dimensional representation to have as close a fuzzy topological representation as possible as measured by cross entropy.</p> <p>When constructing the initial fuzzy topological representation we can take a few shortcuts. In practice, since fuzzy set membership strengths decay away to be vanishingly small, we only need to compute them for the nearest neighbors of each point. Ultimately that means we need a way to quickly compute (approximate) nearest neighbors efficiently, even in high dimensional spaces. We can do this by taking advantage of the <code>Nearest-Neighbor-Descent algorithm of Dong et al &lt;http://www.cs.princeton.edu/cass/papers/www11.pdf&gt;</code>__. The remaining computations are now only dealing with local neighbors of each point and are thus very efficient.</p> <p>In optimizing the low dimensional embedding we can again take some shortcuts. We can use stochastic gradient descent for the optimization process. To make the gradient descent problem easier it is beneficial if the final objective function is differentiable. We can arrange for that by using a smooth approximation of the actual membership strength function for the low dimensional representation, selecting from a suitably versatile family. In practice UMAP uses the family of curves of the form :math:<code>\\frac{1}{1 + a x^{2b}}</code>. Equally we don't want to have to deal with all possible edges, so we can use the negative sampling trick (as used by word2vec and LargeVis), to simply sample negative examples as needed. Finally since the Laplacian of the topological representation is an approximation of the Laplace-Beltrami operator of the manifold we can use spectral embedding techniques to initialize the low dimensional representation into a good state.</p> <p>Putting all these pieces together we arrive at an algorithm that is fast and scalable, yet still built out of sound mathematical theory. Hopefully this introduction has helped provide some intuition for that underlying theory, and for how the UMAP algorithm works in practice.</p>"},{"location":"interactive_viz/","title":"Interactive Visualizations","text":"<p>UMAP has found use in a number of interesting interactive visualization projects, analyzing everything from images from photo archives, to word embedding, animal point clouds, and even sound. Sometimes it has also been used in interesting interactive tools that simply help a user to get an intuition for what the algorithm is doing (by applying it to intuitive 3D data). Below are some amazing projects that make use of UMAP.</p>"},{"location":"interactive_viz/#umap-zoo","title":"UMAP Zoo","text":"<p>An exploration of how UMAP behaves when dimension reducing point clouds of animals. It is interactive, letting you switch between 2D and 3D representations and has a wide selection of different animals. Attempting to guess the animal from the 2D UMAP representation is a fun game. In practice this tool can go a long way to helping to build at least some intuitions for what UMAP tends to do with data.</p> <p></p> <p>UMAP Zoo</p> <p>Thanks to Douglas Duhaime.</p>"},{"location":"interactive_viz/#tensorflow-embedding-projector","title":"Tensorflow Embedding Projector","text":"<p>If you just want to explore UMAP embeddings of datasets then the Embedding Projector from Tensorflow is a great way to do that. As well as having a good interactive 3D view it also has facilities for inspecting and searching labels and tags on the data. By default it loads up word2vec vectors, but you can upload any data you wish. You can then select the UMAP option among the tabs for embeddings choices (alongside PCA and t-SNE).</p> <p></p> <p>Embedding Projector</p> <p>Thanks to Andy Coenen and the Embedding Projector team.</p>"},{"location":"interactive_viz/#pixplot","title":"PixPlot","text":"<p>PixPlot provides an overview of large photo-collections. In the demonstration app from Yale's Digital Humanities lab it provides a window on the Meserve-Kunhardt Collection of historical photographs. The approach uses convolutional neural nets to reduce the images to 2048 dimensions, and then uses UMAP to present them in a 2-dimensional map which the user can interactive pan and zoom around in. This process results in similar photos ending up in similar regions of the map allowing for easy perusal of large photo collections. The PixPlot project is also available on github in case you wish to train it on your own photo collection.</p> <p></p> <p>PixPlot</p> <p>Thanks to Douglas Duhaime and the Digital Humanities lab at Yale.</p>"},{"location":"interactive_viz/#umap-explorer","title":"UMAP Explorer","text":"<p>A great demonstration of building a web based app for interactively exploring a UMAP embedding. In this case it provides an exploration of UMAP run on the MNIST digits dataset. Each point in the embedding is rendered as the digit image, and coloured according to the digit class. Mousing over the images will make them larger and provide a view of the digit in the upper left. You can also pan and zoom around the emebdding to get a better understanding of how UMAP has mapped the different styles of handwritten digits down to 2 dimensions.</p> <p></p> <p>UMAP Explorer</p> <p>Thanks to Grant Custer.</p>"},{"location":"interactive_viz/#audio-explorer","title":"Audio Explorer","text":"<p>The Audio Explorer uses UMAP to embed sound samples into a 2 dimensional space for easy exploration. The goal here is to take a large library of sounds samples and put similar sounds in similar regions of the map, allowing a user to quickly mouse over and listen to various variations of a given sample to quickly find exactly the right sound sample to use. Audio explorer uses MFCCs and/or WaveNet to provide an initial useful vector representation of the sound samples, before applying UMAP to generate the 2D embedding.</p> <p></p> <p>Audio Explorer</p> <p>Thanks to Leon Fedden.</p>"},{"location":"interactive_viz/#orion-search","title":"Orion Search","text":"<p>Orion is an open source research measurement and knowledge discovery tool that enables you to monitor progress in science, visually explore the scientific landscape and search for relevant publications. Orion encodes bioRxiv paper abstracts to dense vectors with Sentence Transformers and projects them to an interactive 3D visualisation with UMAP. You can filter the UMAP embeddings by topic and country. You can also select a subset of the UMAP embeddings and retrieve those papers and their metadata.</p> <p></p> <p>Orion Search</p> <p>Thanks to Kostas Stathoulopoulos, Zac Ioannidis and Lilia Villafuerte.</p>"},{"location":"interactive_viz/#exploring-fashion-mnist","title":"Exploring Fashion MNIST","text":"<p>A web based interactive exploration of a 3D UMAP embedding ran on the Fashion MNIST dataset. Users can freely navigate the 3D space, jumping to a specific image by clicking an image or entering an image id. Like Grant Custer's UMAP Explorer, each point is rendered as the actual image and colored according to the label. It is also similar to the Tensorflow Embedding Projector, but designed more specifically for Fashion MNIST, thus more efficient and capable of showing all the 70k images.</p> <p></p> <p>Exploring Fashion MNIST</p> <p>Thanks to stwind.</p>"},{"location":"interactive_viz/#esm-metagenomic-atlas","title":"ESM Metagenomic Atlas","text":"<p>The ESM Metagenomic Atlas contains over 600 million predicted protein structures, revealing the metagenomic world in a way we have never seen before. The Explore page visualizes a sample of 1 million of these. (That\u2019s about how much a browser can handle.) We represent each protein in this dataset as a single point, and reveal the actual protein structure when zooming in or when hovering over it. The color of each point corresponds to the similarity to the closest match we could find in UniRef90, the reference database of known protein sequences. The position in the map is a two-dimensional projection, which groups sequences by similarity, as determined by our language model\u2019s internal representation. The map reveals structure at different scales: local neighbors in the same cluster tend to have similar structures, while nearby clusters preserve certain patterns like secondary structure elements.</p> <p></p> <p>Thanks to the authors of \"Evolutionary-scale prediction of atomic level protein structure with a language model\".</p> <p>ESM Metagenomic Atlas</p>"},{"location":"interactive_viz/#interactive-umap-with-nomic-atlas","title":"Interactive UMAP with Nomic Atlas","text":"<p>Nomic Atlas is a platform for interactively visualizing and exploring massive datasets. It automates the creation of embeddings and 2D coordinate projections using UMAP.</p> <p></p> <p>Atlas provides:</p> <ul> <li>In-browser analysis of your UMAP data with the Atlas Analyst</li> <li>Vector search over your UMAP data using the Nomic API</li> <li>Interactive features like zooming, recoloring, searching, and filtering in the Nomic Atlas data map</li> <li>Scalability for millions of data points</li> <li>Rich information display on hover</li> <li>Shareable UMAPs via URL links to your embeddings and data maps in Atlas</li> </ul>"},{"location":"interactive_viz/#nomic-atlas-examples","title":"Nomic Atlas Examples","text":"<ul> <li>nomic_atlas_umap_of_text_embeddings</li> <li>nomic_atlas_visualizing_mnist_training_dynamics</li> </ul>"},{"location":"inverse_transform/","title":"Inverse transforms","text":"<p>UMAP has some support for inverse transforms -- generating a high dimensional data sample given a location in the low dimensional embedding space. To start let's load all the relevant libraries.</p> Python<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nimport seaborn as sns\nimport sklearn.datasets\nimport umap\nimport umap.plot\n</code></pre> <p>We will need some data to test with. To start we'll use the MNIST digits dataset. This is a dataset of 70000 handwritten digits encoded as grayscale 28x28 pixel images. Our goal is to use UMAP to reduce the dimension of this dataset to something small, and then see if we can generate new digits by sampling points from the embedding space. To load the MNIST dataset we'll make use of sklearn's <code>fetch_openml</code> function.</p> Python<pre><code>data, labels = sklearn.datasets.fetch_openml('mnist_784', version=1, return_X_y=True)\n</code></pre> <p>Now we need to generate a reduced dimension representation of this data. This is straightforward with UMAP, but in this case rather than using <code>fit_transform</code> we'll use the fit method so that we can retain the trained model for later generating new digits based on samples from the embedding space.</p> Python<pre><code>mapper = umap.UMAP(random_state=42).fit(data)\n</code></pre> <p>To ensure that things worked correctly we can plot the data (since we reduced it to two dimensions). We'll use the <code>umap.plot</code> functionality to do this.</p> Python<pre><code>umap.plot.points(mapper, labels=labels)\n</code></pre> <p></p> <p>This looks much like we would expect. The different digit classes have been decently separated. Now we need to create a set of samples in the embedding space to apply the <code>inverse_transform</code> operation to. To do this we'll generate a grid of samples linearly interpolating between four corner points. To make our selection interesting we'll carefully choose the corners to span over the dataset, and sample different digits so that we can better see the transitions.</p> Python<pre><code>corners = np.array([\n    [-5, -10],  # 1\n    [-7, 6],  # 7\n    [2, -8],  # 2\n    [12, 4],  # 0\n])\n\ntest_pts = np.array([\n    (corners[0]*(1-x) + corners[1]*x)*(1-y) +\n    (corners[2]*(1-x) + corners[3]*x)*y\n    for y in np.linspace(0, 1, 10)\n    for x in np.linspace(0, 1, 10)\n])\n</code></pre> <p>Now we can apply the <code>inverse_transform</code> method to this set of test points. Each test point is a two dimensional point lying somewhere in the embedding space. The <code>inverse_transform</code> method will convert this into an approximation of the high dimensional representation that would have been embedded into such a location. Following the sklearn API this is as simple to use as calling the <code>inverse_transform</code> method of the trained model and passing it the set of test points that we want to convert into high dimensional representations. Be warned that this can be quite expensive computationally.</p> Python<pre><code>inv_transformed_points = mapper.inverse_transform(test_pts)\n</code></pre> <p>Now the goal is to visualize how well we have done. Effectively what we would like to do is show the test points in the embedding space, and then show a grid of the corresponding images generated by the inverse transform. To get all of this in a single matplotlib figure takes a little setting up, but is quite manageable -- mostly it is just a matter of managing <code>GridSpec</code> formatting. Once we have that setup we just need a scatterplot of the embedding, a scatterplot of the test points, and finally a grid of the images we generated (converting the inverse transformed vectors into images is just a matter of reshaping them back to 28 by 28 pixel grids and using <code>imshow</code>).</p> Python<pre><code># Set up the grid\nfig = plt.figure(figsize=(12,6))\ngs = GridSpec(10, 20, fig)\nscatter_ax = fig.add_subplot(gs[:, :10])\ndigit_axes = np.zeros((10, 10), dtype=object)\nfor i in range(10):\n    for j in range(10):\n        digit_axes[i, j] = fig.add_subplot(gs[i, 10 + j])\n\n# Use umap.plot to plot to the major axis\n# umap.plot.points(mapper, labels=labels, ax=scatter_ax)\nscatter_ax.scatter(mapper.embedding_[:, 0], mapper.embedding_[:, 1],\n                   c=labels.astype(np.int32), cmap='Spectral', s=0.1)\nscatter_ax.set(xticks=[], yticks=[])\n\n# Plot the locations of the text points\nscatter_ax.scatter(test_pts[:, 0], test_pts[:, 1], marker='x', c='k', s=15)\n\n# Plot each of the generated digit images\nfor i in range(10):\n    for j in range(10):\n        digit_axes[i, j].imshow(inv_transformed_points[i*10 + j].reshape(28, 28))\n        digit_axes[i, j].set(xticks=[], yticks=[])\n</code></pre> <p></p> <p>The end result looks pretty good -- we did indeed generate plausible looking digit images, and many of the transitions (from 1 to 7 across the top row for example) seem pretty natural and make sense. This can help you to understand the structure of the cluster of 1s (it transitions on the angle, sloping toward what will eventually be 7s), and why 7s and 9s are close together in the embedding. Of course there are also some stranger transitions, especially where the test points fell into large gaps between clusters in the embedding -- in some sense it is hard to interpret what should go in some of those gaps as they don't really represent anything resembling a smooth transition).</p> <p>A further note: None of the test points chosen fall outside the convex hull of the embedding. This is deliberate -- the inverse transform function operates poorly outside the bounds of that convex hull. Be warned that if you select points to inverse transform that are outside the bounds about the embedding you will likely get strange results (often simply snapping to a particular source high dimensional vector).</p> <p>Let's continue the demonstration by looking at the Fashion MNIST dataset. As before we can load this through sklearn.</p> Python<pre><code>data, labels = sklearn.datasets.fetch_openml('Fashion-MNIST', version=1, return_X_y=True)\n</code></pre> <p>Again we can fit this data with UMAP and get a mapper object.</p> Python<pre><code>mapper = umap.UMAP(random_state=42).fit(data)\n</code></pre> <p>Let's plot the embedding to see what we got as a result:</p> Python<pre><code>umap.plot.points(mapper, labels=labels)\n</code></pre> <p></p> <p>Again we'll generate a set of test points by making a grid interpolating between four corners. As before we'll select the corners so that we can stay within the convex hull of the embedding points and ensure nothing too strange happens with the inverse transforms.</p> Python<pre><code>corners = np.array([\n    [-2, -6],  # bags\n    [-9, 3],  # boots?\n    [7, -5],  # shirts/tops/dresses\n    [4, 10],  # pants\n])\n\ntest_pts = np.array([\n    (corners[0]*(1-x) + corners[1]*x)*(1-y) +\n    (corners[2]*(1-x) + corners[3]*x)*y\n    for y in np.linspace(0, 1, 10)\n    for x in np.linspace(0, 1, 10)\n])\n</code></pre> <p>Now we simply apply the inverse transform just as before. Again, be warned, this is quite expensive computationally and may take some time to complete.</p> Python<pre><code>inv_transformed_points = mapper.inverse_transform(test_pts)\n</code></pre> <p>And now we can use similar code as above to set up our plot of the embedding with test points overlaid, and the generated images.</p> Python<pre><code># Set up the grid\nfig = plt.figure(figsize=(12,6))\ngs = GridSpec(10, 20, fig)\nscatter_ax = fig.add_subplot(gs[:, :10])\ndigit_axes = np.zeros((10, 10), dtype=object)\nfor i in range(10):\n    for j in range(10):\n        digit_axes[i, j] = fig.add_subplot(gs[i, 10 + j])\n\n# Use umap.plot to plot to the major axis\n# umap.plot.points(mapper, labels=labels, ax=scatter_ax)\nscatter_ax.scatter(mapper.embedding_[:, 0], mapper.embedding_[:, 1],\n                   c=labels.astype(np.int32), cmap='Spectral', s=0.1)\nscatter_ax.set(xticks=[], yticks=[])\n\n# Plot the locations of the text points\nscatter_ax.scatter(test_pts[:, 0], test_pts[:, 1], marker='x', c='k', s=15)\n\n# Plot each of the generated digit images\nfor i in range(10):\n    for j in range(10):\n        digit_axes[i, j].imshow(inv_transformed_points[i*10 + j].reshape(28, 28))\n        digit_axes[i, j].set(xticks=[], yticks=[])\n</code></pre> <p></p> <p>This time we see some of the interpolations between items looking rather strange -- particularly the points that lie somewhere between shoes and pants -- ultimately it is doing the best it can with a difficult problem. At the same time many of the other transitions seem to work pretty well, so it is, indeed, providing useful information about how the embedding is structured.</p>"},{"location":"mutual_nn_umap/","title":"Improving the Separation Between Similar Classes Using a Mutual k-NN Graph","text":"<p>This post briefly explains how the connectivity of the original graphical representation can adversely affect the resulting UMAP embeddings.</p> <p>In default UMAP, a weighted k nearest neighbor (k-NN) graph, which connects each datapoint to its \ud835\udc58 nearest neighbors based on some distance metric, is constructed and used to generate the initial topological representation of a dataset.</p> <p>However, previous research has shown that using a weighted k-NN graph may not provide an accurate representation of the underlying local structure for a high dimensional dataset. The k-NN graph is relatively susceptible to the \u201ccurse of dimensionality\u201d and the associated distance concentration effect, where distances are similar in high dimensions, as well as the hub effect, where certain points become highly influential when highly connected. This skews the local representation of high dimensional data, deteriorating its performance for various similarity-based machine learning tasks.</p> <p>A recent paper titled Clustering with UMAP: Why and How Connectivity Matters proposes a refinement in the graph construction stage of the UMAP algorithm that uses a weighted mutual k-NN graph rather than it vanilla counterpart, to reduce the undesired distance concentration and hub effects.</p> <p>Mutual k-NN graphs have been shown to contain many desirable properties  when combating the \u201ccurse of dimensionality\u201d as discussed in this paper . However, one pitfall of using a mutual k-NN graph over the original k-NN graph is that it often contains disconnected components and potential isolated vertices.</p> <p>This violates one of UMAP primary assumptions that \"The manifold is locally connected.\" To combat the issue of isolated components, the authors consider different methods that have been previously used to augment and increase the connectivity of the mutual k-NN graph:</p> <ol> <li><code>NN</code>: To minimally connect isolated vertices and satisfy the assumption that the underlying manifold is locally connected, we add an undirected edge between each isolated vertex and its original nearest neighbor (de Sousa, Rezende, and Batista 2013).Note that the resulting graph may still contain disconnected components.</li> <li><code>MST-min</code>: To achieve a connected graph, add the minimum number of edges from a maximum spanning tree to the mutual k-NN graph that has been weighted with similarity-based metrics(Ozaki et al. 2011). We adapt this by calculating the minimum spanning tree for distances.</li> <li><code>MST-all</code>: Adding all the edges of the MST.</li> </ol> <p></p> <p>They also different ways to obtain the new local neighborhood for each point <code>x_i</code>:</p> <ol> <li><code>Adjacent Neighbors</code>: Only consider neighbors that are directly connected(adjacent) to <code>x_i</code> in the connected mutual k-NN graph.</li> <li><code>Path Neighbors</code>: Using shortest path distance to find the new k closest points to <code>x_i</code> with respect to the connected mutual k-NN graph. This shortest path distance can be considered a new distance metric as it directly aligns with UMAP\u2019s definition of an extended pseudo-metric space.</li> </ol> <p></p> <p>:width: 600   :align: center</p>"},{"location":"mutual_nn_umap/#visualizing-the-results","title":"Visualizing the Results","text":"<p>To see the differences between using a mutual k-NN graph vs the original k-NN graph as the starting topology for UMAP, let's visualize the 2D projections generated for MNIST, FMNIST, and 20 NG Count Vectors using each of the discussed methods. For all code snippets to reproduce the results and visualizations, please refer to this Github repo. Will be adding this soon as a mode to the original implementation.</p> <p>We\u2019ll start with MNIST digits, a collection of 70,000 gray-scale images of hand-written digits:</p> <p></p> <p>:width: 850   :align: center</p> <p>In general, for most of the mutual k-NN graph based vectors, there is a better separation between similar classes than the original UMAP vectors regardless of connectivity (NN, MST variants). Connecting isolated vertices in the mutual k-NN graph to their original nearest neighbor produced the desired separation between similar classes such as with the 4, 7, 9 in MNIST. This follows our intuition given that mutual k-NN graphs have previously been shown as a useful method for removing edges between points that are only loosely similar.</p> <p>Similar results are observed for the Fashion-MNIST(FMNIST) dataset, a collection of 70,000 gray-scale images of fashion items:</p> <p></p> <p>:width: 850   :align: center</p> <p>For the FMNIST dataset, the vectors using the aforementioned methods preserve the global structure between clothing classes (T-shirt/top, Coat, Trouser, and etc.) from footwear classes (Sandal, Sneaker, Ankle-boot) while also depicting a clearer separation between the footwear classes. This is contrasted with original UMAP which has poorer separation between similar classes like the footwear classes.</p> <p>For both MNIST and FMNIST, NN which naively connects isolated vertices to their nearest neighbor had multiple small clusters of points scattered throughout the vector space. This makes sense given using NN for connectivity can still cause the resulting manifold to be broken into many small components.</p> <p>It would be fair to assume that augmenting the mutual k-NN graph with a \"higher connectivity\" would always be better as it reduces random scattering of points. However, too much connectivity such as with MST-all can also hurt which is further discussed in the paper.</p> <p>Finally, we depict the embeddings generated using the 20 newsgroup dataset, a collection of 18846 documents, transformed using sklearns CountVectorizer:</p> <p></p> <p>:width: 850   :align: center</p> <p>We can see there is better distinction between similar subjects such as the recreation (rec) topics.</p> <p>Visually, the vector generated using the Adjacent Neighbors and MST-min result in disperse dense clusters of points e.g, the footwear classes in FMNIST and the recreation topics in 20 NG. However for Path Neighbors, the groups of points belonging to the same class are less dispersed. This is because Adjacent Neighbors are not guaranteed to have k connected neighbors for each local neighborhood. Points with smaller neighborhoods will be close to primarily few adjacent neighbors and repelled further away from the other points.</p> <p>To evaluate these methods quantitatively, the authors compare the clustering performance of the resulting low dimensional vectors generated. Below shows the Normalised Mutual Information NMI results after performing KMeans(for more information of the results please refer to <code>the full paper &lt;https://arxiv.org/abs/2108.05525&gt;</code>__).</p> <p></p> <p>These quantitative experiments show that MST variants combined with Path Neighbors can help produce better clustering results and how the initialization of a weighted connected graph is critical to the success of topology based dimensionality reduction methods like UMAP.</p>"},{"location":"mutual_nn_umap/#citing-our-work","title":"Citing our work","text":"<p>If you use this implementation or reference the results in your work, please cite the paper:</p> BibTeX <p>@article{Dalmia2021UMAPConnectivity,     author={Ayush Dalmia and Suzanna Sia},     title={Clustering with {UMAP:} Why and How Connectivity Matters},     journal={CoRR},     volume={abs/2108.05525},     year={2021},     url={https://arxiv.org/abs/2108.05525},     eprinttype={arXiv},     eprint={2108.05525},     timestamp={Wed, 18 Aug 2021 19:45:42 +0200},     biburl={https://dblp.org/rec/journals/corr/abs-2108-05525.bib},     bibsource={dblp computer science bibliography, https://dblp.org}     }</p>"},{"location":"nomic_atlas_umap_of_text_embeddings/","title":"UMAP of Text Embeddings with Nomic Atlas","text":"<p>Nomic Atlas is a platform for interactively visualizing and exploring massive datasets. It automates the creation of embeddings and 2D coordinate projections using UMAP.</p> <p></p> <p>Nomic Atlas automatically generates embeddings for your data and allows you to explore large datasets in a web browser. Atlas provides:</p> <ul> <li>In-browser analysis of your UMAP data with the Atlas Analyst</li> <li>Vector search over your UMAP data using the Nomic API</li> <li>Interactive features like zooming, recoloring, searching, and filtering in the Nomic Atlas data map</li> <li>Scalability for millions of data points</li> <li>Rich information display on hover</li> <li>Shareable UMAPs via URL links to your embeddings and data maps in Atlas</li> </ul> <p>This example demonstrates how to use Nomic Atlas to create interactive maps of text using embeddings and UMAP.</p>"},{"location":"nomic_atlas_umap_of_text_embeddings/#setup","title":"Setup","text":"<ol> <li>Get the required python packages with <code>pip instll nomic pandas</code></li> <li>Get a Nomic API key here</li> <li>Run <code>nomic login nk-...</code> in a terminal window or use the following code:</li> </ol> Python<pre><code>import nomic\nnomic.login('nk-...')\n</code></pre>"},{"location":"nomic_atlas_umap_of_text_embeddings/#download-example-data","title":"Download Example Data","text":"Python<pre><code>import pandas as pd\n\n# Example data\ndf = pd.read_csv(\"https://docs.nomic.ai/singapore_airlines_reviews.csv\")\n</code></pre>"},{"location":"nomic_atlas_umap_of_text_embeddings/#create-atlas-dataset","title":"Create Atlas Dataset","text":"Python<pre><code>from nomic import AtlasDataset\ndataset = AtlasDataset(\"airline-reviews-data\")\n</code></pre>"},{"location":"nomic_atlas_umap_of_text_embeddings/#upload-to-atlas","title":"Upload to Atlas","text":"Python<pre><code>dataset.add_data(df)\n</code></pre>"},{"location":"nomic_atlas_umap_of_text_embeddings/#create-data-map","title":"Create Data Map","text":"<p>We specify the <code>text</code> field from <code>df</code> as the field to create embeddings from. We choose some standard UMAP parameters as well.</p> Python<pre><code>from nomic.data_inference import ProjectionOptions\n\n# model=\"umap\" is how you choose UMAP in Nomic Atlas\n# You can adjust n_neighbors, min_dist,\n# and n_epochs as you would with the UMAP library.\natlas_map = dataset.create_index(\n    indexed_field='text',\n    projection=ProjectionOptions(\n      model=\"umap\",\n      n_neighbors=20,\n      min_dist=0.01,\n      n_epochs=200\n  )\n)\n\nprint(f\"Explore your interactive map at: {atlas_map.map_link}\")\n</code></pre> <p>Your map will be available in your Atlas Dashboard.</p>"},{"location":"nomic_atlas_visualizing_mnist_training_dynamics/","title":"Visualizing MNIST Training Dynamics with Nomic Atlas","text":"<p>Nomic Atlas is a platform for interactively visualizing and exploring massive datasets. It automates the creation of embeddings and 2D coordinate projections using UMAP.</p> <p></p> <p>Nomic Atlas automatically generates embeddings for your data and allows you to explore large datasets in a web browser. Atlas provides:</p> <ul> <li>In-browser analysis of your UMAP data with the Atlas Analyst</li> <li>Vector search over your UMAP data using the Nomic API</li> <li>Interactive features like zooming, recoloring, searching, and filtering in the Nomic Atlas data map</li> <li>Scalability for millions of data points</li> <li>Rich information display on hover</li> <li>Shareable UMAPs via URL links to your embeddings and data maps in Atlas</li> </ul> <p>This example demonstrates how to use Nomic Atlas to visualize the training dynamics of your neural network using embeddings and UMAP.</p>"},{"location":"nomic_atlas_visualizing_mnist_training_dynamics/#setup","title":"Setup","text":"<ol> <li>Get the python package with <code>pip install nomic</code></li> <li>Get a Nomic API key here</li> <li>Run <code>nomic login nk-...</code> in a terminal window or use the following code:</li> </ol> Python<pre><code>\n</code></pre> <p>import nomic    nomic.login('nk-...')</p>"},{"location":"nomic_atlas_visualizing_mnist_training_dynamics/#download-example-data","title":"Download Example Data","text":"Python <p>import torch    import torch.nn as nn    import torch.optim as optim    import torchvision    import torchvision.transforms as transforms    from torch.utils.data import DataLoader, Subset    import numpy as np    import time    from PIL import Image    import base64    import io</p> <p>NUM_EPOCHS = 20    LEARNING_RATE = 3e-6    BATCH_SIZE = 128    NUM_VIS_SAMPLES = 2000    EMBEDDING_DIM = 128    ATLAS_DATASET_NAME = \"mnist_training_embeddings\"    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    print(f\"Using device: {device}\\n\")</p> <p>def tensor_to_html(tensor):       \"\"\"Helper function to convert image tensors to HTML for rendering in Nomic Atlas\"\"\"       # Denormalize the image       img = torch.clamp(tensor.clone().detach().cpu().squeeze(0) * 0.3081 + 0.1307, 0, 1)       img_pil = Image.fromarray((img.numpy() * 255).astype('uint8'), mode='L')       buffered = io.BytesIO()       img_pil.save(buffered, format=\"PNG\")       img_str = base64.b64encode(buffered.getvalue()).decode()       return f''</p> <p>class MNIST_CNN(nn.Module):       def init(self, embedding_dim=128):          super(MNIST_CNN, self).init()          self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)          self.relu1 = nn.ReLU()          self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)          self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)          self.relu2 = nn.ReLU()          self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)          self.flatten = nn.Flatten()          self.fc1 = nn.Linear(64 * 7 * 7, embedding_dim)          self.relu3 = nn.ReLU()          self.fc2 = nn.Linear(embedding_dim, 10)</p> Text Only<pre><code>  def forward(self, x):\n     x = self.pool1(self.relu1(self.conv1(x)))\n     x = self.pool2(self.relu2(self.conv2(x)))\n     x = self.flatten(x)\n     embeddings = self.relu3(self.fc1(x))\n     output = self.fc2(embeddings)\n     return output, embeddings\n</code></pre> <p>transform = transforms.Compose([       transforms.ToTensor(),       transforms.Normalize((0.1307,), (0.3081,))    ])</p> <p>train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)    test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)</p> <p>persistent_workers_flag = True if device.type not in ['mps', 'cpu'] else False    num_workers_val = 2 if persistent_workers_flag else 0    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers_val, persistent_workers=persistent_workers_flag if num_workers_val &gt; 0 else False)    vis_indices = list(range(NUM_VIS_SAMPLES))    vis_subset = Subset(test_dataset, vis_indices)    test_loader_for_vis = DataLoader(vis_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers_val, persistent_workers=persistent_workers_flag if num_workers_val &gt; 0 else False)    print(f\"Training on {len(train_dataset)} samples, visualizing {NUM_VIS_SAMPLES} test samples per epoch.\\n\")</p>"},{"location":"nomic_atlas_visualizing_mnist_training_dynamics/#collect-embeddings-during-training","title":"Collect Embeddings During Training","text":"Python <p>model = MNIST_CNN(embedding_dim=EMBEDDING_DIM).to(device)    criterion = nn.CrossEntropyLoss()    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)    all_embeddings_list = []    all_metadata_list = []    all_images_html = []    overall_start_time = time.time()    for epoch in range(NUM_EPOCHS):       epoch_start_time = time.time()       model.train()       running_loss = 0.0       for batch_idx, (data, target) in enumerate(train_loader):          data, target = data.to(device), target.to(device)          optimizer.zero_grad()          outputs, _ = model(data)          loss = criterion(outputs, target)          loss.backward()          optimizer.step()          running_loss += loss.item()          if (batch_idx + 1) % 200 == 0:                print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Batch [{batch_idx+1}/{len(train_loader)}], Avg Loss: {running_loss / 200:.4f}')                running_loss = 0.0       print(f\"Epoch {epoch+1}/{NUM_EPOCHS} training finished in {time.time() - epoch_start_time:.2f}s.\\n\")       model.eval()       vis_samples_collected_this_epoch = 0       image_offset_in_vis_subset = 0       with torch.no_grad():          for data, target in test_loader_for_vis:                data, target = data.to(device), target.to(device)                , embeddings_batch = model(data)                for i in range(embeddings_batch.size(0)):                   original_idx_in_subset = image_offset_in_vis_subset + i                   if original_idx_in_subset &gt;= NUM_VIS_SAMPLES:                      continue                   all_embeddings_list.append(embeddings_batch[i].cpu().numpy())                   img_html = tensor_to_html(data[i])                   all_images_html.append(img_html)                   all_metadata_list.append({                      'id': f'vis_img{original_idx_in_subset}epoch{epoch}',                      'epoch': epoch,                      'label': f'Digit: {target[i].item()}',                      'vis_sample_idx': original_idx_in_subset,                      'image_html': img_html                   })                   vis_samples_collected_this_epoch += 1                image_offset_in_vis_subset += embeddings_batch.size(0)                if vis_samples_collected_this_epoch &gt;= NUM_VIS_SAMPLES:                   break       print(f\"Collected {vis_samples_collected_this_epoch} embeddings for visualization in epoch {epoch+1}.\\n\")    total_script_time = time.time() - overall_start_time    print(f\"Total training and embedding extraction time: {total_script_time:.2f}s\\n\")</p>"},{"location":"nomic_atlas_visualizing_mnist_training_dynamics/#create-atlas-dataset","title":"Create Atlas Dataset","text":"Python <p>from nomic import AtlasDataset    dataset = AtlasDataset(\"mnist-training-embeddings\")</p>"},{"location":"nomic_atlas_visualizing_mnist_training_dynamics/#upload-to-atlas","title":"Upload to Atlas","text":"Python <p>dataset.add_data(data=all_metadata_list, embeddings=np.array(all_embeddings_list))</p>"},{"location":"nomic_atlas_visualizing_mnist_training_dynamics/#create-data-map","title":"Create Data Map","text":"<p>We specify the <code>text</code> field from <code>df</code> as the field to create embeddings from. We choose some standard UMAP parameters as well.</p> Python<pre><code>\n</code></pre> <p>dataset.create_index(projection='umap', topic_model=False)</p> <p>Your map will be available in your Atlas Dashboard.</p>"},{"location":"outliers/","title":"Outlier detection using UMAP","text":"<p>While an earlier tutorial looked at using <code>UMAP for clustering &lt;https://umap.readthedocs.io/en/latest/clustering.html&gt;</code>__, it can also be used for outlier detection, providing that some care is taken. This tutorial will look at how to use UMAP in this manner, and what to look out for, by finding anomalous digits in the MNIST handwritten digits dataset. To start with let's load the relevant libraries:</p> Python<pre><code>import numpy as np\nimport sklearn.datasets\nimport sklearn.neighbors\nimport umap\nimport umap.plot\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</code></pre> <p>With this in hand, let's grab the MNIST digits dataset from the internet, using the new <code>fetch_ml</code> loader in sklearn.</p> Python<pre><code>data, labels = sklearn.datasets.fetch_openml('mnist_784', version=1, return_X_y=True)\n</code></pre> <p>Before we get started we should try looking for outliers in terms of the native 784 dimensional space that MNIST digits live in. To do this we will make use of the <code>Local Outlier Factor (LOF) &lt;https://en.wikipedia.org/wiki/Local_outlier_factor&gt;</code>__ method for determining outliers since sklearn has an easy to use implementation. The essential intuition of LOF is to look for points that have a (locally approximated) density that differs significantly from the average density of their neighbors. In our case the actual details are not so important -- it is enough to know that the algorithm is reasonably robust and effective on vector space data. We can apply it using the <code>fit_predict</code> method of the sklearn class. The LOF class take a parameter <code>contamination</code> which specifies the percentage of data that the user expects to be noise. For this use case we will set it to 0.001428 since, given the 70,000 samples in MNIST, this will result in 100 outliers, which we can then look at in more detail.</p> Python<pre><code>%%time\noutlier_scores = sklearn.neighbors.LocalOutlierFactor(contamination=0.001428).fit_predict(data)\n</code></pre> Text Only<pre><code>CPU times: user 1h 29min 10s, sys: 12.4 s, total: 1h 29min 22s\nWall time: 1h 29min 53s\n</code></pre> <p>It is worth noting how long that took. Over an hour and a half! Why did it take so long? Because LOF requires a notion of density, which in turn relies on a nearest neighbor type computation -- which is expensive in sklearn for high dimensional data. This alone is potentially a reason to look at reducing the dimension of the data -- it makes it more amenable to existing techniques like LOF.</p> <p>Now that we have a set of outlier scores we can find the actual outlying digit images -- these are the ones with scores equal to -1. Let's extract that data, and check that we got 100 different digit images.</p> Python<pre><code>outlying_digits = data[outlier_scores == -1]\noutlying_digits.shape\n</code></pre> Text Only<pre><code>(100, 784)\n</code></pre> <p>Now that we have the outlying digit images the first question we should be asking is \"what do they look like?\". Fortunately for us we can convert the 784 dimensional vectors back into image and plot them, making it easier to look at. Since we extracted the 100 most outlying digit images we can just display a 10x10 grid of them.</p> Python<pre><code>fig, axes = plt.subplots(7, 10, figsize=(10,10))\nfor i, ax in enumerate(axes.flatten()):\n    ax.imshow(outlying_digits[i].reshape((28,28)))\n    plt.setp(ax, xticks=[], yticks=[])\nplt.tight_layout()\n</code></pre> <p></p> <p>These do certainly look like somewhat strange looking handwritten digits, so our outlier detection seems to be working to some extent.</p> <p>Now let's try a naive approach using UMAP and see how far that gets us. First let's just apply UMAP directly with default parameters to the MNIST data.</p> Python<pre><code>mapper = umap.UMAP().fit(data)\n</code></pre> <p>Now we can see what we got using the new plotting tools in umap.plot.</p> Python<pre><code>umap.plot.points(mapper, labels=labels)\n</code></pre> Text Only<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1c3db71358&gt;\n</code></pre> <p></p> <p>That looks like what we have come to expect from a UMAP embedding of MNIST. The question is have we managed to preserve outliers well enough that LOF can still find the bizarre digit images, or has the embedding lost that information and contracted the outliers into the individual digit clusters? We can simply apply LOF to the embedding and see what that returns.</p> Python<pre><code>%%time\noutlier_scores = sklearn.neighbors.LocalOutlierFactor(contamination=0.001428).fit_predict(mapper.embedding_)\n</code></pre> <p>This was obviously much faster since we are operating in a much lower dimensional space that is more amenable to the spatial indexing methods that sklearn uses to find nearest neighbors. As before we extract the outlying digit images, and verify that we got 100 of them,</p> Python<pre><code>outlying_digits = data[outlier_scores == -1]\noutlying_digits.shape\n</code></pre> Text Only<pre><code>(100, 784)\n</code></pre> <p>Now we need to plot the outlying digit images to see what kinds of digit images this approach found to be particularly strange.</p> Python<pre><code>fig, axes = plt.subplots(7, 10, figsize=(10,10))\nfor i, ax in enumerate(axes.flatten()):\n    ax.imshow(outlying_digits[i].reshape((28,28)))\n    plt.setp(ax, xticks=[], yticks=[])\nplt.tight_layout()\n</code></pre> <p></p> <p>In many ways this looks to be a better result than the original LOF in the high dimensional space. While the digit images that the high dimensional LOF found to be strange were indeed somewhat odd looking, many of these digit images are considerably stranger -- significantly odd line thickness, warped shapes, and images that are hard to even recognise as digits. This helps to demonstrate a certain amount of confirmation bias when examining outliers: since we expect things tagged as outliers to be strange we tend to find aspects of them that justify that classification, potentially unaware of how much stranger some of the data may in fact be. This should make us wary of even this outlier set: what else might lurk in the dataset?</p> <p>We can, in fact, potentially improve on this result by tuning the UMAP embedding a little for the task of finding outliers. When UMAP combines together the different local simplicial sets (see :doc:<code>how_umap_works</code> for more details) the standard approach uses a union, but we could instead take an intersection. An intersection ensures that outliers remain disconnected, which is certainly beneficial when seeking to find outliers. A downside of the intersection is that it tends to break up the resulting simplicial set into many disconnected components and a lot of the more non-local and global structure is lost, resulting in a lot lower quality of the resulting embedding. We can, however, interpolate between the union and intersection. In UMAP this is given by the <code>set_op_mix_ratio</code>, where a value of 0.0 represents an intersection, and a value of 1.0 represents a union (the default value is 1.0). By setting this to a lower value, say 0.25, we can encourage the embedding to do a better job of preserving outliers as outlying, while still retaining the benefits of a union operation.</p> Python<pre><code>mapper = umap.UMAP(set_op_mix_ratio=0.25).fit(data)\n</code></pre> Python<pre><code>umap.plot.points(mapper, labels=labels)\n</code></pre> Text Only<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1c3f496908&gt;\n</code></pre> <p></p> <p>As you can see the embedding is not as well structured overall as when we had a <code>set_op_mix_ratio</code> of 1.0, but we have potentially done a better job of ensuring that outliers remain outlying. We can test that hypothesis by running LOF on this embedding and looking at the resulting digit images we get out. Ideally we should expect to find some potentially even stranger results.</p> Python<pre><code>%%time\noutlier_scores = sklearn.neighbors.LocalOutlierFactor(contamination=0.001428).fit_predict(mapper.embedding_)\n</code></pre> Python<pre><code>outlying_digits = data[outlier_scores == -1]\noutlying_digits.shape\n</code></pre> Text Only<pre><code>(100, 784)\n</code></pre> <p>We have the expected 100 most outlying digit images, so let's visualise the results and see if they really are particularly strange.</p> Python<pre><code>fig, axes = plt.subplots(10, 10, figsize=(10,10))\nfor i, ax in enumerate(axes.flatten()):\n    ax.imshow(outlying_digits[i].reshape((28,28)))\n    plt.setp(ax, xticks=[], yticks=[])\nplt.tight_layout()\n</code></pre> <p></p> <p>Here we see that the line thickness variation (particularly \"fat\" digits, or particularly \"fine\" lines) that the original embedding helped surface come through even more strongly here. We also see a number of clearly corrupted images with extra lines, dots, or strange blurring occurring.</p> <p>So, in summary, using UMAP to reduce dimension prior to running classical outlier detection methods such as LOF can improve both the speed with which the algorithm runs, and the quality of results the outlier detection can find. Furthermore we have introduced the <code>set_op_mix_ratio</code> parameter, and explained how it can be used to potentially improve the performance of outlier detection approaches applied to UMAP embeddings.</p>"},{"location":"parameters/","title":"Basic UMAP Parameters","text":"<p>UMAP is a fairly flexible non-linear dimension reduction algorithm. It seeks to learn the manifold structure of your data and find a low dimensional embedding that preserves the essential topological structure of that manifold. In this notebook we will generate some visualisable 4-dimensional data, demonstrate how to use UMAP to provide a 2-dimensional representation of it, and then look at how various UMAP parameters can impact the resulting embedding. This documentation is based on the work of Philippe Rivi\u00e8re for visionscarto.net.</p> <p>To start we'll need some basic libraries. First <code>numpy</code> will be needed for basic array manipulation. Since we will be visualising the results we will need <code>matplotlib</code> and <code>seaborn</code>. Finally we will need <code>umap</code> for doing the dimension reduction itself.</p> Python<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\nimport umap\n%matplotlib inline\n</code></pre> Python<pre><code>sns.set(style='white', context='poster', rc={'figure.figsize':(14,10)})\n</code></pre> <p>Next we will need some data to embed into a lower dimensional representation. To make the 4-dimensional data \"visualisable\" we will generate data uniformly at random from a 4-dimensional cube such that we can interpret a sample as a tuple of (R,G,B,a) values specifying a color (and translucency). Thus when we plot low dimensional representations each point can be colored according to its 4-dimensional value. For this we can use <code>numpy</code>. We will fix a random seed for the sake of consistency.</p> Python<pre><code>np.random.seed(42)\ndata = np.random.rand(800, 4)\n</code></pre> <p>Now we need to find a low dimensional representation of the data. As in the Basic Usage documentation, we can do this by using the <code>umap.umap_.UMAP.fit_transform</code> method on a <code>umap.umap_.UMAP</code> object.</p> Python<pre><code>fit = umap.UMAP()\n%time u = fit.fit_transform(data)\n</code></pre> Text Only<pre><code>CPU times: user 7.73 s, sys: 211 ms, total: 7.94 s\nWall time: 6.8 s\n</code></pre> <p>The resulting value <code>u</code> is a 2-dimensional representation of the data. We can visualise the result by using <code>matplotlib</code> to draw a scatter plot of <code>u</code>. We can color each point of the scatter plot by the associated 4-dimensional color from the source data.</p> Python<pre><code>plt.scatter(u[:,0], u[:,1], c=data)\nplt.title('UMAP embedding of random colours');\n</code></pre> <p></p> <p>As you can see the result is that the data is placed in 2-dimensional space such that points that were nearby in 4-dimensional space (i.e. are similar colors) are kept close together. Since we drew a random selection of points in the color cube there is a certain amount of induced structure from where the random points happened to clump up in color space.</p> <p>UMAP has several hyperparameters that can have a significant impact on the resulting embedding. In this notebook we will be covering the four major ones:</p> <ul> <li><code>n_neighbors</code></li> <li><code>min_dist</code></li> <li><code>n_components</code></li> <li><code>metric</code></li> </ul> <p>Each of these parameters has a distinct effect, and we will look at each in turn. To make exploration simpler we will first write a short utility function that can fit the data with UMAP given a set of parameter choices, and plot the result.</p> Python<pre><code>def draw_umap(n_neighbors=15, min_dist=0.1, n_components=2, metric='euclidean', title=''):\n    fit = umap.UMAP(\n        n_neighbors=n_neighbors,\n        min_dist=min_dist,\n        n_components=n_components,\n        metric=metric\n    )\n    u = fit.fit_transform(data);\n    fig = plt.figure()\n    if n_components == 1:\n        ax = fig.add_subplot(111)\n        ax.scatter(u[:,0], range(len(u)), c=data)\n    if n_components == 2:\n        ax = fig.add_subplot(111)\n        ax.scatter(u[:,0], u[:,1], c=data)\n    if n_components == 3:\n        ax = fig.add_subplot(111, projection='3d')\n        ax.scatter(u[:,0], u[:,1], u[:,2], c=data, s=100)\n    plt.title(title, fontsize=18)\n</code></pre>"},{"location":"parameters/#n_neighbors","title":"<code>n_neighbors</code>","text":"<p>This parameter controls how UMAP balances local versus global structure in the data. It does this by constraining the size of the local neighborhood UMAP will look at when attempting to learn the manifold structure of the data. This means that low values of <code>n_neighbors</code> will force UMAP to concentrate on very local structure (potentially to the detriment of the big picture), while large values will push UMAP to look at larger neighborhoods of each point when estimating the manifold structure of the data, losing fine detail structure for the sake of getting the broader of the data.</p> <p>We can see that in practice by fitting our dataset with UMAP using a range of <code>n_neighbors</code> values. The default value of <code>n_neighbors</code> for UMAP (as used above) is 15, but we will look at values ranging from 2 (a very local view of the manifold) up to 200 (a quarter of the data).</p> Python<pre><code>for n in (2, 5, 10, 20, 50, 100, 200):\n    draw_umap(n_neighbors=n, title='n_neighbors = {}'.format(n))\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>With a value of <code>n_neighbors=2</code> we see that UMAP merely glues together small chains, but due to the narrow/local view, fails to see how those connect together. It also leaves many different components (and even singleton points). This represents the fact that from a fine detail point of view the data is very disconnected and scattered throughout the space.</p> <p>As <code>n_neighbors</code> is increased UMAP manages to see more of the overall structure of the data, gluing more components together, and better coverying the broader structure of the data. By the stage of <code>n_neighbors=20</code> we have a fairly good overall view of the data showing how the various colors interelate to each other over the whole dataset.</p> <p>As <code>n_neighbors</code> increases further more and more focus in placed on the overall structure of the data. This results in, with <code>n_neighbors=200</code> a plot where the overall structure (blues, greens, and reds; high luminance versus low) is well captured, but at the loss of some of the finer local structure (individual colors are no longer necessarily immediately near their closest color match).</p> <p>This effect well exemplifies the local/global tradeoff provided by <code>n_neighbors</code>.</p>"},{"location":"parameters/#min_dist","title":"<code>min_dist</code>","text":"<p>The <code>min_dist</code> parameter controls how tightly UMAP is allowed to pack points together. It, quite literally, provides the minimum distance apart that points are allowed to be in the low dimensional representation. This means that low values of <code>min_dist</code> will result in clumpier embeddings. This can be useful if you are interested in clustering, or in finer topological structure. Larger values of <code>min_dist</code> will prevent UMAP from packing points together and will focus on the preservation of the broad topological structure instead.</p> <p>The default value for <code>min_dist</code> (as used above) is 0.1. We will look at a range of values from 0.0 through to 0.99.</p> Python<pre><code>for d in (0.0, 0.1, 0.25, 0.5, 0.8, 0.99):\n    draw_umap(min_dist=d, title='min_dist = {}'.format(d))\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Here we see that with <code>min_dist=0.0</code> UMAP manages to find small connected components, clumps and strings in the data, and emphasises these features in the resulting embedding. As <code>min_dist</code> is increased these structures are pushed apart into softer more general features, providing a better overarching view of the data at the loss of the more detailed topological structure.</p>"},{"location":"parameters/#n_components","title":"<code>n_components</code>","text":"<p>As is standard for many <code>scikit-learn</code> dimension reduction algorithms UMAP provides a <code>n_components</code> parameter option that allows the user to determine the dimensionality of the reduced dimension space we will be embedding the data into. Unlike some other visualisation algorithms such as t-SNE, UMAP scales well in the embedding dimension, so you can use it for more than just visualisation in 2- or 3-dimensions.</p> <p>For the purposes of this demonstration (so that we can see the effects of the parameter) we will only be looking at 1-dimensional and 3-dimensional embeddings, which we have some hope of visualizing.</p> <p>First of all we will set <code>n_components</code> to 1, forcing UMAP to embed the data in a line. For visualisation purposes we will randomly distribute the data on the y-axis to provide some separation between points.</p> Python<pre><code>draw_umap(n_components=1, title='n_components = 1')\n</code></pre> <p></p> <p>Now we will try <code>n_components=3</code>. For visualisation we will make use of <code>matplotlib</code>'s basic 3-dimensional plotting.</p> Python<pre><code>draw_umap(n_components=3, title='n_components = 3')\n</code></pre> Text Only<pre><code>/opt/anaconda3/envs/umap_dev/lib/python3.6/site-packages/sklearn/metrics/pairwise.py:257: RuntimeWarning: invalid value encountered in sqrt\n  return distances if squared else np.sqrt(distances, out=distances)\n</code></pre> <p></p> <p>Here we can see that with more dimensions in which to work UMAP has an easier time separating out the colors in a way that respects the topological structure of the data.</p> <p>As mentioned, there is really no requirement to stop at <code>n_components=3</code>. If you are interested in (density based) clustering, or other machine learning techniques, it can be beneficial to pick a larger embedding dimension (say 10, or 50) closer to the the dimension of the underlying manifold on which your data lies.</p>"},{"location":"parameters/#metric","title":"<code>metric</code>","text":"<p>The final UMAP parameter we will be considering in this notebook is the <code>metric</code> parameter. This controls how distance is computed in the ambient space of the input data. By default UMAP supports a wide variety of metrics, including:</p> <p>Minkowski style metrics</p> <ul> <li>euclidean</li> <li>manhattan</li> <li>chebyshev</li> <li>minkowski</li> </ul> <p>Miscellaneous spatial metrics</p> <ul> <li>canberra</li> <li>braycurtis</li> <li>haversine</li> </ul> <p>Normalized spatial metrics</p> <ul> <li>mahalanobis</li> <li>wminkowski</li> <li>seuclidean</li> </ul> <p>Angular and correlation metrics</p> <ul> <li>cosine</li> <li>correlation</li> </ul> <p>Metrics for binary data</p> <ul> <li>hamming</li> <li>jaccard</li> <li>dice</li> <li>russellrao</li> <li>kulsinski</li> <li>rogerstanimoto</li> <li>sokalmichener</li> <li>sokalsneath</li> <li>yule</li> </ul> <p>Precomputed</p> <p>All of the above metrics assume your input data is \"raw\" in some N-dimensional space. Sometimes, you have already calculated the pairwise distances between points, and your input data is a distance/similarity matrix. In this case, you can do something like <code>UMAP(metric='precomputed').fit_transform(&lt;distance matrix&gt;)</code>.</p> <p>Any of which can be specified by setting <code>metric='&lt;metric name&gt;'</code>; for example to use cosine distance as the metric you would use <code>metric='cosine'</code>.</p> <p>UMAP offers more than this however -- it supports custom user defined metrics as long as those metrics can be compiled in <code>nopython</code> mode by numba. For this notebook we will be looking at such custom metrics. To define such metrics we'll need numba ...</p> Python<pre><code>import numba\n</code></pre> <p>For our first custom metric we'll define the distance to be the absolute value of difference in the red channel.</p> Python<pre><code>@numba.njit()\ndef red_channel_dist(a,b):\n    return np.abs(a[0] - b[0])\n</code></pre> <p>To get more adventurous it will be useful to have some colorspace conversion -- to keep things simple we'll just use HSL formulas to extract the hue, saturation, and lightness from an (R,G,B) tuple.</p> Python<pre><code>@numba.njit()\ndef hue(r, g, b):\n    cmax = max(r, g, b)\n    cmin = min(r, g, b)\n    delta = cmax - cmin\n    if cmax == r:\n        return ((g - b) / delta) % 6\n    elif cmax == g:\n        return ((b - r) / delta) + 2\n    else:\n        return ((r - g) / delta) + 4\n\n@numba.njit()\ndef lightness(r, g, b):\n    cmax = max(r, g, b)\n    cmin = min(r, g, b)\n    return (cmax + cmin) / 2.0\n\n@numba.njit()\ndef saturation(r, g, b):\n    cmax = max(r, g, b)\n    cmin = min(r, g, b)\n    chroma = cmax - cmin\n    light = lightness(r, g, b)\n    if light == 1:\n        return 0\n    else:\n        return chroma / (1 - abs(2*light - 1))\n</code></pre> <p>With that in hand we can define three extra distances. The first simply measures the difference in hue, the second measures the euclidean distance in a combined saturation and lightness space, while the third measures distance in the full HSL space.</p> Python<pre><code>@numba.njit()\ndef hue_dist(a, b):\n    diff = (hue(a[0], a[1], a[2]) - hue(b[0], b[1], b[2])) % 6\n    if diff &lt; 0:\n        return diff + 6\n    else:\n        return diff\n\n@numba.njit()\ndef sl_dist(a, b):\n    a_sat = saturation(a[0], a[1], a[2])\n    b_sat = saturation(b[0], b[1], b[2])\n    a_light = lightness(a[0], a[1], a[2])\n    b_light = lightness(b[0], b[1], b[2])\n    return (a_sat - b_sat)**2 + (a_light - b_light)**2\n\n@numba.njit()\ndef hsl_dist(a, b):\n    a_sat = saturation(a[0], a[1], a[2])\n    b_sat = saturation(b[0], b[1], b[2])\n    a_light = lightness(a[0], a[1], a[2])\n    b_light = lightness(b[0], b[1], b[2])\n    a_hue = hue(a[0], a[1], a[2])\n    b_hue = hue(b[0], b[1], b[2])\n    return (a_sat - b_sat)**2 + (a_light - b_light)**2 + (((a_hue - b_hue) % 6) / 6.0)\n</code></pre> <p>With such custom metrics in hand we can get UMAP to embed the data using those metrics to measure the distance between our input data points. Note that <code>numba</code> provides significant flexibility in what we can do in defining distance functions. Despite this we retain the high performance we expect from UMAP even using such custom functions.</p> Python<pre><code>for m in (\"euclidean\", red_channel_dist, sl_dist, hue_dist, hsl_dist):\n    name = m if type(m) is str else m.__name__\n    draw_umap(n_components=2, metric=m, title='metric = {}'.format(name))\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p>And here we can see the effects of the metrics quite clearly. The pure red channel correctly sees the data as living on a one dimensional manifold, the hue metric interprets the data as living in a circle, and the HSL metric fattens out the circle according to the saturation and lightness. This provides a reasonable demonstration of the power and flexibility of UMAP in understanding the underlying topology of data, and finding a suitable low dimensional representation of that topology.</p>"},{"location":"parametric_umap/","title":"Parametric (neural network) Embedding","text":"<p>:language: python</p> <p>UMAP is comprised of two steps: First, compute a graph representing your data, second, learn an embedding for that graph:</p> <p></p> <p>Parametric UMAP replaces the second step, minimizing the same objective function as UMAP (we'll call it non-parametric UMAP here), but learning the relationship between the data and embedding using a neural network, rather than learning the embeddings directly:</p> <p></p> <p>Parametric UMAP is simply a subclass of UMAP, so it can be used just like nonparametric UMAP, replacing :python:<code>umap.UMAP</code> with :python:<code>parametric_umap.ParametricUMAP</code>. The most basic usage of parametric UMAP would be to simply replace UMAP with ParametricUMAP in your code:</p> Python<pre><code>from umap.parametric_umap import ParametricUMAP\nembedder = ParametricUMAP()\nembedding = embedder.fit_transform(my_data)\n</code></pre> <p>In this implementation, we use Keras and Tensorflow as a backend to train that neural network. The added complexity of a learned embedding presents a number of configurable settings available in addition to those in non-parametric UMAP. A set of Jupyter notebooks walking you through these parameters are available on the  GitHub repository</p>"},{"location":"parametric_umap/#defining-your-own-network","title":"Defining your own network","text":"<p>By default, Parametric UMAP uses 3-layer 100-neuron fully-connected neural network. To extend Parametric UMAP to use a more complex architecture, like a convolutional neural network, we simply need to define the network and pass it in as an argument to ParametricUMAP. This can be done easliy, using tf.keras.Sequential. Here's an example for MNIST:</p> Python<pre><code># define the network\nimport tensorflow as tf\ndims = (28, 28, 1)\nn_components = 2\nencoder = tf.keras.Sequential([\n    tf.keras.layers.InputLayer(input_shape=dims),\n    tf.keras.layers.Conv2D(\n        filters=32, kernel_size=3, strides=(2, 2), activation=\"relu\", padding=\"same\"\n    ),\n    tf.keras.layers.Conv2D(\n        filters=64, kernel_size=3, strides=(2, 2), activation=\"relu\", padding=\"same\"\n    ),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(units=256, activation=\"relu\"),\n    tf.keras.layers.Dense(units=256, activation=\"relu\"),\n    tf.keras.layers.Dense(units=n_components),\n])\nencoder.summary()\n</code></pre> <p>To load pass the data into ParametricUMAP, we first need to flatten it from 28x28x1 images to a 784-dimensional vector.</p> Python<pre><code>from tensorflow.keras.datasets import mnist\n(train_images, Y_train), (test_images, Y_test) = mnist.load_data()\ntrain_images = train_images.reshape((train_images.shape[0], -1))/255.\ntest_images = test_images.reshape((test_images.shape[0], -1))/255.\n</code></pre> <p>We can then pass the network into ParametricUMAP and train:</p> Python<pre><code># pass encoder network to ParametricUMAP\nembedder = ParametricUMAP(encoder=encoder, dims=dims)\nembedding = embedder.fit_transform(train_images)\n</code></pre> <p>If you are unfamilar with Tensorflow/Keras and want to train your own model, we reccomend that you take a look at the Tensorflow documentation.</p>"},{"location":"parametric_umap/#saving-and-loading-your-model","title":"Saving and loading your model","text":"<p>Unlike non-parametric UMAP Parametric UMAP cannot be saved simply by pickling the UMAP object because of the Keras networks it contains. To save Parametric UMAP, there is a built in function:</p> Python<pre><code>embedder.save('/your/path/here')\n</code></pre> <p>You can then load parametric UMAP elsewhere:</p> Python<pre><code>from umap.parametric_umap import load_ParametricUMAP\nembedder = load_ParametricUMAP('/your/path/here')\n</code></pre> <p>This loads both the UMAP object and the parametric networks it contains.</p>"},{"location":"parametric_umap/#plotting-loss","title":"Plotting loss","text":"<p>Parametric UMAP monitors loss during training using Keras. That loss will be printed after each epoch during training. This loss is saved in :python:<code>embedder._history</code>, and can be plotted:</p> Python<pre><code>print(embedder._history)\nfig, ax = plt.subplots()\nax.plot(embedder._history['loss'])\nax.set_ylabel('Cross Entropy')\nax.set_xlabel('Epoch')\n</code></pre> <p></p> <p>Much like other keras models, if you continue to train your model via the :python:<code>fit</code> method of the model, the :python:<code>embedder._history</code> will be updated with further training epoch losses.</p>"},{"location":"parametric_umap/#parametric-inverse_transform-reconstruction","title":"Parametric inverse_transform (reconstruction)","text":"<p>To use a second neural network to learn an inverse mapping between data and embeddings, we simply need to pass <code>parametric_reconstruction= True</code> to the ParametricUMAP.</p> <p>Like the encoder, a custom decoder can also be passed to ParametricUMAP, e.g.</p> Python<pre><code>        decoder = tf.keras.Sequential([\n            tf.keras.layers.InputLayer(input_shape=(n_components)),\n            tf.keras.layers.Dense(units=256, activation=\"relu\"),\n            tf.keras.layers.Dense(units=7 * 7 * 256, activation=\"relu\"),\n            tf.keras.layers.Reshape(target_shape=(7, 7, 256)),\n            tf.keras.layers.UpSampling2D((2)),\n            tf.keras.layers.Conv2D(\n                filters=64, kernel_size=3, padding=\"same\", activation=\"relu\"\n            ),\n            tf.keras.layers.UpSampling2D((2)),\n            tf.keras.layers.Conv2D(\n                filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"\n            ),\n\n        ])\n</code></pre> <p>In addition, validation data can be used to test reconstruction loss on out-of-dataset samples:</p> Python<pre><code>validation_images = test_images.reshape((test_images.shape[0], -1))/255.\n</code></pre> <p>Finally, we can pass the validation data and the networks to ParametricUMAP and train:</p> Python<pre><code>        embedder = ParametricUMAP(\n            encoder=encoder,\n            decoder=decoder,\n            dims=dims,\n            parametric_reconstruction= True,\n            reconstruction_validation=validation_images,\n            verbose=True,\n        )\n        embedding = embedder.fit_transform(train_images)\n</code></pre>"},{"location":"parametric_umap/#autoencoding-umap","title":"Autoencoding UMAP","text":"<p>In the example above, the encoder is trained to minimize UMAP loss, and the decoder is trained to minimize reconstruction loss. To train the encoder jointly on both UMAP loss and reconstruction loss, pass :python:<code>autoencoder_loss = True</code> into the ParametricUMAP.</p> Python<pre><code>        embedder = ParametricUMAP(\n            encoder=encoder,\n            decoder=decoder,\n            dims=dims,\n            parametric_reconstruction= True,\n            reconstruction_validation=validation_images,\n            autoencoder_loss = True,\n            verbose=True,\n        )\n</code></pre>"},{"location":"parametric_umap/#early-stopping-and-keras-callbacks","title":"Early stopping and Keras callbacks","text":"<p>It can sometimes be useful to train the embedder until some plateau in training loss is met. In deep learning, early stopping is one way to do this. Keras provides custom callbacks that allow you to implement checks during training, such as early stopping. We can use callbacks, such as early stopping, with ParametricUMAP to stop training early based on a predefined training threshold, using the :python:<code>keras_fit_kwargs</code> argument:</p> Python<pre><code>keras_fit_kwargs = {\"callbacks\": [\n    tf.keras.callbacks.EarlyStopping(\n        monitor='loss',\n        min_delta=10**-2,\n        patience=10,\n        verbose=1,\n    )\n]}\n\nembedder = ParametricUMAP(\n    verbose=True,\n    keras_fit_kwargs = keras_fit_kwargs,\n    n_training_epochs=20\n)\n</code></pre> <p>We also passed in :python:<code>n_training_epochs = 20</code>, allowing early stopping to end training before 20 epochs are reached.</p>"},{"location":"parametric_umap/#additional-important-parameters","title":"Additional important parameters","text":"<ul> <li>batch_size: ParametricUMAP in trained over batches of edges randomly sampled from the UMAP graph, and then trained via gradient descent.  ParametricUMAP defaults to a batch size of 1000 edges, but can be adjusted to a value that fits better on your GPU or CPU.</li> <li>loss_report_frequency: If set to 1, an epoch in in the Keras embedding refers to a single iteration over the graph computed in UMAP. Setting :python:<code>loss_report_frequency</code> to 10, would split up that epoch into 10 seperate epochs, for more frequent reporting.</li> <li>n_training_epochs: The number of epochs over the UMAP graph to train for (irrespective of :python:<code>loss_report_frequency</code>). Training the network for multiple epochs will result in better embeddings, but take longer. This parameter is different than :python:<code>n_epochs</code> in the base UMAP class, which corresponds to the maximum number of times an edge is trained in a single ParametricUMAP epoch.</li> <li>optimizer: The optimizer used to train the neural network. by default Adam (:python:<code>tf.keras.optimizers.Adam(1e-3)</code>) is used. You might be able to speed up or improve training by using a different optimizer.</li> <li>parametric_embedding: If set to false, a non-parametric embedding is learned, using the same code as the parametric embedding, which can serve as a direct comparison between parametric and non-parametric embedding using the same optimizer. The parametric embeddings are performed over the entire dataset simultaneously.</li> <li>global_correlation_loss_weight: Whether to additionally train on correlation of global pairwise relationships (multidimensional scaling)</li> <li>landmark_loss_fn: The loss function to use when re-training on landmarked data, where you have provided a desired location in the embedding space to the :python:<code>fit</code> method of the model. By default, euclidean loss is used. For more information on re-training, landmarks, and why you might use them, see :doc:<code>transform_landmarked_pumap</code>.</li> <li>landmark_loss_weight: How to weight the landmark loss relative to umap loss, by default 1.0.</li> </ul>"},{"location":"parametric_umap/#extending-the-model","title":"Extending the model","text":"<p>You may want to customize parametric UMAP beyond what we have implemented in this package. To make it as easy as possible to tinker around with Parametric UMAP, we made a few Jupyter notebooks that show you how to extend Parametric UMAP to your own use-cases.</p> <ul> <li>https://colab.research.google.com/drive/1WkXVZ5pnMrm17m0YgmtoNjM_XHdnE5Vp?usp=sharing</li> </ul>"},{"location":"parametric_umap/#citing-our-work","title":"Citing our work","text":"<p>If you use Parametric UMAP in your work, please cite our paper:</p> BibTeX<pre><code>@article{sainburg2021parametric,\n title={Parametric UMAP Embeddings for Representation and Semisupervised Learning},\n author={Sainburg, Tim and McInnes, Leland and Gentner, Timothy Q},\n journal={Neural Computation},\n volume={33},\n number={11},\n pages={2881--2907},\n year={2021},\n publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~\u2026}\n</code></pre> <p>}</p>"},{"location":"performance/","title":"Performance Comparison of Dimension Reduction Implementations","text":"<p>Different dimension reduction techniques can have quite different computational complexity. Beyond the algorithm itself there is also the question of how exactly it is implemented. These two factors can have a significant role in how long it actually takes to run a given dimension reduction. Furthermore the nature of the data you are trying to reduce can also matter \u2013 mostly the involves the dimensionality of the original data. Here we will take a brief look at the performance characterstics of a number of dimension reduction implementations.</p> <p>To start let\u2019s get the basic tools we\u2019ll need loaded up \u2013 numpy and pandas obviously, but also tools to get and resample the data, and the time module so we can perform some basic benchmarking.</p> Python<pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.utils import resample\nimport time\n</code></pre> <p>Next we\u2019ll need the actual dimension reduction implementations. For the purposes of this explanation we\u2019ll mostly stick with scikit-learn, but for the sake of comparison we\u2019ll also include the MulticoreTSNE implementation of t-SNE, and openTSNE both of which have historically had significantly better performance than scikit-learn t-SNE (more recent versions of scikit-learn have improved t-SNE performance).</p> Python<pre><code>from sklearn.manifold import TSNE, LocallyLinearEmbedding, Isomap, MDS, SpectralEmbedding\nfrom sklearn.decomposition import PCA\nfrom MulticoreTSNE import MulticoreTSNE\nfrom openTSNE import TSNE as OpenTSNE\nfrom umap import UMAP\n</code></pre> <p>Next we\u2019ll need out plotting tools, and, of course, some data to work with. For this performance comparison we\u2019ll default to the now standard benchmark of manifold learning: the MNIST digits dataset. We can use scikit-learn\u2019s <code>fetch_openml</code> to grab it for us.</p> Python<pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n</code></pre> Python<pre><code>sns.set(context='notebook',\n        rc={'figure.figsize':(12,10)},\n        palette=sns.color_palette('tab10', 10))\n</code></pre> Python<pre><code>mnist = fetch_openml('mnist_784', version=1, return_X_y=True)\n</code></pre> Python<pre><code>mnist_data = mnist[0]\nmnist_labels = mnist[1].astype(int)\n</code></pre> <p>Now it is time to start looking at performance. To start with let\u2019s look at how performance scales with increasing dataset size.</p>"},{"location":"performance/#performance-scaling-by-dataset-size","title":"Performance scaling by dataset size","text":"<p>As the size of a dataset increases the runtime of a given dimension reduction algorithm will increase at varying rates. If you ever want to run your algorithm on larger datasets you will care not just about the comparative runtime on a single small dataset, but how the performance scales out as you move to larger datasets. We can similate this by subsampling from MNIST digits (via scikit-learn\u2019s convenient <code>resample</code> utility) and looking at the runtime for varying sized subsamples. Since there is some randomness involved here (both in the subsample selection, and in some of the algorithms which have stochastic aspects) we will want to run a few examples for each dataset size. We can easily package all of this up in a simple function that will return a convenient pandas dataframe of dataset sizes and runtimes given an algorithm.</p> Python<pre><code>def data_size_scaling(algorithm, data, sizes=[100, 200, 400, 800, 1600], n_runs=5):\n    result = []\n    for size in sizes:\n        for run in range(n_runs):\n            subsample = resample(data, n_samples=size)\n            start_time = time.time()\n            algorithm.fit(subsample)\n            elapsed_time = time.time() - start_time\n            del subsample\n            result.append((size, elapsed_time))\n    return pd.DataFrame(result, columns=('dataset size', 'runtime (s)'))\n</code></pre> <p>Now we just want to run this for each of the various dimension reduction implementations so we can look at the results. Since we don\u2019t know how long these runs might take we\u2019ll start off with a very small set of samples, scaling up to only 1600 samples.</p> Python<pre><code>all_algorithms = [\n    PCA(),\n    UMAP(),\n    MulticoreTSNE(),\n    OpenTSNE(),\n    TSNE(),\n    LocallyLinearEmbedding(),\n    SpectralEmbedding(),\n    Isomap(),\n    MDS(),\n]\nperformance_data = {}\nfor algorithm in all_algorithms:\n    if 'openTSNE' in str(algorithm.__class__):\n        alg_name = \"OpenTSNE\"\n    elif 'MulticoreTSNE' in str(algorithm.__class__):\n        alg_name = \"MulticoreTSNE\"\n    else:\n        alg_name = str(algorithm).split('(')[0]\n\n    performance_data[alg_name] = data_size_scaling(algorithm, mnist_data, n_runs=5)\n\n    print(f\"[{time.asctime(time.localtime())}] Completed {alg_name}\")\n</code></pre> Text Only<pre><code>[Sat Feb 22 09:50:24 2020] Completed PCA\n[Sat Feb 22 09:51:23 2020] Completed UMAP\n[Sat Feb 22 09:53:24 2020] Completed MulticoreTSNE\n[Sat Feb 22 10:00:50 2020] Completed OpenTSNE\n[Sat Feb 22 10:02:22 2020] Completed TSNE\n[Sat Feb 22 10:02:44 2020] Completed LocallyLinearEmbedding\n[Sat Feb 22 10:03:06 2020] Completed SpectralEmbedding\n[Sat Feb 22 10:03:31 2020] Completed Isomap\n[Sat Feb 22 10:11:45 2020] Completed MDS\n</code></pre> <p>Now let\u2019s plot the results so we can see what is going on. We\u2019ll use seaborn\u2019s regression plot to interpolate the effective scaling. For some algorithms this can be a little noisy, especially in this relatively small dataset regime, but it will give us a good idea of what is going on.</p> Python<pre><code>for alg_name, perf_data in performance_data.items():\n    sns.regplot('dataset size', 'runtime (s)', perf_data, order=2, label=alg_name)\nplt.legend()\n</code></pre> <p></p> <p>We can see straight away that there are some outliers here. It is notable that openTSNE does poorly on small datasets. It does not have the scaling properties of MDS however; for larger dataset sizes MDS is going to quickly become completely unmanageable which openTSNE has fairly flat scaling. At the same time MulticoreTSNE demonstrates that t-SNE can run fairly efficiently. It is hard to tell much about the other implementations other than the fact that PCA is far and away the fastest option. To see more we\u2019ll have to look at runtimes on larger dataset sizes. Both MDS, Isomap and SpectralEmbedding will actually take too long to run so let\u2019s restrict ourselves to the fastest performing implementations and see what happens as we extend out to larger dataset sizes.</p> Python<pre><code>fast_algorithms = [\n    PCA(),\n    UMAP(),\n    MulticoreTSNE(),\n    OpenTSNE(),\n    TSNE(),\n    LocallyLinearEmbedding(),\n]\nfast_performance_data = {}\nfor algorithm in fast_algorithms:\n    if 'openTSNE' in str(algorithm.__class__):\n        alg_name = \"OpenTSNE\"\n    elif 'MulticoreTSNE' in str(algorithm.__class__):\n        alg_name = \"MulticoreTSNE\"\n    else:\n        alg_name = str(algorithm).split('(')[0]\n\n    fast_performance_data[alg_name] = data_size_scaling(algorithm, mnist_data,\n                                                   sizes=[1600, 3200, 6400, 12800, 25600], n_runs=4)\n\n    print(f\"[{time.asctime(time.localtime())}] Completed {alg_name}\")\n</code></pre> Text Only<pre><code>[Sat Feb 22 10:12:15 2020] Completed PCA\n[Sat Feb 22 10:14:51 2020] Completed UMAP\n[Sat Feb 22 11:16:05 2020] Completed MulticoreTSNE\n[Sat Feb 22 11:50:17 2020] Completed OpenTSNE\n[Sat Feb 22 13:06:38 2020] Completed TSNE\n[Sat Feb 22 14:14:36 2020] Completed LocallyLinearEmbedding\n</code></pre> Python<pre><code>for alg_name, perf_data in fast_performance_data.items():\n    sns.regplot('dataset size', 'runtime (s)', perf_data, order=2, label=alg_name)\nplt.legend()\n</code></pre> <p></p> <p>At this point we begin to see some significant differentiation among the different implementations. In the earlier plot OpenTSNE looked to be performing relatively poorly, but now the scaling effects kick in, and we see that is is faster than most. Similarly MulticoreTSNE looked to be slower than some of the other algorithms in th earlier plot, but as we scale out to larger datasets we see that its relative scaling performance is superior to the scikit-learn implementations of TSNE and locally linear embedding.</p> <p>It is probably worth extending out further \u2013 up to the full MNIST digits dataset. To manage to do that in any reasonable amount of time we\u2019ll have to restrict out attention to an even smaller subset of implementations. We will pare things down to just OpenTSNE, MulticoreTSNE, PCA and UMAP.</p> Python<pre><code>very_fast_algorithms = [\n    PCA(),\n    UMAP(),\n    MulticoreTSNE(),\n    OpenTSNE(),\n]\nvfast_performance_data = {}\nfor algorithm in very_fast_algorithms:\n    if 'openTSNE' in str(algorithm.__class__):\n        alg_name = \"OpenTSNE\"\n    elif 'MulticoreTSNE' in str(algorithm.__class__):\n        alg_name = \"MulticoreTSNE\"\n    else:\n        alg_name = str(algorithm).split('(')[0]\n\n    vfast_performance_data[alg_name] = data_size_scaling(algorithm, mnist_data,\n                                                    sizes=[3200, 6400, 12800, 25600, 51200, 70000], n_runs=2)\n\n    print(f\"[{time.asctime(time.localtime())}] Completed {alg_name}\")\n</code></pre> Text Only<pre><code>[Sat Feb 22 14:15:22 2020] Completed PCA\n[Sat Feb 22 14:18:59 2020] Completed UMAP\n[Sat Feb 22 17:04:58 2020] Completed MulticoreTSNE\n[Sat Feb 22 17:54:14 2020] Completed OpenTSNE\n</code></pre> Python<pre><code>for alg_name, perf_data in vfast_performance_data.items():\n    sns.regplot('dataset size', 'runtime (s)', perf_data, order=2, label=alg_name)\nplt.legend()\n</code></pre> <p></p> <p>Here we see UMAP\u2019s advantages over t-SNE really coming to the forefront. While UMAP is clearly slower than PCA, its scaling performance is dramatically better than MulticoreTSNE, and, despite the impressive scaling performance of openTSNE, UMAP continues to outperform it. Based on the slopes of the lines, for even larger datasets the difference between UMAP and t-SNE is only going to grow.</p> <p>This concludes our look at scaling by dataset size. The short summary is that PCA is far and away the fastest option, but you are potentially giving up a lot for that speed. UMAP, while not competitive with PCA, is clearly the next best option in terms of performance among the implementations explored here. Given the quality of results that UMAP can provide we feel it is clearly a good option for dimension reduction.</p>"},{"location":"plotting/","title":"Plotting UMAP results","text":"<p>UMAP is often used for visualization by reducing data to 2-dimensions. Since this is such a common use case the umap package now includes utility routines to make plotting UMAP results simple, and provide a number of ways to view and diagnose the results. Rather than seeking to provide a comprehensive solution that covers all possible plotting needs this umap extension seeks to provide a simple to use interface to make the majority of plotting needs easy, and help provide sensible plotting choices wherever possible. To get started looking at the plotting options let's load a variety of data to work with.</p> Python<pre><code>import sklearn.datasets\nimport pandas as pd\nimport numpy as np\nimport umap\n</code></pre> Python<pre><code>pendigits = sklearn.datasets.load_digits()\nmnist = sklearn.datasets.fetch_openml('mnist_784')\nfmnist = sklearn.datasets.fetch_openml('Fashion-MNIST')\n</code></pre> <p>To start we will fit a UMAP model to the pendigits data. This is as simple as running the fit method and assigning the result to a variable.</p> Python<pre><code>mapper = umap.UMAP().fit(pendigits.data)\n</code></pre> <p>If we want to do plotting we will need the <code>umap.plot</code> package. While the umap package has a fairly small set of requirements it is worth noting that if you want to using <code>umap.plot</code> you will need a variety of extra libraries that are not in the default requirements for umap. In particular you will need:</p> <ul> <li>matplotlib</li> <li>pandas</li> <li>datashader</li> <li>bokeh</li> <li>holoviews</li> </ul> <p>All should be either pip or conda installable. With those in hand you can import the <code>umap.plot</code> package.</p> Python<pre><code>import umap.plot\n</code></pre> <p>Now that we have the package loaded, how do we use it? The most straightforward thing to do is plot the umap results as points. We can achieve this via the function <code>umap.plot.points</code>. In its most basic form you can simply pass the trained UMAP model to <code>umap.plot.points</code>:</p> Python<pre><code>umap.plot.points(mapper)\n</code></pre> <p></p> <p>As you can see we immediately get a scatterplot of the UMAP embedding. Note that the function automatically selects a point-size based on the data density, and watermarks the image with the UMAP parameters that were used (this will include the metric if it is non-standard). The function also returns the matplotlib axes object associated to the plot, so further matplotlib functions, such as adding titles, axis labels etc. can be applied by the user if required.</p> <p>It is common for data passed to UMAP to have an associated set of labels, which may have been derived from ground-truth, from clustering, or via other means. In such cases it is desirable to be able to color the scatterplot according to the labelling. We can do this by simply passing the array of label information in with the <code>labels</code> keyword. The <code>umap.plot.points</code> function will color the data with a categorical colormap according to the labels provided.</p> Python<pre><code>umap.plot.points(mapper, labels=pendigits.target)\n</code></pre> <p></p> <p>Alternatively you may have extra data that is continuous rather than categorical. In this case you will want to use a continuous colormap to shade the data. Again this is straightforward to do -- pass in the continuous data with the <code>values</code> keyword and data will be colored accordingly using a continuous colormap.</p> <p>Furthermore, if you don't like the default color choices the <code>umap.plot.points</code> function offers a number of 'themes' that provide predefined color choices. Themes include:</p> <ul> <li>fire</li> <li>viridis</li> <li>inferno</li> <li>blue</li> <li>red</li> <li>green</li> <li>darkblue</li> <li>darkred</li> <li>darkgreen</li> </ul> <p>Here we will make use of the 'fire' theme to demonstrate how simple it is to change the aesthetics.</p> Python<pre><code>umap.plot.points(mapper, values=pendigits.data.mean(axis=1), theme='fire')\n</code></pre> <p></p> <p>If you want greater control you can specify exact colormaps and background colors. For example here we want to color the data by label, but use a black background and use the 'Paired' colormap for the categorical coloring (passed as <code>color_key_cmap</code>; the <code>cmap</code> keyword defines the continuous colormap).</p> Python<pre><code>umap.plot.points(mapper, labels=pendigits.target, color_key_cmap='Paired', background='black')\n</code></pre> <p></p> <p>Many more options are available including a <code>color_key</code> to specify a dictionary mapping of discrete labels to colors, <code>cmap</code> to specify the continous colormap, or the width and height of the resulting plot. Again, this does not provide comprehensive control of the plot aesthetics, but the goal here is to provide a simple to use interface rather than the ability for the user to fine tune all aspects -- users seeking such control are far better served making use of the individual underlying packages (matplotlib, datashader, and bokeh) by themselves.</p>"},{"location":"plotting/#plotting-larger-datasets","title":"Plotting larger datasets","text":"<p>Once you have a lot of data it becomes easier for a simple scatter plot to lie to you. Most notably overplotting, where markers for points overlap and pile up on top of each other, can deceive you into thinking that extremely dense clumps may only contain a few points. While there are things that can be done to help remedy this, such as reducing the point size, or adding an alpha channel, few are sufficient to be sure the plot isn't subtly lying to you in some way. <code>This essay &lt;https://datashader.org/user_guide/Plotting_Pitfalls.html&gt;</code>_ in the datashader documentation does an excellent job of describing the issues with overplotting, why the obvious solutions are not quite sufficient, and how to get around the problem. To make life easier for users the <code>umap.plot</code> package will automatically switch to using datashader for rendering once your dataset gets large enough. This helps to ensure you don't get fooled by overplotting. We can see this in action by working with one of the larger datasets such as Fashion-MNIST.</p> Python<pre><code>mapper = umap.UMAP().fit(fmnist.data)\n</code></pre> <p>Having fit the data with UMAP we can call <code>umap.plot.points</code> exactly as before, but this time, since the data is large enough to have potential overplotting, datashader will be used in the background for rendering.</p> Python<pre><code>umap.plot.points(mapper)\n</code></pre> <p></p> <p>All the same plot options as before hold, so we can color by labels, and apply the same themes, and it will all seamlessly use datashader for the actual rendering. Thus, regardless of how much data you have <code>umap.plot.points</code> will render it well with a transparent user interface. You, as a user, don't need to worry about switching to plotting with datashader, or how to convert your plotting to its slightly different API -- you can just use the same API and trust the results you get.</p> Python<pre><code>umap.plot.points(mapper, labels=fmnist.target, theme='fire')\n</code></pre> <p></p>"},{"location":"plotting/#interactive-plotting-and-hover-tools","title":"Interactive plotting, and hover tools","text":"<p>Rendering good looking static plots is important, but what if you want to be able to interact with your data -- pan around, and zoom in on the clusters to see the finer structure? What if you want to annotate your data with more complex labels than merely colors? Wouldn't it be good to be able to hover over data points and get more information about the individual point? Since this is a very common use case <code>umap.plot</code> tries to make it easy to quickly generate such plots, and provide basic utilities to allow you to have annotated hover tools working quickly. Again, the goal is not to provide a comprehensive solution that can do everything, but rather a simple to use and consistent API to get users up and running fast.</p> <p>To make a good example of this let's use a subset of the Fashion MNIST dataset. We can quickly train a new mapper object on that.</p> Python<pre><code>mapper = umap.UMAP().fit(fmnist.data[:30000])\n</code></pre> <p>The goal is to be able to hover over different points and see data associated with the given point (or points) under the cursor. For this simple demonstration we'll just use the target information of the point. To create hover information you need to construct a dataframe of all the data you would like to appear in the hover. Each row should correspond to a source of data points (appearing in the same order), and the columns can provide whatever extra data you would like to display in the hover tooltip. In this case we'll need a dataframe that can include the index of the point, its target number, and the actual name of the type of fashion item that target corresponds to. This is easy to quickly put together using pandas.</p> Python<pre><code>hover_data = pd.DataFrame({'index':np.arange(30000),\n                           'label':fmnist.target[:30000]})\nhover_data['item'] = hover_data.label.map(\n    {\n        '0':'T-shirt/top',\n        '1':'Trouser',\n        '2':'Pullover',\n        '3':'Dress',\n        '4':'Coat',\n        '5':'Sandal',\n        '6':'Shirt',\n        '7':'Sneaker',\n        '8':'Bag',\n        '9':'Ankle Boot',\n    }\n)\n</code></pre> <p>For interactive use the <code>umap.plot</code> package makes use of bokeh. Bokeh has several output methods, but in the approach we'll be outputting inline in a notebook. We have to enable this using the <code>output_notebook</code> function. Alteratively we could use <code>output_file</code> or other similar options -- see the bokeh documentation for more details.</p> Python<pre><code>umap.plot.output_notebook()\n</code></pre> Text Only<pre><code>&lt;div class=\"bk-root\"&gt;\n    &lt;a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"&gt;&lt;/a&gt;\n    &lt;span id=\"1001\"&gt;Loading BokehJS ...&lt;/span&gt;\n&lt;/div&gt;\n</code></pre> <p>Now we can make an interactive plot using <code>umap.plot.interactive</code>. This has a very similar API to the <code>umap.plot.points</code> approach, but also supports a <code>hover_data</code> keyword which, if passed a suitable dataframe, will provide hover tooltips in the interactive plot. Since bokeh allows different outputs, to display it in the notebook we will have to take the extra stop of calling <code>show</code> on the result.</p> Python<pre><code>p = umap.plot.interactive(mapper, labels=fmnist.target[:30000], hover_data=hover_data, point_size=2)\numap.plot.show(p)\n</code></pre> <p>View html file</p> <p>We get the sort of result one would like -- a fully interactive plot that can be zoomed in on, and more, but we also now have an interactive hover tool which presents the data from the dataframe we constructed. This allows a quick and easy method to get up and running with a richer interactive exploration of your UMAP plot. <code>umap.plot.interactive</code> supports all the same aesthetic parameters as <code>umap.plot.points</code> so you can theme your plot, color by label or value, and other similar operations explained above for <code>umap.plot.points</code>.</p>"},{"location":"plotting/#interactive-plotting-with-nomic-atlas","title":"Interactive plotting with Nomic Atlas","text":"<p>For interactive exploration, especially with large datasets, you can use Nomic Atlas. Nomic Atlas is a platform for embedding generation, visualization, analysis, retrieval, and everything you need to operationalize your embeddings and make them useful for your applications. It directly integrates UMAP as one of its projection models, allowing you to leverage UMAP within a powerful visualization environment.</p> <p>Nomic Atlas handles the embedding and UMAP dimensionality reduction process and provides an interactive interface with features like searching, filtering, coloring by metadata, and displaying rich information on hover. Atlas can help when you want to share your understanding of your UMAP visualizations or collaborate with others, as the map is accessible via a URL.</p> <p></p>"},{"location":"plotting/#plotting-connectivity","title":"Plotting connectivity","text":"<p>UMAP works by constructing an intermediate topological representation of the approximate manifold the data may have been sampled from. In practice this structure can be simplified down to a weighted graph. Sometimes it can be beneficial to see how that graph (representing connectivity in the manifold) looks with respect to the resulting embedding. It can be used to better understand the embedding, and for diagnostic purposes. To see the connectivity you can use the <code>umap.plot.connectivity</code> function. It works very similarly to the <code>umap.plot.points</code> function, and has the option as to whether to display the embedding point, or just the connectivity. To start let's do a simple plot showing the points:</p> Python<pre><code>umap.plot.connectivity(mapper, show_points=True)\n</code></pre> <p></p> <p>As with <code>umap.plot.points</code> there are options to control the basic aesthetics, including theme options and an <code>edge_cmap</code> keyword argument to specify the colormap used for displaying the edges.</p> <p>Since this approach already leverages datashader for edge plotting, we can go a step further and make use of the edge-bundling options available in datashader. This can provide a less busy view of connectivity, but can be expensive to compute, particularly for larger datasets.</p> Python<pre><code>umap.plot.connectivity(mapper, edge_bundling='hammer')\n</code></pre> <p></p>"},{"location":"plotting/#diagnostic-plotting","title":"Diagnostic plotting","text":"<p>Plotting the connectivity provides at least one basic diagnostic view that helps a user understand what is going on with an embedding. More views on data are better, of course, so <code>umap.plot</code> includes a <code>umap.plot.diagnostic</code> function that can provide various diagnostic plots. We'll look at a few of them here. To do so we'll use the full MNIST digits data set.</p> Python<pre><code>mapper = umap.UMAP().fit(mnist.data)\n</code></pre> <p>The first diagnostic type is a Principal Components Analysis based diagnostic, which you can select with <code>diagnostic_type='pca'</code>. The essence of the approach is that we can use PCA, which preserves global structure, to reduce the data to three dimensions. If we scale the results to fit in a 3D cube we can convert the 3D PCA coordinates of each point into an RGB description of a color. By then coloring the points in the UMAP embedding with the colors induced by the PCA it is possible to get a sense of how some of the more large scale global structure has been represented in the embedding.</p> Python<pre><code>umap.plot.diagnostic(mapper, diagnostic_type='pca')\n</code></pre> <p></p> <p>What we are looking for here is a generally smooth transition of colors, and an overall layout that broadly respects the color transitions. In this case the far left has a bottom cluster that transitions from dark green at the bottom to blue at the top, and this matches well with the cluster in the upper right which have a similar shade of blue at the bottom before transitioning to more cyan and blue. In contast in the right of the plot the lower cluster runs from purplish pink to green from top to bottom, while the cluster above it has its bottom edge more purple than green, suggesting that perhaps one or the other of these clusters has been flipped vertically during the optimization process, and this was never quite corrected.</p> <p>An alternative, but similar, approach is to use vector quantization as the method to generate a 3D embedding to generate colors. Vector quantization effectively finds 3 representative centers for the data, and then describes each data point in terms of its distance to these centers. Clearly this, again, captures a lot of the broad global structure of the data.</p> Python<pre><code>umap.plot.diagnostic(mapper, diagnostic_type='vq')\n</code></pre> <p></p> <p>Again we are looking for largely smooth transitions, and for related colors to match up between clusters. This view supports the fact that the left hand side of the embedding has worked well, but looking at the right hand side it seems clear that it is the upper two of the clusters that has been inadvertently flipped vertically. By contrasting views like this one can get a better sense of how well the embedding is working.</p> <p>For a different perspective we can look at approximations of the local dimension around each data point. Ideally the local dimension should match the embedding dimension (although this is often a lot to hope for. In practice when the local dimension is high this represents points (or areas of the space) that UMAP will have a harder time embedding as well. Thus one can trust the embedding to be more accurate in regions where the points have consistently lower local dimension.</p> Python<pre><code>local_dims = umap.plot.diagnostic(mapper, diagnostic_type='local_dim')\n</code></pre> <p></p> <p>As you can see, the local dimension of the data varies quite widely across the data. In particular the lower left cluster has the lowest local dimension -- this is actually unsurprising as this is the cluster corresponding to the digits 1: there are relatively few degrees of freedom over how a person draws a number one, and so the resulting local dimension is lower. In contrast the clusters in the middle have a much higher local dimension. We should expect the embedidng to be a little less accurate in these regions: it is hard to represent seven dimensional data well in only two dimensions, and compromises will need to be made.</p> <p>The final diagnostic we'll look at is how well local neighborhoods are preserved. We can measure this in terms of the Jaccard index of the local neighborhood in the high dimensional space compared to the equivalent neighborhood in the embedding. The Jaccard index is essentially the ratio of the number of neighbors that the two neighborhoods have in common over the total number of unique neighbors across the two neighborhoods. Higher values mean that the local neighborhood has been more accurately preserved.</p> Python<pre><code>umap.plot.diagnostic(mapper, diagnostic_type='neighborhood')\n</code></pre> <p></p> <p>As one might expect the local neighborhood preservation tends to be a lot better for those points that had a lower local dimension (as seen in the last plot). There is also a tendency for the edges of clusters (where there were clear boundaries to be followed) to have a better preservation of neighborhoods than the centers of the clusters that had higher local dimension. Again, this provides a view on which areas of the embedding you can have greater trust in, and which regions had to make compromises to embed into two dimensions.</p>"},{"location":"precomputed_k-nn/","title":"Precomputed k-nn","text":"<p>The purpose of this tutorial is to explore some cases where using a precomputed_knn might be useful and then discuss how we can obtain reproducible results with it.</p>"},{"location":"precomputed_k-nn/#practical-uses","title":"Practical Uses","text":""},{"location":"precomputed_k-nn/#trying-umap-with-various-parameters","title":"Trying UMAP with various parameters","text":"<p>Let\u2019s look at how we can use precomputed_knn to save time. First we will test it out on MNIST which has 70,000 samples of 784 dimensions. If we want to test out a series of n_neighbors and min_dist parameters, we might lose quite a bit of time recomputing the knn matrices for our data. Instead, we can compute the knn for the largest n_neighbors we wish to analyze and then feed that precomputed_knn to UMAP. UMAP will automatically prune it to the right n_neighbors value and skip the nearest neighbors step, saving us a lot of time.</p> <p>We note that we don\u2019t use a random state in order to leverage UMAP\u2019s parallelization and speed up the calculations.</p> Python<pre><code>from sklearn.datasets import fetch_openml\nimport numpy as np\nimport umap\nimport umap.plot\nfrom umap.umap_ import nearest_neighbors\n\ndata, labels = fetch_openml('mnist_784', version=1, return_X_y=True)\nlabels = np.asarray(labels, dtype=np.int32)\n\nn_neighbors = [5, 50, 100, 250]\nmin_dists = [0, 0.2, 0.5, 0.9]\nnormal_embeddings = np.zeros((4, 4, 70000, 2))\nprecomputed_knn_embeddings = np.zeros((4, 4, 70000, 2))\n</code></pre> Python<pre><code>%%time\n# UMAP run on the grid of parameters without precomputed_knn\nfor i, k in enumerate(n_neighbors):\n    for j, dist in enumerate(min_dists):\n        normal_embeddings[i, j] = umap.UMAP(n_neighbors=k,\n                                            min_dist=dist,\n                                           ).fit_transform(data)\nprint(\"\\033[1m\"+\"Time taken to compute UMAP on grid of parameters:\\033[0m\")\n</code></pre> Text Only<pre><code>**Time taken to compute UMAP on grid of parameters:**\nWall time: 31min 57s\n</code></pre> Python<pre><code>%%time\n# UMAP run on list of n_neighbors without precomputed_knn\n\n# We compute the knn for max(n_neighbors)=250\nmnist_knn = nearest_neighbors(data,\n                              n_neighbors=250,\n                              metric=\"euclidean\",\n                              metric_kwds=None,\n                              angular=False,\n                              random_state=None,\n                             )\n# Now we compute the embeddings for the grid of parameters\nfor i, k in enumerate(n_neighbors):\n    for j, dist in enumerate(min_dists):\n        precomputed_knn_embeddings[i, j] = umap.UMAP(n_neighbors=k,\n                                                      min_dist=dist,\n                                                      precomputed_knn=mnist_knn,\n                                                      ).fit_transform(data)\nprint(\"\\033[1m\"+\"Time taken to compute UMAP on grid of parameters with precomputed_knn:\\033[0m\")\n</code></pre> Text Only<pre><code>**Time taken to compute UMAP on grid of parameters with precomputed_knn:**\nWall time: 17min 54s\n</code></pre> <p>Using a precomputed_knn we have cut the computation time in half! Observe that half of our n_neighbors values are relatively small. If instead, we had had a higher distribution of values, the time savings would have been even greater. Additionaly, we could've saved even more time by first computing UMAP normally with an n_neighbors value of 250 and then extracting the k-nn graph from that UMAP object.</p> <p>With this, we can easily visualize how the n_neighbors parameter affects our embedding.</p> Python<pre><code>import matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(4, 4, figsize=(20, 20))\n\nfor i, ax_row in enumerate(axs):\n    for j, ax in enumerate(ax_row):\n        ax.scatter(precomputed_knn_embeddings[i, j, :, 0],\n                   precomputed_knn_embeddings[i, j, :, 1],\n                   c=labels / 9,\n                   cmap='tab10',\n                   alpha=0.1,\n                   s=1,\n                   )\n        ax.set_xticks([])\n        ax.set_yticks([])\n        if i == 0:\n            ax.set_title(\"min_dist = {}\".format(min_dists[j]), size=15)\n        if j == 0:\n            ax.set_ylabel(\"n_neighbors = {}\".format(n_neighbors[i]), size=15)\nfig.suptitle(\"UMAP embedding of MNIST digits with grid of parameters\", y=0.92, size=20)\nplt.subplots_adjust(wspace=0.05, hspace=0.05)\n</code></pre> <p></p> <p>We see that in this case, the embedding is robust to the choice of n_neighbors and that lower min_dist values simply pack the clusters more tightly.</p>"},{"location":"precomputed_k-nn/#reproducibility","title":"Reproducibility","text":"<p>We strongly recommend that you review the UMAP <code>reproducibility section &lt;https://umap.readthedocs.io/en/latest/reproducibility.html&gt;</code>__ in the docs before attempting to reproduce results with precomputed_knn.</p>"},{"location":"precomputed_k-nn/#standard-case","title":"Standard Case","text":"<p>Out of the box, UMAP with precomputed_knn supports creating reproducible results. This works in exactly the same way as regular UMAP, where, the user can set a random seed state to ensure that results can be reproduced exactly. However, some important considerations must be taken into account.</p> <p>UMAP embeddings are entirely dependent on first, computing the graphical representation in higher dimensions and second, learning an embedding that preserves the structure of that graph. Recall that our graphical representation is based on the k-nn graph of our data. If we have two different k-nn graphs, then we will naturally have two different graphical representations of our data. Therefore, we can only ensure reproducible results when we use the same k-nn graph. In our case, this means that all reproducible results are tied to three values:</p> <ol> <li>  The random seed when computing the k-nn.       </li> <li>  The n_neighbors value when computing the k-nn.       </li> <li>  The random seed when running UMAP.       </li> </ol> <p>Two different runs of UMAP, with these three values being equal, are guaranteed to return the same result. Let\u2019s look at how this works with an example. To do this, we\u2019ll create some data to work with; three random blobs in 60-dimensional space.</p> Python<pre><code>y = np.random.rand(1700, 60)\nX = np.concatenate((y+20, y, y-20))\nsynthetic_labels = np.repeat([1, 2, 3], repeats=1700)\n</code></pre> <p>With the data in hand, we can fix the three parameters listed above and see how two different UMAP runs give the same result. To avoid confusion we\u2019ll assume that the UMAP random seed is the same value as the knn random seed.</p> Python<pre><code>import umap.plot\nrandom_seed = 10\n\nknn = nearest_neighbors(\n                        X,\n                        n_neighbors=50,\n                        metric='euclidean',\n                        metric_kwds=None,\n                        angular=False,\n                        random_state=random_seed,\n                        )\n\nknn_umap = umap.UMAP(n_neighbors=30, precomputed_knn=knn, random_state=random_seed).fit(X)\nknn_umap2 = umap.UMAP(n_neighbors=30, precomputed_knn=knn, random_state=random_seed).fit(X)\n\nfig, ax = plt.subplots(1, 2, figsize=(13,7))\numap.plot.points(knn_umap, labels=synthetic_labels, ax=ax[0], theme='green')\numap.plot.points(knn_umap2, labels=synthetic_labels, ax=ax[1], theme='green')\nax[0].set_title(\"Precomuted knn 1st run\", size=16)\nax[1].set_title(\"Precomuted knn 2nd run\", size=16)\nplt.show()\n\nprint(\"\\033[1m\"+\"Are the embeddings for knn_umap and knn_umap2 the same?\\033[0m\")\nprint((knn_umap.embedding_ == knn_umap2.embedding_).all())\n</code></pre> <p></p> Text Only<pre><code>**Are the embeddings for knn_umap and knn_umap2 the same?**\nTrue\n</code></pre> <p>As we can see, by fixing the random_seed and the n_neighbors for the knn, we have been able to obtain identical results from both UMAP runs. In contrast, if these differ, we can\u2019t guarantee the same result.</p> Python<pre><code>random_seed2 = 15\n\n# Different n_neighbors\nknn3 = nearest_neighbors(\n                        X,\n                        n_neighbors=40,\n                        metric='euclidean',\n                        metric_kwds=None,\n                        angular=False,\n                        random_state=random_seed,\n                        )\n# Different random seed\nknn4 = nearest_neighbors(\n                        X,\n                        n_neighbors=50,\n                        metric='euclidean',\n                        metric_kwds=None,\n                        angular=False,\n                        random_state=random_seed2,\n                        )\n\nknn_umap3 = umap.UMAP(n_neighbors=30, precomputed_knn=knn3, random_state=random_seed).fit(X)\nknn_umap4 = umap.UMAP(n_neighbors=30, precomputed_knn=knn4, random_state=random_seed2).fit(X)\n\nfig, ax = plt.subplots(1, 2, figsize=(13,7))\numap.plot.points(knn_umap3, labels=synthetic_labels, ax=ax[0], theme='green')\numap.plot.points(knn_umap4, labels=synthetic_labels, ax=ax[1], theme='green')\nax[0].set_title(\"Precomuted knn; different knn n_neighbors\", size=16)\nax[1].set_title(\"Precomuted knn; different random_seed\", size=16)\nplt.show()\n\nprint(\"\\033[1m\"+\"Are the embeddings for knn_umap and knn_umap3 the same?\\033[0m\")\nprint((knn_umap.embedding_ == knn_umap3.embedding_).all())\n\nprint(\"\\033[1m\"+\"Are the embeddings for knn_umap and knn_umap4 the same?\\033[0m\")\nprint((knn_umap.embedding_ == knn_umap4.embedding_).all())\n</code></pre> <p></p> Text Only<pre><code>**Are the embeddings for knn_umap and knn_umap3 the same?**\nFalse\n**Are the embeddings for knn_umap and knn_umap4 the same?**\nFalse\n</code></pre> <p>Without those three parameters being equal between runs, we have obtained different results.</p>"},{"location":"precomputed_k-nn/#reproducing-normal-umap-with-precomputed_knn","title":"Reproducing normal UMAP with precomputed_knn","text":"<p>With some extra considerations, we can also reproduce precomputed_knn results with normal UMAP and vice-versa. As in the previous case, we must keep in mind that the k-nn graphs have to be same. Additionaly, we also must consider how UMAP uses the random_seed that we provide it.</p> <p>If you provide UMAP a random_seed, it converts it into an np.random.RandomState (RNG). This RNG is then used to fix the state for all the relevant steps in the algorithm. The important thing to note, is that the RNG is mutated everytime it\u2019s used. So, if we want to reproduce results with precomputed_knn we\u2019ll have to mimic how UMAP manipulates the RNG when calling the fit() function.</p> <p>For more information on random states and their behavior, please refer to [1].</p> <p>We\u2019ll look at one example of how this can be accomplished. Other cases can be easily infered from this. Using the same random blobs as before, we seek to run UMAP normally and then reproduce the results with a precomputed_knn. To accomplish this, we have to create a new k-nn graph using the nearest_neighbors() function in the same way that fit() would.</p> Python<pre><code>from sklearn.utils import check_random_state\n\n# First we run the normal UMAP to compare with\nrandom_seed3 = 12\nnormal_umap = umap.UMAP(n_neighbors=30, random_state=random_seed3).fit(X)\n\n# Now we run precomputed_knn UMAP\nrandom_state3 = check_random_state(random_seed3)\n# random_state3 = numpy.random.RandomState(random_seed3)\nknn5 = nearest_neighbors(\n                        X,\n                        n_neighbors=30,\n                        metric='euclidean',\n                        metric_kwds=None,\n                        angular=False,\n                        random_state=random_state3,\n                        )\n# This mutated RNG can now be fed into precompute_knn UMAP to obtain\n# the same results as in normal UMAP\nknn_umap5 = umap.UMAP(n_neighbors=30,\n                      precomputed_knn=knn5,\n                      random_state=random_state3,  # &lt;--- This is a RNG\n                     ).fit(X)\n</code></pre> <p>Note that in this case we create a numpy.random.mtrand.RandomState instance with check_random_state() because we want to ensure that our RNG is created and mutated in exactly the same way that UMAP normally does. Equivalently, we could call numpy.random.RandomState() directly.</p> <p>Graphing and comparing the embeddings, we see that we were able to obtain the same results.</p> Python<pre><code>fig, ax = plt.subplots(1, 2, figsize=(13,7))\numap.plot.points(normal_umap, labels=synthetic_labels, ax=ax[0], theme='green')\numap.plot.points(knn_umap5, labels=synthetic_labels, ax=ax[1], theme='green')\nax[0].set_title(\"Normal UMAP\", size=16)\nax[1].set_title(\"Precomuted knn UMAP\", size=16)\nplt.show()\n\nprint(\"\\033[1m\"+\"Are the embeddings for normal_umap and knn_umap5 the same?\\033[0m\")\nprint((normal_umap.embedding_ == knn_umap5.embedding_).all())\n</code></pre> <p></p> Text Only<pre><code>**Are the embeddings for normal_umap and knn_umap5 the same?**\nTrue\n</code></pre>"},{"location":"release_notes/","title":"Release Notes","text":"<p>Some notes on new features in various releases</p>"},{"location":"release_notes/#whats-new-in-05","title":"What's new in 0.5","text":"<ul> <li>ParametricUMAP learns embeddings with neural networks.</li> <li>AlignedUMAP can align multiple embeddings using relations between datasets.</li> <li>DensMAP can preserve local density information in embeddings.</li> <li>UMAP now depends on PyNNDescent, but has faster more parallel performance as a result.</li> <li>UMAP now supports an <code>update</code> method to add new data and retrain.</li> <li>Various performance improvements and bug fixes.</li> <li>Additional plotting support, including text searching in interactive plots.</li> <li>Support for \"maximal distances\" in neighbor graphs.</li> </ul>"},{"location":"release_notes/#whats-new-in-04","title":"What's new in 0.4","text":"<ul> <li>Inverse transform method. Generate points in the original space corresponding to points in embedded space. (Thanks to Joseph Courtney)</li> <li>Different embedding spaces. Support for embedding to a variety of different spaces other than Euclidean. (Thanks to Joseph Courtney)</li> <li>New metrics, including Hellinger distance for sparse count data.</li> <li>New discrete/label metrics, including hierarchical categories, counts, ordinal data, and string edit distance.</li> <li>Support for parallelism in neighbor search and layout optimization. (Thanks to Tom White)</li> <li>Support for alternative methods to handling duplicated data samples. (Thanks to John Healy)</li> <li>New plotting methods for fast and easy plots.</li> <li>Initial support for dataframe embedding -- still experimental, but worth trying.</li> <li>Support for transform methods with sparse data.</li> <li>Multithreading support when no random seed is set.</li> </ul>"},{"location":"release_notes/#whats-new-in-03","title":"What's new in 0.3","text":"<ul> <li>Supervised and semi-supervised dimension reduction. Support for using labels or partial labels for dimension reduction.</li> <li>Transform method. Support for adding new unseen points to an existing embedding.</li> <li>Performance improvements.</li> </ul>"},{"location":"release_notes/#whats-new-in-02","title":"What's new in 0.2","text":"<ul> <li>A new layout algorithm that handles large datasets (more) correctly.</li> <li>Performance improvements.</li> </ul>"},{"location":"reproducibility/","title":"UMAP Reproducibility","text":"<p>UMAP is a stochastic algorithm -- it makes use of randomness both to speed up approximation steps, and to aid in solving hard optimization problems. This means that different runs of UMAP can produce different results. UMAP is relatively stable -- thus the variance between runs should ideally be relatively small -- but different runs may have variations none the less. To ensure that results can be reproduced exactly UMAP allows the user to set a random seed state.</p> <p>Since version 0.4 UMAP also support multi-threading for faster performance; when performing optimization this exploits the fact that race conditions between the threads are acceptable within certain optimization phases. Unfortunately this means that the randomness in UMAP outputs for the multi-threaded case depends not only on the random seed input, but also on race conditions between threads during optimization, over which no control can be had. This means that multi-threaded UMAP results cannot be explicitly reproduced.</p> <p>In this tutorial we'll look at how UMAP can be used in multi-threaded mode for performance purposes, and alternatively how we can fix random states to ensure exact reproducibility at the cost of some performance. First let's load the relevant libraries and get some data; in this case the MNIST digits dataset.</p> Python<pre><code>import numpy as np\nimport sklearn.datasets\nimport umap\nimport umap.plot\n</code></pre> Python<pre><code>data, labels = sklearn.datasets.fetch_openml(\n    'mnist_784', version=1, return_X_y=True\n)\n</code></pre> <p>With data in hand let's run UMAP on it, and note how long it takes to run:</p> Python<pre><code>%%time\nmapper1 = umap.UMAP().fit(data)\n</code></pre> Text Only<pre><code>CPU times: user 3min 18s, sys: 3.84 s, total: 3min 22s\nWall time: 1min 29s\n</code></pre> <p>The thing to note here is that the \"Wall time\" is significantly smaller than the CPU time -- this means that multiple CPU cores were used. For this particular demonstration I am making use of the latest version of PyNNDescent for nearest neighbor search (UMAP will use it if you have it installed) which supports multi-threading as well. The result is a very fast fitting to the data that does an effective job of using several cores. If you are on a large server with many cores available and don't wish to use them all (which is the default situation) you can currently control the number of cores used by setting the numba environment variable <code>NUMBA_NUM_THREADS</code>; see the <code>numba documentation &lt;https://numba.pydata.org/numba-doc/dev/reference/envvars.html#threading-control&gt;</code>__ for more details.</p> <p>Now let's plot our result to see what the embedding looks like:</p> Python<pre><code>umap.plot.points(mapper1, labels=labels)\n</code></pre> <p></p> <p>Now, let's run UMAP again and compare the results to that of our first run.</p> Python<pre><code>%%time\nmapper2 = umap.UMAP().fit(data)\n</code></pre> Text Only<pre><code>CPU times: user 2min 53s, sys: 4.16 s, total: 2min 57s\nWall time: 1min 5s\n</code></pre> <p>You will note that this time we ran even faster. This is because during the first run numba was still JIT compiling some of the code in the background. In contrast, this time that work has already been done, so it no longer takes up any of our run-time. We see that we are still making use of mutliple cores well.</p> <p>Now let's plot the results of this second run and compare to the first:</p> Python<pre><code>umap.plot.points(mapper2, labels=labels)\n</code></pre> <p></p> <p>Qualitatively this looks very similar, but a little closer inspection will quickly show that the results are actually different between the runs. Note that even in versions of UMAP prior to 0.4 this would have been the case -- since we fixed no specific random seed, and were thus using the current random state of the system which will naturally differ between runs. This is the default behaviour, as is standard with sklearn estimators that are stochastic. Rather than having a default random seed the user is required to explicitly provide one should they want a reproducible result. As noted by Vito Zanotelli</p> Text Only<pre><code>... setting a random seed is like signing a waiver \"I am aware that\nthis is a stochastic algorithm and I have done sufficient tests to\nconfirm that my main conclusions are not affected by this\nrandomness\".\n</code></pre> <p>With that in mind, let's see what happens if we set an explicit <code>random_state</code> value:</p> Python<pre><code>%%time\nmapper3 = umap.UMAP(random_state=42).fit(data)\n</code></pre> Text Only<pre><code>CPU times: user 2min 27s, sys: 4.16 s, total: 2min 31s\nWall time: 1min 56s\n</code></pre> <p>The first thing to note that that this run took significantly longer (despite having all the functions JIT compiled by numba already). Then note that the Wall time and CPU times are now much closer to each other -- we are no longer exploiting multiple cores to anywhere near the same degree. This is because by setting a <code>random_state</code> we are effectively turning off any of the multi-threading that does not support explicit reproducibility. Let's plot the results:</p> Python<pre><code>umap.plot.points(mapper3, labels=labels)\n</code></pre> <p></p> <p>We arrive at much the same results as before from a qualitative point of view, but again inspection will show that there are some differences. More importantly this result should now be reproducible. Thus we can run UMAP again, with the same <code>random_state</code> set ...</p> Python<pre><code>%%time\nmapper4 = umap.UMAP(random_state=42).fit(data)\n</code></pre> Text Only<pre><code>CPU times: user 2min 26s, sys: 4.13 s, total: 2min 30s\nWall time: 1min 54s\n</code></pre> <p>Again, this takes longer than the earlier runs with no <code>random_state</code> set. However when we plot the results of the second run we see that they look not merely qualitatively similar, but instead appear to be almost identical:</p> Python<pre><code>umap.plot.points(mapper4, labels=labels)\n</code></pre> <p></p> <p>We can, in fact, check that the results are identical by verifying that each and every coordinate of the resulting embeddings match perfectly:</p> Python<pre><code>np.all(mapper3.embedding_ == mapper4.embedding_)\n</code></pre> Text Only<pre><code>True\n</code></pre> <p>So we have, in fact, reproduced the embedding exactly.</p>"},{"location":"scientific_papers/","title":"Scientific Papers","text":"<p>UMAP has been used in a wide variety of scientific publications from a diverse range of fields. Here we will highlight a small selection of papers that demonstrate both the depth of analysis, and breadth of subjects, UMAP can be used for. These range from biology, to machine learning, and even social science.</p>"},{"location":"scientific_papers/#the-single-cell-transcriptional-landscape-of-mammalian-organogenesis","title":"The single-cell transcriptional landscape of mammalian organogenesis","text":"<p>A detailed look at the development of mouse embryos from a single-cell view. UMAP is used as a core piece of The Monocle3 software suite for identifying cell types and trajectories. This was a major paper in Nature, demonstrating the power of UMAP for large scale scientific endeavours.</p> <p></p> <p>Link to the paper</p>"},{"location":"scientific_papers/#a-lineage-resolved-molecular-atlas-of-c-elegans-embryogenesis-at-single-cell-resolution","title":"A lineage-resolved molecular atlas of C. elegans embryogenesis at single-cell resolution","text":"<p>Still in the realm of single cell biology this paper looks at the developmental landscape of the round-word C. elegans. UMAP is used for detailed analysis of the developmental trajectories of cells, looking at global scales, and then digging down to look at individual organs. The result is an impressive array of UMAP visualisations that tease out ever finer structures in cellular development.</p> <p></p> <p>Link to the paper</p>"},{"location":"scientific_papers/#exploring-neural-networks-with-activation-atlases","title":"Exploring Neural Networks with Activation Atlases","text":"<p>Understanding the image processing capabilities (and deficits!) of modern convolutional neural networks is a challenge. This interactive paper from Distill seeks to provide a way to \"peek inside the black box\" by looking at the activations throughout the network. By mapping this high dimensional data down to 2D with UMAP the authors can construct an \"atlas\" of how different images are perceived by the network.</p> <p></p> <p>Link to the paper</p>"},{"location":"scientific_papers/#timecluster-dimension-reduction-applied-to-temporal-data-for-visual-analytics","title":"TimeCluster: dimension reduction applied to temporal data for visual analytics","text":"<p>An interesting approach to time-series analysis, targeted toward cases where the time series has repeating patterns -- though no necessarily of a consistently periodic nature. The approach involves dimension reduction and clustering of sliding window blocks of the time-series. The result is a map where repeating behaviour is exposed as loop structures. This can be useful for both clustering similar blocks within a time-series, or finding outliers.</p> <p></p> <p>Link to the paper</p>"},{"location":"scientific_papers/#dimensionality-reduction-for-visualizing-single-cell-data-using-umap","title":"Dimensionality reduction for visualizing single-cell data using UMAP","text":"<p>An early paper on applying UMAP to single-cell biology data. It looks at both, gene-expression data and flow-cytometry data, and compares UMAP to t-SNE both in terms of performance and quality of results. This is a good introduction to using UMAP for single-cell biology data.</p> <p></p> <p>Link to the paper</p>"},{"location":"scientific_papers/#revealing-multi-scale-population-structure-in-large-cohorts","title":"Revealing multi-scale population structure in large cohorts","text":"<p>A paper looking at population genetics which uses UMAP as a means to visualise population structures. This produced some intriguing visualizations, and was one of the first of several papers taking this visualization approach. It also includes some novel visualizations using UMAP projections to 3D as RGB color specifications for data points, allowing the UMAP structure to be visualized in geographic maps based on where the samples were drawn from.</p> <p></p> <p>Link to the paper</p>"},{"location":"scientific_papers/#understanding-vulnerability-of-children-in-surrey","title":"Understanding Vulnerability of Children in Surrey","text":"<p>An example of the use of UMAP in sociological studies -- in this case looking at children in Surrey, British Columbia. Here UMAP is used as a tool to aid in general data analysis, and proves effective for the tasks to which it was put.</p> <p></p> <p>Link to the paper</p>"},{"location":"sparse/","title":"UMAP on sparse data","text":"<p>Sometimes datasets get very large, and potentially very very high dimensional. In many such cases, however, the data itself is sparse -- that is, while there are many many features, any given sample has only a small number of non-zero features observed. In such cases the data can be represented much more efficiently in terms of memory usage by a sparse matrix data structure. It can be hard to find dimension reduction techniques that work directly on such sparse data -- often one applies a basic linear technique such as <code>TruncatedSVD</code> from sklearn (which does accept sparse matrix input) to get the data in a format amenable to other more advanced dimension reduction techniques. In the case of UMAP this is not necessary -- UMAP can run directly on sparse matrix input. This tutorial will walk through a couple of examples of doing this. First we'll need some libraries loaded. We need <code>numpy</code> obviously, but we'll also make use of <code>scipy.sparse</code> which provides sparse matrix data structures. One of our examples will be purely mathematical, and we'll make use of <code>sympy</code> for that; the other example is test based and we'll use sklearn for that (specifically <code>sklearn.feature_extraction.text</code>). Beyond that we'll need umap, and plotting tools.</p> Python<pre><code>import numpy as np\nimport scipy.sparse\nimport sympy\nimport sklearn.datasets\nimport sklearn.feature_extraction.text\nimport umap\nimport umap.plot\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</code></pre>"},{"location":"sparse/#a-mathematical-example","title":"A mathematical example","text":"<p>Our first example constructs a sparse matrix of data out of pure math. This example is inspired by the work of <code>John Williamson &lt;https://johnhw.github.io/umap_primes/index.md.html&gt;</code>__, and if you haven't looked at that work you are strongly encouraged to do so. The dataset under consideration will be the integers. We will represent each integer by a vector of its divisibility by distinct primes. Thus our feature space is the space of prime numbers (less than or equal to the largest integer we will be considering) -- potentially very high dimensional. In practice a given integer is divisible by only a small number of distinct primes, so each sample will be mostly made up of zeros (all the primes that the number is not divisible by), and thus we will have a very sparse dataset.</p> <p>To get started we'll need a list of all the primes. Fortunately we have <code>sympy</code> at our disposal and we can quickly get that information with a single call to <code>primerange</code>. We'll also need a dictionary mapping the different primes to the column number they correspond to in our data structure; effectively we'll just be enumerating the primes.</p> Python<pre><code>primes = list(sympy.primerange(2, 110000))\nprime_to_column = {p:i for i, p in enumerate(primes)}\n</code></pre> <p>Now we need to construct our data in a format we can put into a sparse matrix easily. At this point a little background on sparse matrix data structures is useful. For this purpose we'll be using the so called <code>\"LIL\" format &lt;https://scipy-lectures.org/advanced/scipy_sparse/lil_matrix.html&gt;</code>__. LIL is short for \"List of Lists\", since that is how the data is internally stored. There is a list of all the rows, and each row is stored as a list giving the column indices of the non-zero entries. To store the data values there is a parallel structure containing the value of the entry corresponding to a given row and column.</p> <p>To put the data together in this sort of format we need to construct such a list of lists. We can do that by iterating over all the integers up to a fixed bound, and for each integer (i.e. each row in our dataset) generating the list of column indices which will be non-zero. The column indices will simply be the indices corresponding to the primes that divide the number. Since <code>sympy</code> has a function <code>primefactors</code> which returns a list of the unique prime factors of any integer we simply need to map those through our dictionary to covert the primes into column numbers.</p> <p>Parallel to that we'll construct the corresponding structure of values to insert into a matrix. Since we are only concerned with divisibility this will simply be a one in every non-zero entry, so we can just add a list of ones of the appropriate length for each row.</p> Python<pre><code>%%time\nlil_matrix_rows = []\nlil_matrix_data = []\nfor n in range(100000):\n    prime_factors = sympy.primefactors(n)\n    lil_matrix_rows.append([prime_to_column[p] for p in prime_factors])\n    lil_matrix_data.append([1] * len(prime_factors))\n</code></pre> Text Only<pre><code>CPU times: user 2.07 s, sys: 26.4 ms, total: 2.1 s\nWall time: 2.1 s\n</code></pre> <p>Now we need to get that into a sparse matrix. Fortunately the <code>scipy.sparse</code> package makes this easy, and we've already built the data in a fairly useful structure. First we create a sparse matrix of the correct format (LIL) and the right shape (as many rows as we have generated, and as many columns as there are primes). This is essentially just an empty matrix however. We can fix that by setting the <code>rows</code> attribute to be the rows we have generated, and the <code>data</code> attribute to be the corresponding structure of values (all ones). The result is a sparse matrix data structure which can then be easily manipulated and converted into other sparse matrix formats easily.</p> Python<pre><code>factor_matrix = scipy.sparse.lil_matrix((len(lil_matrix_rows), len(primes)), dtype=np.float32)\nfactor_matrix.rows = np.array(lil_matrix_rows)\nfactor_matrix.data = np.array(lil_matrix_data)\nfactor_matrix\n</code></pre> Text Only<pre><code>&lt;100000x10453 sparse matrix of type '&lt;class 'numpy.float32'&gt;'\n    with 266398 stored elements in LInked List format&gt;\n</code></pre> <p>As you can see we have a matrix with 100000 rows and over 10000 columns. If we were storing that as a numpy array it would take a great deal of memory. In practice, however, there are only 260000 or so entries that are not zero, and that's all we really need to store, making it much more compact.</p> <p>The question now is how can we feed that sparse matrix structure into UMAP to have it learn an embedding. The answer is surprisingly straightforward -- we just hand it directly to the fit method. Just like other sklearn estimators that can handle sparse input UMAP will detect the sparse matrix and just do the right thing.</p> Python<pre><code>%%time\nmapper = umap.UMAP(metric='cosine', random_state=42, low_memory=True).fit(factor_matrix)\n</code></pre> Text Only<pre><code>CPU times: user 9min 36s, sys: 6.76 s, total: 9min 43s\nWall time: 9min 7s\n</code></pre> <p>That was easy! But is it really working? We can easily plot the results:</p> Python<pre><code>umap.plot.points(mapper, values=np.arange(100000), theme='viridis')\n</code></pre> <p></p> <p>And this looks very much in line with the results <code>John Williamson got &lt;https://johnhw.github.io/umap_primes/index.md.html&gt;</code>__ with the proviso that we only used 100,000 integers instead of 1,000,000 to ensure that most users should be able to run this example (the full million may require a large memory compute node). So it seems like this is working well. The next question is whether we can use the <code>transform</code> functionality to map new data into this space. To test that we'll need some more data. Fortunately there are more integers. We'll grab the next 10,000 and put them in a sparse matrix, much as we did for the first 100,000.</p> Python<pre><code>%%time\nlil_matrix_rows = []\nlil_matrix_data = []\nfor n in range(100000, 110000):\n    prime_factors = sympy.primefactors(n)\n    lil_matrix_rows.append([prime_to_column[p] for p in prime_factors])\n    lil_matrix_data.append([1] * len(prime_factors))\n</code></pre> Text Only<pre><code>CPU times: user 214 ms, sys: 1.99 ms, total: 216 ms\nWall time: 222 ms\n</code></pre> Python<pre><code>new_data = scipy.sparse.lil_matrix((len(lil_matrix_rows), len(primes)), dtype=np.float32)\nnew_data.rows = np.array(lil_matrix_rows)\nnew_data.data = np.array(lil_matrix_data)\nnew_data\n</code></pre> Text Only<pre><code>&lt;10000x10453 sparse matrix of type '&lt;class 'numpy.float32'&gt;'\n    with 27592 stored elements in LInked List format&gt;\n</code></pre> <p>To map the new data we generated we can simply hand it to the <code>transform</code> method of our trained model. This is a little slow, but it does work.</p> Python<pre><code>new_data_embedding = mapper.transform(new_data)\n</code></pre> <p>And we can plot the results. Since we just got the locations of the points this time (rather than a model) we'll have to resort to matplotlib for plotting.</p> Python<pre><code>fig = plt.figure(figsize=(12,12))\nax = fig.add_subplot(111)\nplt.scatter(new_data_embedding[:, 0], new_data_embedding[:, 1], s=0.1, c=np.arange(10000), cmap='viridis')\nax.set(xticks=[], yticks=[], facecolor='black');\n</code></pre> <p></p> <p>The color scale is different in this case, but you can see that the data has been mapped into locations corresponding to the various structures seen in the original embedding. Thus, even with large sparse data we can embed the data, and even add new data to the embedding.</p>"},{"location":"sparse/#a-text-analysis-example","title":"A text analysis example","text":"<p>Let's look at a more classical machine learning example of working with sparse high dimensional data -- working with text documents. Machine learning on text is hard, and there is a great deal of literature on the subject, but for now we'll just consider a basic approach. Part of the difficulty of machine learning with text is turning language into numbers, since numbers are really all most machine learning algorithms understand (at heart anyway). One of the most straightforward ways to do this for documents is what is known as the <code>\"bag-of-words\" model &lt;https://en.wikipedia.org/wiki/Bag-of-words_model&gt;</code>__. In this model we view a document as simply a multi-set of the words contained in it -- we completely ignore word order. The result can be viewed as a matrix of data by setting the feature space to be the set of all words that appear in any document, and a document is represented by a vector where the value of the i th entry is the number of times the i th word occurs in that document. This is a very common approach, and is what you will get if you apply sklearn's <code>CountVectorizer</code> to a text dataset for example. The catch with this approach is that the feature space is often very large, since we have a feature for each and every word that ever occurs in the entire corpus of documents. The data is sparse however, since most documents only use a small portion of the total possible vocabulary. Thus the default output format of <code>CountVectorizer</code> (and other similar feature extraction tools in sklearn) is a <code>scipy.sparse</code> format matrix.</p> <p>For this example we'll make use of the classic 20newsgroups dataset, a sampling of newsgroup messages from the old NNTP newsgroup system covering 20 different newsgroups. The <code>sklearn.datasets</code> module can easily fetch the data, and, in fact, we can fetch a pre-vectorized version to save us the trouble of running <code>CountVectorizer</code> ourselves. We'll grab both the training set, and the test set for later use.</p> Python<pre><code>news_train = sklearn.datasets.fetch_20newsgroups_vectorized(subset='train')\nnews_test = sklearn.datasets.fetch_20newsgroups_vectorized(subset='test')\n</code></pre> <p>If we look at the actual data we have pulled back, we'll see that sklearn has run a <code>CountVectorizer</code> and produced the data in the sparse matrix format.</p> Python<pre><code>news_train.data\n</code></pre> Text Only<pre><code>&lt;11314x130107 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 1787565 stored elements in Compressed Sparse Row format&gt;\n</code></pre> <p>The value of the sparse matrix format is immediately obvious in this case; while there are only 11,000 samples there are 130,000 features! If the data were stored in a standard <code>numpy</code> array we would be using up 10GB of memory! And most of that memory would simply be storing the number zero, over and over again. In the sparse matrix format it easily fits in memory on most machines. This sort of dimensionality of data is very common with text workloads.</p> <p>The raw counts are, however, not ideal since common words such as \"the\" and \"and\" will dominate the counts for most documents, while contributing very little information about the actual content of the document. We can correct for this by using a <code>TfidfTransformer</code> from sklearn, which will convert the data into <code>TF-IDF format &lt;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&gt;</code>__. There are lots of ways to think about the transformation done by TF-IDF, but I like to think of it intuitively as follows. The information content of a word can be thought of as (roughly) proportional to the negative log of the frequency of the word; the more often a word is used, the less information it tends to carry, and infrequent words carry more information. What TF-IDF is going to do can be thought of as akin to re-weighting the columns according to the information content of the word associated to that column. Thus the common words like \"the\" and \"and\" will get down-weighted, as carrying less information about the document, while infrequent words will be deemed more imporant and have their associated columns up-weighted. We can apply this transformation to both the train and test sets (using the same transformer trained on the training set).</p> Python<pre><code>tfidf = sklearn.feature_extraction.text.TfidfTransformer(norm='l1').fit(news_train.data)\ntrain_data = tfidf.transform(news_train.data)\ntest_data = tfidf.transform(news_test.data)\n</code></pre> <p>The result is still a sparse matrix, since TF-IDF doesn't change the zero elements at all, nor the number of features.</p> Python<pre><code>train_data\n</code></pre> Text Only<pre><code>&lt;11314x130107 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 1787565 stored elements in Compressed Sparse Row format&gt;\n</code></pre> <p>Now we need to pass this very high dimensional data to UMAP. Unlike some other non-linear dimension reduction techniques we don't need to apply PCA first to get the data down to a reasonable dimensionality; nor do we need to use other techniques to reduce the data to be able to be represented as a dense <code>numpy</code> array; we can work directly on the 130,000 dimensional sparse matrix.</p> Python<pre><code>%%time\nmapper = umap.UMAP(metric='hellinger', random_state=42).fit(train_data)\n</code></pre> Text Only<pre><code>CPU times: user 8min 40s, sys: 3.07 s, total: 8min 44s\nWall time: 8min 43s\n</code></pre> <p>Now we can plot the results, with labels according to the target variable of the data -- which newsgroup the posting was drawn from.</p> Python<pre><code>umap.plot.points(mapper, labels=news_train.target)\n</code></pre> <p></p> <p>We can see that even going directly from a 130,000 dimensional space down to only 2 dimensions UMAP has done a decent job of separating out many of the different newsgroups.</p> <p>We can now attempt to add the test data to the same space using the <code>transform</code> method.</p> Python<pre><code>test_embedding = mapper.transform(test_data)\n</code></pre> <p>While this is somewhat expensive computationally, it does work, and we can plot the end result:</p> Python<pre><code>fig = plt.figure(figsize=(12,12))\nax = fig.add_subplot(111)\nplt.scatter(test_embedding[:, 0], test_embedding[:, 1], s=1, c=news_test.target, cmap='Spectral')\nax.set(xticks=[], yticks=[]);\n</code></pre> <p></p>"},{"location":"supervised/","title":"UMAP for Supervised Dimension Reduction and Metric Learning","text":"<p>While UMAP can be used for standard unsupervised dimension reduction the algorithm offers significant flexibility allowing it to be extended to perform other tasks, including making use of categorical label information to do supervised dimension reduction, and even metric learning. We'll look at some examples of how to do that below.</p> <p>First we will need to load some base libraries -- <code>numpy</code>, obviously, but also <code>mnist</code> to read in the Fashion-MNIST data, and matplotlib and seaborn for plotting.</p> Python<pre><code>import numpy as np\nfrom mnist.loader import MNIST\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style='white', context='poster')\n</code></pre> <p>Our example dataset for this exploration will be the <code>Fashion-MNIST dataset from Zalando Research &lt;https://github.com/zalandoresearch/fashion-mnist&gt;</code>. It is designed to be a drop-in replacement for the classic MNIST digits dataset, but uses images of fashion items (dresses, coats, shoes, bags, etc.) instead of handwritten digits. Since the images are more complex it provides a greater challenge than MNIST digits. We can load it in (after downloading the dataset) using the <code>mnist library &lt;https://pypi.org/project/python-mnist/&gt;</code>. We can then package up the train and test sets into one large dataset, normalise the values (to be in the range [0,1]), and set up labels for the 10 classes.</p> Python<pre><code>mndata = MNIST('fashion-mnist/data/fashion')\ntrain, train_labels = mndata.load_training()\ntest, test_labels = mndata.load_testing()\ndata = np.array(np.vstack([train, test]), dtype=np.float64) / 255.0\ntarget = np.hstack([train_labels, test_labels])\nclasses = [\n    'T-shirt/top',\n    'Trouser',\n    'Pullover',\n    'Dress',\n    'Coat',\n    'Sandal',\n    'Shirt',\n    'Sneaker',\n    'Bag',\n    'Ankle boot']\n</code></pre> <p>Next we'll load the <code>umap</code> library so we can perform dimension reduction on this dataset.</p> Python<pre><code>import umap\n</code></pre>"},{"location":"supervised/#umap-on-fashion-mnist","title":"UMAP on Fashion MNIST","text":"<p>First we'll just do standard unsupervised dimension reduction using UMAP so we have a baseline of what the results look like for later comparison. This is simply a matter of instantiating a <code>umap.umap_.UMAP</code> object (in this case setting the :attr:<code>~umap.umap_.UMAP.n_neighbors</code> parameter to be 5 -- we are interested mostly in very local information), then calling the <code>umap.umap_.UMAP.fit_transform</code> method with the data we wish to reduce. By default UMAP reduces to two dimensions, so we'll be able to view the results as a scatterplot.</p> Python<pre><code>%%time\nembedding = umap.UMAP(n_neighbors=5).fit_transform(data)\n</code></pre> Text Only<pre><code>CPU times: user 1min 45s, sys: 7.22 s, total: 1min 52s\nWall time: 1min 26s\n</code></pre> <p>That took a little time, but not all that long considering it is 70,000 data points in 784 dimensional space. We can simply plot the results as a scatterplot, colored by the class of the fashion item. We can use matplotlib's colorbar with suitable tick-labels to give us the color key.</p> Python<pre><code>fig, ax = plt.subplots(1, figsize=(14, 10))\nplt.scatter(*embedding.T, s=0.3, c=target, cmap='Spectral', alpha=1.0)\nplt.setp(ax, xticks=[], yticks=[])\ncbar = plt.colorbar(boundaries=np.arange(11)-0.5)\ncbar.set_ticks(np.arange(10))\ncbar.set_ticklabels(classes)\nplt.title('Fashion MNIST Embedded via UMAP');\n</code></pre> <p></p> <p>The result is fairly good. We successfully separated a number of the classes, and the global structure (separating pants and footwear from shirts, coats and dresses) is well preserved as well. Unlike results for MNIST digits, however, there were a number of classes that did not separate quite so cleanly. In particular T-shirts, shirts, dresses, pullovers, and coats are all a little mixed. At the very least the dresses are largely separated, and the T-shirts are mostly in one large clump, but they are not well distinguished from the others. Worse still are the coats, shirts, and pullovers (somewhat unsurprisingly as these can certainly look very similar) which all have significant overlap with one another. Ideally we would like much better class separation. Since we have the label information we can actually give that to UMAP to use!</p>"},{"location":"supervised/#using-labels-to-separate-classes-supervised-umap","title":"Using Labels to Separate Classes (Supervised UMAP)","text":"<p>How do we go about coercing UMAP to make use of target labels? If you are familiar with the sklearn API you'll know that the <code>umap.umap_.UMAP.fit</code> method takes a target parameter <code>y</code> that specifies supervised target information (for example when training a supervised classification model). We can simply pass the <code>umap.umap_.UMAP</code> model that target data when fitting and it will make use of it to perform supervised dimension reduction!</p> Python<pre><code>%%time\nembedding = umap.UMAP().fit_transform(data, y=target)\n</code></pre> Text Only<pre><code>CPU times: user 3min 28s, sys: 9.17 s, total: 3min 37s\nWall time: 2min 45s\n</code></pre> <p>This took a little longer -- both because we are using a larger obj:<code>~umap.umap_.UMAP.n_neighbors</code> value (which is suggested when doing supervised dimension reduction; here we are using the default value of 15), and because we need to condition on the label data. As before we have reduced the data down to two dimensions so we can again visualize the data with a scatterplot, coloring by class.</p> Python<pre><code>fig, ax = plt.subplots(1, figsize=(14, 10))\nplt.scatter(*embedding.T, s=0.1, c=target, cmap='Spectral', alpha=1.0)\nplt.setp(ax, xticks=[], yticks=[])\ncbar = plt.colorbar(boundaries=np.arange(11)-0.5)\ncbar.set_ticks(np.arange(10))\ncbar.set_ticklabels(classes)\nplt.title('Fashion MNIST Embedded via UMAP using Labels');\n</code></pre> <p></p> <p>The result is a cleanly separated set of classes (and a little bit of stray noise -- points that were sufficiently different from their class as to not be grouped with the rest). Aside from the clear class separation however (which is expected -- we gave the algorithm all the class information), there are a couple of important points to note. The first point to note is that we have retained the internal structure of the individual classes. Both the shirts and pullovers still have the distinct banding pattern that was visible in the original unsupervised case; the pants, t-shirts and bags both retained their shape and internal structure; etc. The second point to note is that we have also retained the global structure. While the individual classes have been cleanly separated from one another, the inter-relationships among the classes have been preserved: footwear classes are all near one another; trousers and bags are at opposite sides of the plot; and the arc of pullover, shirts, t-shirts and dresses is still in place.</p> <p>The key point is this: the important structural properties of the data have been retained while the known classes have been cleanly pulled apart and isolated. If you have data with known classes and want to separate them while still having a meaningful embedding of individual points then supervised UMAP can provide exactly what you need.</p>"},{"location":"supervised/#using-partial-labelling-semi-supervised-umap","title":"Using Partial Labelling (Semi-Supervised UMAP)","text":"<p>What if we only have some of our data labelled, however, and a number of items are without labels. Can we still make use of the label information we do have? This is now a semi-supervised learning problem, and yes, we can work with those cases too. To set up the example we'll mask some of the target information -- we'll do this by using the sklearn standard of giving unlabelled points a label of -1 (such as, for example, the noise points from a DBSCAN clustering).</p> Python<pre><code>masked_target = target.copy().astype(np.int8)\nmasked_target[np.random.choice(70000, size=10000, replace=False)] = -1\n</code></pre> <p>Now that we have randomly masked some of the labels we can try to perform supervised learning again. Everything works as before, but UMAP will interpret the -1 label as being an unlabelled point and learn accordingly.</p> Python<pre><code>%%time\nfitter = umap.UMAP().fit(data, y=masked_target)\nembedding = fitter.embedding_\n</code></pre> Text Only<pre><code>CPU times: user 3min 8s, sys: 7.85 s, total: 3min 16s\nWall time: 2min 40s\n</code></pre> <p>Again we can look at a scatterplot of the data colored by class.</p> Python<pre><code>fig, ax = plt.subplots(1, figsize=(14, 10))\nplt.scatter(*embedding.T, s=0.1, c=target, cmap='Spectral', alpha=1.0)\nplt.setp(ax, xticks=[], yticks=[])\ncbar = plt.colorbar(boundaries=np.arange(11)-0.5)\ncbar.set_ticks(np.arange(10))\ncbar.set_ticklabels(classes)\nplt.title('Fashion MNIST Embedded via UMAP using Partial Labels');\n</code></pre> <p></p> <p>The result is much as we would expect -- while we haven't cleanly separated the data as we did in the totally supervised case, the classes have been made cleaner and more distinct. This semi-supervised approach provides a powerful tool when labelling is potentially expensive, or when you have more data than labels, but want to make use of that extra data.</p>"},{"location":"supervised/#training-with-labels-and-embedding-unlabelled-test-data-metric-learning-with-umap","title":"Training with Labels and Embedding Unlabelled Test Data (Metric Learning with UMAP)","text":"<p>If we have learned a supervised embedding, can we use that to embed new previously unseen (and now unlabelled) points into the space? This would provide an algorithm for <code>metric learning &lt;https://en.wikipedia.org/wiki/Similarity_learning#Metric_learning&gt;</code>__, where we can use a labelled set of points to learn a metric on data, and then use that learned metric as a measure of distance between new unlabelled points. This can be particularly useful as part of a machine learning pipline where we learn a supervised embedding as a form of supervised feature engineering, and then build a classifier on that new space -- this is viable as long as we can pass new data to the embedding model to be transformed to the new space.</p> <p>To try this out with UMAP let's use the train/test split provided by Fashion MNIST:</p> Python<pre><code>train_data = np.array(train)\ntest_data = np.array(test)\n</code></pre> <p>Now we can fit a model to the training data, making use of the training labels to learn a supervised embedding.</p> Python<pre><code>%%time\nmapper = umap.UMAP(n_neighbors=10).fit(train_data, np.array(train_labels))\n</code></pre> Text Only<pre><code>CPU times: user 2min 18s, sys: 7.53 s, total: 2min 26s\nWall time: 1min 52s\n</code></pre> <p>Next we can use the <code>umap.umap_.UMAP.transform</code> method on that model to transform the test set into the learned space. This time we won't pass the label information and let the model attempt to place the data correctly.</p> Python<pre><code>%%time\ntest_embedding = mapper.transform(test_data)\n</code></pre> Text Only<pre><code>CPU times: user 17.3 s, sys: 986 ms, total: 18.3 s\nWall time: 15.4 s\n</code></pre> <p>UMAP transforms are not as fast as some approaches, but as you can see this was still fairly efficient. The important question is how well we managed to embed the test data into the existing learned space. To start let's visualise the embedding of the training data so we can get a sense of where things should go.</p> Python<pre><code>fig, ax = plt.subplots(1, figsize=(14, 10))\nplt.scatter(*mapper.embedding_.T, s=0.3, c=np.array(train_labels), cmap='Spectral', alpha=1.0)\nplt.setp(ax, xticks=[], yticks=[])\ncbar = plt.colorbar(boundaries=np.arange(11)-0.5)\ncbar.set_ticks(np.arange(10))\ncbar.set_ticklabels(classes)\nplt.title('Fashion MNIST Train Digits Embedded via UMAP Transform');\n</code></pre> <p></p> <p>As you can see this has done a similar job as before, successfully embedding the separate classes while retaining both the internal structure and the overall global structure. We can now look at how the test set, for which we provided no label information, was embedded via the <code>umap.umap_.UMAP.transform</code> method.</p> Python<pre><code>fig, ax = plt.subplots(1, figsize=(14, 10))\nplt.scatter(*test_embedding.T, s=2, c=np.array(test_labels), cmap='Spectral', alpha=1.0)\nplt.setp(ax, xticks=[], yticks=[])\ncbar = plt.colorbar(boundaries=np.arange(11)-0.5)\ncbar.set_ticks(np.arange(10))\ncbar.set_ticklabels(classes)\nplt.title('Fashion MNIST Test Digits Embedded via UMAP');\n</code></pre> <p></p> <p>As you can see we have replicated the layout of the training data, including much of the internal structure of the classes. For the most part assignment of new points follows the classes well. The greatest source of confusion are some t-shirts that ended up mixed with the shirts, and some pullovers which are confused with the coats. Given the difficulty of the problem this is a good result, particularly when compared with current state-of-the-art approaches such as <code>siamese and triplet networks &lt;https://github.com/adambielski/siamese-triplet/blob/master/Experiments_FashionMNIST.ipynb&gt;</code>__.</p>"},{"location":"supervised/#supervised-umap-on-the-galaxy10sdss-dataset","title":"Supervised UMAP on the Galaxy10SDSS dataset","text":"<p>The Galaxy10SDSS dataset is a crowd sourced human labelled dataset of galaxy images, which have been separated in to ten classes. Umap can learn an embedding that partially separates the data. To keep runtime small, UMAP is applied to a subset of the data.</p> Python<pre><code>import numpy as np\nimport h5py\nimport matplotlib.pyplot as plt\nimport umap\nimport os\nimport math\nimport requests\n\nif not os.path.isfile(\"Galaxy10.h5\"):\n    url = \"http://astro.utoronto.ca/~bovy/Galaxy10/Galaxy10.h5\"\n    r = requests.get(url, allow_redirects=True)\n    open(\"Galaxy10.h5\", \"wb\").write(r.content)\n\n# To get the images and labels from file\nwith h5py.File(\"Galaxy10.h5\", \"r\") as F:\n    images = np.array(F[\"images\"])\n    labels = np.array(F[\"ans\"])\n\nX_train = np.empty([math.floor(len(labels) / 100), 14283], dtype=np.float64)\ny_train = np.empty([math.floor(len(labels) / 100)], dtype=np.float64)\nX_test = X_train\ny_test = y_train\n# Get a subset of the data\nfor i in range(math.floor(len(labels) / 100)):\n    X_train[i, :] = np.array(np.ndarray.flatten(images[i, :, :, :]), dtype=np.float64)\n    y_train[i] = labels[i]\n    X_test[i, :] = np.array(\n        np.ndarray.flatten(images[i + math.floor(len(labels) / 100), :, :, :]),\n        dtype=np.float64,\n    )\n    y_test[i] = labels[i + math.floor(len(labels) / 100)]\n\n# Plot distribution\nclasses, frequency = np.unique(y_train, return_counts=True)\nfig = plt.figure(1, figsize=(4, 4))\nplt.clf()\nplt.bar(classes, frequency)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Data Subset\")\nplt.savefig(\"galaxy10_subset.svg\")\n</code></pre> <p></p> <p>The figure shows that the selected subset of the data set is unbalanced, but the entire dataset is also unbalanced, so this experiment will still use this subset. The next step is to examine the output of the standard UMAP algorithm.</p> Python<pre><code>reducer = umap.UMAP(\n    n_components=2, n_neighbors=5, random_state=42, transform_seed=42, verbose=False\n)\nreducer.fit(X_train)\n\ngalaxy10_umap = reducer.transform(X_train)\nfig = plt.figure(1, figsize=(4, 4))\nplt.clf()\nplt.scatter(\n    galaxy10_umap[:, 0],\n    galaxy10_umap[:, 1],\n    c=y_train,\n    cmap=plt.cm.nipy_spectral,\n    edgecolor=\"k\",\n    label=y_train,\n)\nplt.colorbar(boundaries=np.arange(11) - 0.5).set_ticks(np.arange(10))\nplt.savefig(\"galaxy10_2D_umap.svg\")\n</code></pre> <p></p> <p>The standard UMAP algorithm does not separate the galaxies according to their type. Supervised UMAP can do better.</p> Python<pre><code>reducer = umap.UMAP(\n    n_components=2, n_neighbors=15, random_state=42, transform_seed=42, verbose=False\n)\nreducer.fit(X_train, y_train)\n\ngalaxy10_umap_supervised = reducer.transform(X_train)\nfig = plt.figure(1, figsize=(4, 4))\nplt.clf()\nplt.scatter(\n    galaxy10_umap_supervised[:, 0],\n    galaxy10_umap_supervised[:, 1],\n    c=y_train,\n    cmap=plt.cm.nipy_spectral,\n    edgecolor=\"k\",\n    label=y_train,\n)\nplt.colorbar(boundaries=np.arange(11) - 0.5).set_ticks(np.arange(10))\nplt.savefig(\"galaxy10_2D_umap_supervised.svg\")\n</code></pre> <p></p> <p>Supervised UMAP does indeed do better. There is a litle overlap between some of the classes, but the original dataset also has some ambiguities in the classification.  The best check of this method is to project the testing data onto the learned embedding.</p> Python<pre><code>galaxy10_umap_supervised_prediction = reducer.transform(X_test)\nfig = plt.figure(1, figsize=(4, 4))\nplt.clf()\nplt.scatter(\n    galaxy10_umap_supervised_prediction[:, 0],\n    galaxy10_umap_supervised_prediction[:, 1],\n    c=y_test,\n    cmap=plt.cm.nipy_spectral,\n    edgecolor=\"k\",\n    label=y_test,\n)\nplt.colorbar(boundaries=np.arange(11) - 0.5).set_ticks(np.arange(10))\nplt.savefig(\"galaxy10_2D_umap_supervised_prediction.svg\")\n</code></pre> <p></p> <p>This shows that the learned embedding can be used on new data sets, and so this method may be helpful for examining images of galaxies. Try out this method on the full 200 Mb dataset as well as the newer 2.54 Gb Galaxy 10 DECals dataset</p>"},{"location":"transform/","title":"Transforming New Data with UMAP","text":"<p>UMAP is useful for generating visualisations, but if you want to make use of UMAP more generally for machine learning tasks it is important to be be able to train a model and then later pass new data to the model and have it transform that data into the learned space. For example if we use UMAP to learn a latent space and then train a classifier on data transformed into the latent space then the classifier is only useful for prediction if we can transform data for which we want a prediction into the latent space the classifier uses. Fortunately UMAP makes this possible, albeit more slowly than some other transformers that allow this.</p> <p>This tutorial will step through a simple case where we expect the overall distribution in our higher-dimensional vectors to be consistent between the training and testing data. For more detail on how this can go wrong, and how we can fix it using Parametric UMAP, see :doc:<code>transform_landmarked_pumap</code>.</p> <p>To demonstrate this functionality we'll make use of scikit-learn and the digits dataset contained therein (see :doc:<code>basic_usage</code> for an example of the digits dataset). First let's load all the modules we'll need to get this done.</p> Python<pre><code>import numpy as np\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n</code></pre> Python<pre><code>sns.set(context='notebook', style='white', rc={'figure.figsize':(14,10)})\n</code></pre> Python<pre><code>digits = load_digits()\n</code></pre> <p>To keep everything honest let's use sklearn <code>train_test_split</code> to separate out a training and test set (stratified over the different digit types). By default <code>train_test_split</code> will carve off 25% of the data for testing, which seems suitable in this case.</p> Python<pre><code>X_train, X_test, y_train, y_test = train_test_split(digits.data,\n                                                    digits.target,\n                                                    stratify=digits.target,\n                                                    random_state=42)\n</code></pre> <p>Now to get a benchmark idea of what we are looking at let's train a couple of different classifiers and then see how well they score on the test set. For this example let's try a support vector classifier and a KNN classifier. Ideally we should be tuning hyper-parameters (perhaps a grid search using k-fold cross validation), but for the purposes of this simple demo we will simply use default parameters for both classifiers.</p> Python<pre><code>svc = SVC().fit(X_train, y_train)\nknn = KNeighborsClassifier().fit(X_train, y_train)\n</code></pre> <p>The next question is how well these classifiers perform on the test set. Conveniently sklearn provides a <code>score</code> method that can output the accuracy on the test set.</p> Python<pre><code>svc.score(X_test, y_test), knn.score(X_test, y_test)\n</code></pre> Text Only<pre><code>(0.62, 0.9844444444444445)\n</code></pre> <p>The result is that the support vector classifier apparently had poor hyper-parameters for this case (I expect with some tuning we could build a much more accurate model) and the KNN classifier is doing very well.</p> <p>The goal now is to make use of UMAP as a preprocessing step that one could potentially fit into a pipeline. We will therefore obviously need the <code>umap</code> module loaded.</p> Python<pre><code>import umap\n</code></pre> <p>To make use of UMAP as a data transformer we first need to fit the model with the training data. This works exactly as in the :doc:<code>basic_usage</code> example using the fit method. In this case we simply hand it the training data and it will learn an appropriate (two dimensional by default) embedding.</p> Python<pre><code>trans = umap.UMAP(n_neighbors=5, random_state=42).fit(X_train)\n</code></pre> <p>Since we embedded to two dimensions we can visualise the results to ensure that we are getting a potential benefit out of this approach. This is simply a matter of generating a scatterplot with data points colored by the class they come from. Note that the embedded training data can be accessed as the <code>.embedding_</code> attribute of the UMAP model once we have fit the model to some data.</p> Python<pre><code>plt.scatter(trans.embedding_[:, 0], trans.embedding_[:, 1], s= 5, c=y_train, cmap='Spectral')\nplt.title('Embedding of the training set by UMAP', fontsize=24);\n</code></pre> <p></p> <p>This looks very promising! Most of the classes got very cleanly separated, and that gives us some hope that it could help with classifier performance. It is worth noting that this was a completely unsupervised data transform; we could have used the training label information, but that is the subject of :doc:<code>a later tutorial &lt;supervised&gt;</code>.</p> <p>We can now train some new models (again an SVC and a KNN classifier) on the embedded training data. This looks exactly as before but now we pass it the embedded data. Note that calling <code>transform</code> on input identical to what the model was trained on will simply return the <code>embedding_</code> attribute, so sklearn pipelines will work as expected.</p> Python<pre><code>svc = SVC().fit(trans.embedding_, y_train)\nknn = KNeighborsClassifier().fit(trans.embedding_, y_train)\n</code></pre> <p>Now we want to work with the test data which none of the models (UMAP or the classifiers) have seen. To do this we use the standard sklearn API and make use of the <code>transform</code> method, this time handing it the new unseen test data. We will assign this to <code>test_embedding</code> so that we can take a closer look at the result of applying an existing UMAP model to new data.</p> Python<pre><code>%time test_embedding = trans.transform(X_test)\n</code></pre> Text Only<pre><code>CPU times: user 867 ms, sys: 70.7 ms, total: 938 ms\nWall time: 335 ms\n</code></pre> <p>Note that the transform operations works very efficiently -- taking less than half a second. Compared to some other transformers this is a little on the slow side, but it is fast enough for many uses. Note that as the size of the training and/or test sets increase the performance will slow proportionally. It's also worth noting that the first call to transform may be slow due to Numba JIT overhead -- further runs will be very fast.</p> <p>The next important question is what the transform did to our test data. In principle we have a new two dimensional representation of the test-set, and ideally this should be based on the existing embedding of the training set. We can check this by visualising the data (since we are in two dimensions) to see if this is true. A simple scatterplot as before will suffice.</p> Python<pre><code>plt.scatter(test_embedding[:, 0], test_embedding[:, 1], s= 5, c=y_test, cmap='Spectral')\nplt.title('Embedding of the test set by UMAP', fontsize=24);\n</code></pre> <p></p> <p>The results look like what we should expect; the test data has been embedded into two dimensions in exactly the locations we should expect (by class) given the embedding of the training data visualised above. This means we can now try out models that were trained on the embedded training data by handing them the newly transformed test set.</p> Python<pre><code>svc.score(trans.transform(X_test), y_test), knn.score(trans.transform(X_test), y_test)\n</code></pre> Text Only<pre><code>(0.9844444444444445, 0.9844444444444445)\n</code></pre> <p>The results are pretty good. While the accuracy of the KNN classifier did not improve there was not a lot of scope for improvement given the data. On the other hand the SVC has improved to have equal accuracy to the KNN classifier. Of course we could probably have achieved this level of accuracy by better setting SVC hyper-parameters, but the point here is that we can use UMAP as if it were a standard sklearn transformer as part of an sklearn machine learning pipeline.</p> <p>Just for fun we can run the same experiments, but this time reduce to ten dimensions (where we can no longer visualise). In practice this will have little gain in this case -- for the digits dataset two dimensions is plenty for UMAP and more dimensions won't help. On the other hand for more complex datasets where more dimensions may allow for a much more faithful embedding it is worth noting that we are not restricted to only two dimension.</p> Python<pre><code>trans = umap.UMAP(n_neighbors=5, n_components=10, random_state=42).fit(X_train)\n</code></pre> Python<pre><code>svc = SVC().fit(trans.embedding_, y_train)\nknn = KNeighborsClassifier().fit(trans.embedding_, y_train)\n</code></pre> Python<pre><code>svc.score(trans.transform(X_test), y_test), knn.score(trans.transform(X_test), y_test)\n</code></pre> Text Only<pre><code>(0.9822222222222222, 0.9822222222222222)\n</code></pre> <p>And we see that in this case we actually marginally lowered our accuracy scores (within the potential noise in such scoring mind you). However for more interesting datasets the larger dimensional embedding might have been a significant gain -- it is certainly worth exploring as one of the parameters in a grid search across a pipeline that includes UMAP.</p>"},{"location":"transform_landmarked_pumap/","title":"Transforming New Data with Parametric UMAP","text":"<p>There are many cases where one may want to take an existing UMAP model and use it to embed new data into the learned space. For a simple example where the overall distribution of the higher-dimensional training data matches that of the new data being embedded, see :doc:<code>transform</code>. We can't always be sure that this will be the case, however. To simulate a case where we have novel behaviour that we want to include in our embedding space, we will use the MNIST digits dataset (see :doc:<code>basic_usage</code> for a basic example).</p> <p>To follow along with this example, see the MNIST_Landmarks notebook on the GitHub repository</p> Text Only<pre><code>import keras\nfrom sklearn.model_selection import train_test_split\n\nfrom umap import UMAP, ParametricUMAP\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\n</code></pre> <p>We'll start by loading in the dataset, and splitting it into 2 equal parts with <code>sklearn</code>'s <code>train_test_split</code> function. This will give us two partitions to work with, one to train our original embedding and another to test it. In order to simulate new behaviour appearing in our data we remove one of the MNIST categories <code>N</code> from the <code>x1</code> partition. In this case we'll use <code>N=2</code>, so our model will be trained on all of the digits other than 2.</p> Python<pre><code>(X, y), (_, _) = keras.datasets.mnist.load_data()\nx1, x2, y1, y2 = train_test_split(X, y, test_size=0.5, random_state=42)\n\n# Reshape to 1D vectors\nx1 = x1.reshape((x1.shape[0], 28*28))\nx2 = x2.reshape((x2.shape[0], 28*28))\n\n# Remove one category from the train dataset.\n# In the case of MNIST digits, this will be the digit we are removing.\nN = 2\n\nx1 = x1[y1 != N]\ny1 = y1[y1 != N]\n\nprint(x1.shape, x2.shape)\n</code></pre> Text Only<pre><code>(26995, 784) (30000, 784)\n</code></pre>"},{"location":"transform_landmarked_pumap/#new-data-with-umap","title":"New data with UMAP","text":"<p>To start with, we'll identify the issues with using UMAP as-is in this case, and then we'll see how to fix them with Parametric UMAP. First off, we need to train a <code>UMAP</code> model on our <code>x1</code> partition:</p> Python<pre><code>embedder = UMAP()\n\nemb_x1 = embedder.fit_transform(x1)\n</code></pre> <p>Visualising our results:</p> Python<pre><code>plt.scatter(emb_x1[:,0], emb_x1[:,1], c=y1, cmap='Spectral', s=2, alpha=0.2)\n</code></pre> <p></p> <p>This is a clean and successful embedding, as we would expect from UMAP on this relatively-simple example. We see the normal structure one would expect from embedding MNIST, but without any of the 2s. The <code>UMAP</code> class is built to be compatible with <code>scikit-learn</code>, so passing new data through is as simple as using the <code>transform</code> method and passing through the new data. We'll pass through <code>x2</code>, which contains unseen examples of the original classes, and also samples from our holdout class, <code>N</code> (the 2s).</p> <p>To make samples from <code>N</code> Stand out more, we'll over-plot them in black.</p> Python<pre><code>emb_x2 = embedder.transform(x2)\n</code></pre> Python<pre><code>plt.scatter(emb_x2[:,0], emb_x2[:,1], c=y2, cmap='Spectral', s=2, alpha=0.2)\nplt.scatter(emb_x2[y2==N][:,0], emb_x2[y2==N][:,1], c='k', s=2, alpha=0.5)\n</code></pre> <p></p> <p>While our <code>UMAP</code> embedder has correctly handled the classes present in <code>x1</code> it has treated examples from our holdout class <code>N</code> poorly. Many of these points are concentrated on top of existing classes, with some spread out between them. This inability to generalize is not unique to UMAP, but is more generally a difficulty with learned embeddings. It also may or may not be an issue, depending on your use case.</p>"},{"location":"transform_landmarked_pumap/#new-data-with-parametric-umap","title":"New data with Parametric UMAP","text":"<p>We can improve this outcome with Parametric UMAP. Parametric UMAP differs from UMAP in that it learns the relationship between the data and embedding with a neural network, instead of learning embeddings directly. This means we can incorporate new data by continuing to train the neural network, updating the weights to incorporate our new information.</p> <p></p> <p>For more complete information on Parametric UMAP and the many options it provides, see :doc:<code>parametric_umap</code>.</p> <p>We will start adressing this by training a <code>ParametricUMAP</code> embedding model, and running the same experiment:</p> Python<pre><code>p_embedder = ParametricUMAP()\n\np_emb_x1 = p_embedder.fit_transform(x1)\n</code></pre> Python<pre><code>plt.scatter(p_emb_x1[:,0], p_emb_x1[:,1], c=y1, cmap='Spectral', s=2, alpha=0.2)\n</code></pre> <p></p> <p>Again, we get good results on our initial embedding of <code>x1</code>. If we pass <code>x2</code> through without re-training, we get a similar problem to our <code>UMAP</code> model:</p> Python<pre><code>p_emb_x2 = p_embedder.transform(x2)\n</code></pre> Python<pre><code>plt.scatter(p_emb_x2[:,0], p_emb_x2[:,1], c=y2, cmap='Spectral', s=2, alpha=0.2)\nplt.scatter(p_emb_x2[y2==N][:,0], p_emb_x2[y2==N][:,1], c='k', s=2, alpha=0.5)\n</code></pre> <p></p>"},{"location":"transform_landmarked_pumap/#re-training-parametric-umap-with-landmarks","title":"Re-training Parametric UMAP with landmarks","text":"<p>To update our embedding to include the new class, we'll fine-tune our existing <code>ParametricUMAP</code> model. Doing this without any other changes will start from where we left off, but our embedding space's structure may drift and change. This is because the UMAP loss function is invariant to translation and rotation, as it is only concerned with the relative positions and distances between points.</p> <p>In order to keep our embedding space more consistent, we'll use the landmarks option for <code>ParametricUMAP</code>. We retrain the model on the <code>x2</code> partition, along with some points chosen as landmarks from <code>x1</code>. We'll choose 1% of the samples in <code>x1</code> to be included, along with their current position in the embedding space to be used in the landmarks loss function.</p> <p>The default <code>landmark_loss_fn</code> is the euclidean distance between the point's original position and it's current one. The only change we'll make is to set <code>landmark_loss_weight=0.01</code>.</p> Python<pre><code># Select landmarks indexes from x1.\n#\nlandmark_idx = list(np.random.choice(range(x1.shape[0]), int(x1.shape[0]/100), replace=False))\n\n# Add the landmark points to x2 for training.\n#\nx2_lmk = np.concatenate((x2, x1[landmark_idx]))\ny2_lmk = np.concatenate((y2, y1[landmark_idx]))\n\n# Make our landmarks vector, which is nan where we have no landmark information.\n#\nlandmarks = np.stack(\n    [np.array([np.nan, np.nan])]*x2.shape[0] + list(\n        p_embedder.transform(\n            x1[landmark_idx]\n        )\n    )\n)\n\n# Set landmark loss weight and continue training our Parametric UMAP model.\n#\np_embedder.landmark_loss_weight = 0.01\np_embedder.fit(x2_lmk, landmark_positions=landmarks)\np_emb2_x2 = p_embedder.transform(x2)\n\n# Check how x1 looks when embedded in the space retrained on x2 and landmarks.\n#\np_emb2_x1 = p_embedder.transform(x1)\n</code></pre> <p>Plotting all of the different embeddings to compare them:</p> Python<pre><code>fig, axs = plt.subplots(3, 2, figsize=(16, 24), sharex=True, sharey=True)\n\naxs[0,0].scatter(\n    emb_x1[:, 0], emb_x1[:, 1], c=y1, cmap='Spectral', s=2, alpha=0.2,\n)\naxs[0,0].set_ylabel('UMAP Embedding', fontsize=20)\n\naxs[0,1].scatter(\n    emb_x2[:, 0], emb_x2[:, 1], c=y2, cmap='Spectral', s=2, alpha=0.2,\n)\naxs[0,1].scatter(\n    emb_x2[y2==N][:,0], emb_x2[y2==N][:,1], c='k', s=2, alpha=0.5,\n)\n\naxs[1,0].scatter(\n    p_emb_x1[:, 0], p_emb_x1[:, 1], c=y1, cmap='Spectral', s=2, alpha=0.2,\n)\naxs[1,0].set_ylabel('Initial P-UMAP Embedding', fontsize=20)\n\naxs[1,1].scatter(\n    p_emb_x2[:, 0], p_emb_x2[:, 1], c=y2, cmap='Spectral', s=2, alpha=0.2,\n)\naxs[1,1].scatter(\n    p_emb_x2[y2==N][:,0], p_emb_x2[y2==N][:,1], c='k', s=2, alpha=0.5\n)\n\naxs[2,0].scatter(\n    p_emb2_x1[:, 0], p_emb2_x1[:, 1], c=y1, cmap='Spectral', s=2, alpha=0.2,\n)\naxs[2,0].set_ylabel('Updated P-UMAP Embedding', fontsize=20)\naxs[2,0].set_xlabel(f'x1, No {N}s', fontsize=20)\n\naxs[2,1].scatter(\n    p_emb2_x2[:, 0], p_emb2_x2[:, 1], c=y2, cmap='Spectral', s=2, alpha=0.2,\n)\naxs[2,1].scatter(\n    p_emb2_x2[y2==N][:,0], p_emb2_x2[y2==N][:,1], c='k', s=2, alpha=0.5,\n)\naxs[2,1].set_xlabel('x2, All Classes', fontsize=20)\n\nplt.tight_layout()\n</code></pre> <p></p> <p>Here we see that our approach has been successful, The embedding space has been kept consistent and we now have a clear cluster of our new class, the 2s. This new cluster shows up in a sensible part of the embedding space, and the rest of the structure is preserved.</p> <p>It is worth double checking here that the landmark loss is not too constraining, we still would like a good UMAP structure. To do so, we can interrogate the history of our embedder, which will retain the history through our re-training steps.</p> Python<pre><code>plt.plot(p_embedder._history['loss'])\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\n</code></pre> <p></p> <p>We can identify the spike in loss where we introduce <code>x2</code>, and can confirm that the resulting loss is comparable to the loss from our initial training on <code>x1</code>. This tells us that the model is not having to compromise too much between the UMAP loss and the landmark loss. If this were not the case, it could potentially be improved by lowering the <code>landmark_loss_weight</code> attribute of our embedder object. There is a tradeoff to be made here between the consistency of the space and minimizing UMAP loss, but the key is we have smooth variation in the embedding space, which will make downstream tasks easier to adjust. In this case, we could probably stand to increase the <code>landmark_loss_weight</code> to keep the space more consistent.</p> <p>In addition to <code>landmark_loss_weight</code>, there are a number of other options available to us to try and get better results on this or other examples:</p> <ul> <li>Continuing the training with a larger portion of points from the original data, in our case <code>x1</code>. Not all of these points need to be landmarked, but they can contribute to a consistent graph structure in higher dimensions.</li> <li>Changing the <code>landmark_loss_fn</code>. For example, if we want to allow for points to move if they have to we could truncate the default euclidean loss function, allowing the metaphorical rubber band to snap at a certain point and prioritising a good UMAP structure once we discover that sticking to the landmark position is not correct.</li> <li>Being more intelligent with our selection of landmark points, for example using submodular optimization with a package like apricot-select or chosing points from different parts of a hierarchical clustering like HDBSCAN</li> </ul>"}]}
